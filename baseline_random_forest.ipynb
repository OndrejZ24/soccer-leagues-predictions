{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e467abf",
   "metadata": {},
   "source": [
    "# Baseline Random Forest Classifier\n",
    "\n",
    "This notebook builds and evaluates a baseline Random Forest classifier for soccer match prediction: Over/Under 2.5 goals (binary classification).\n",
    "\n",
    "## Structure:\n",
    "- **#0**: Setup and Data Loading\n",
    "- **#1**: Baseline Model Training\n",
    "- **#2**: Hyperparameter Tuning\n",
    "- **#3**: Feature Importance Analysis\n",
    "- **#4**: Learning Curves\n",
    "- **#5**: Final Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2f7bf8",
   "metadata": {},
   "source": [
    "## #0: Setup and Data Loading\n",
    "\n",
    "### #0.1: Import Required Libraries\n",
    "Import all necessary libraries for model training, evaluation, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3ee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                             roc_auc_score, confusion_matrix, classification_report,\n",
    "                             roc_curve, auc)\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59d6a55",
   "metadata": {},
   "source": [
    "### #0.2: Load Preprocessed Data\n",
    "Load the baseline preprocessed data containing training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421c1678",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./processed/baseline_preprocessed.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_val = data['X_val']\n",
    "y_val = data['y_val']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc7550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data shapes\n",
    "print(f\"Training set: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Validation set: X={X_val.shape}, y={y_val.shape}\")\n",
    "print(f\"Test set: X={X_test.shape}, y={y_test.shape}\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "class_dist = pd.Series(y_train).value_counts(normalize=True).sort_index()\n",
    "class_names = {0: 'Under 2.5 goals', 1: 'Over 2.5 goals'}\n",
    "for idx, val in class_dist.items():\n",
    "    print(f\"{class_names[idx]}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2ce20",
   "metadata": {},
   "source": [
    "## #1: Baseline Model Training\n",
    "\n",
    "### #1.1: Train Baseline Random Forest\n",
    "Train an untuned Random Forest classifier with default parameters on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a623835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline Random Forest with default parameters\n",
    "baseline_rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training baseline Random Forest...\")\n",
    "baseline_rf.fit(X_train, y_train)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2cb51b",
   "metadata": {},
   "source": [
    "### #1.2: Evaluate Baseline Model on Validation Set\n",
    "Generate predictions and calculate performance metrics on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba2c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, dataset_name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Evaluate model and return comprehensive metrics.\n",
    "    \"\"\"\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_proba = model.predict_proba(X)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # ROC-AUC for binary classification\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y, y_pred_proba[:, 1])\n",
    "    except:\n",
    "        roc_auc = None\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Performance Metrics - {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    if roc_auc:\n",
    "        print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Classification Report\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y, y_pred, target_names=['Under 2.5', 'Over 2.5'], zero_division=0))\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'confusion_matrix': cm,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "\n",
    "# Evaluate on training set\n",
    "baseline_train_results = evaluate_model(baseline_rf, X_train, y_train, \"Baseline - Training Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e180dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "baseline_val_results = evaluate_model(baseline_rf, X_val, y_val, \"Baseline - Validation Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b354e6",
   "metadata": {},
   "source": [
    "### #1.3: Visualize Confusion Matrix\n",
    "Create a heatmap visualization of the confusion matrix for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7db4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix as a heatmap.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Under 2.5', 'Over 2.5'],\n",
    "                yticklabels=['Under 2.5', 'Over 2.5'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(baseline_val_results['confusion_matrix'], \n",
    "                     \"Baseline Model - Validation Set Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac7c0d8",
   "metadata": {},
   "source": [
    "## #2: Hyperparameter Tuning\n",
    "\n",
    "### #2.1: Bayesian Optimization with Optuna\n",
    "Use Optuna to perform Bayesian optimization to find optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857b7045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna hyperparameter optimization.\n",
    "    \"\"\"\n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Create study and optimize\n",
    "print(\"Starting Bayesian Optimization with Optuna...\")\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest trial:\")\n",
    "print(f\"  Value (F1-Score): {study.best_trial.value:.4f}\")\n",
    "print(f\"  Params: \")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab2b26c",
   "metadata": {},
   "source": [
    "### #2.2: Visualize Optuna Optimization History\n",
    "Plot the optimization history to understand how the search progressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca8edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot all trials\n",
    "trials_df = study.trials_dataframe()\n",
    "ax1.plot(trials_df['number'], trials_df['value'], 'b-', alpha=0.3, label='All trials')\n",
    "ax1.plot(trials_df['number'], trials_df['value'].cummax(), 'r-', linewidth=2, label='Best so far')\n",
    "ax1.set_xlabel('Trial Number')\n",
    "ax1.set_ylabel('F1-Score')\n",
    "ax1.set_title('Optuna Optimization History')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot top 10 trials\n",
    "top_trials = trials_df.nlargest(10, 'value')\n",
    "ax2.barh(range(len(top_trials)), top_trials['value'])\n",
    "ax2.set_yticks(range(len(top_trials)))\n",
    "ax2.set_yticklabels([f\"Trial {int(t)}\" for t in top_trials['number']])\n",
    "ax2.set_xlabel('F1-Score')\n",
    "ax2.set_title('Top 10 Trials')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0652b660",
   "metadata": {},
   "source": [
    "### #2.3: Grid Search Refinement\n",
    "Perform a focused grid search around the best parameters from Optuna to fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc29b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best parameters from Optuna\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "# Define refined grid around best parameters\n",
    "param_grid = {\n",
    "    'n_estimators': [max(100, best_params['n_estimators'] - 50), \n",
    "                     best_params['n_estimators'], \n",
    "                     best_params['n_estimators'] + 50],\n",
    "    'max_depth': [max(5, best_params['max_depth'] - 2), \n",
    "                  best_params['max_depth'], \n",
    "                  min(30, best_params['max_depth'] + 2)],\n",
    "    'min_samples_split': [max(2, best_params['min_samples_split'] - 2), \n",
    "                          best_params['min_samples_split'], \n",
    "                          best_params['min_samples_split'] + 2],\n",
    "    'min_samples_leaf': [max(1, best_params['min_samples_leaf'] - 1), \n",
    "                         best_params['min_samples_leaf'], \n",
    "                         best_params['min_samples_leaf'] + 1],\n",
    "    'max_features': [best_params['max_features']],\n",
    "    'bootstrap': [best_params['bootstrap']],\n",
    "    'random_state': [42],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "\n",
    "print(\"Performing Grid Search refinement...\")\n",
    "print(f\"Grid search space size: {np.prod([len(v) for v in param_grid.values() if len(v) > 1])} combinations\")\n",
    "\n",
    "# Combine train and validation for grid search (since we're using cv)\n",
    "X_train_val = pd.concat([X_train, X_val])\n",
    "y_train_val = pd.concat([pd.Series(y_train), pd.Series(y_val)]).values\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "print(f\"\\nBest Grid Search F1-Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Best Grid Search Parameters:\")\n",
    "for key, value in grid_search.best_params_.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198ff48a",
   "metadata": {},
   "source": [
    "### #2.4: Train Final Tuned Model\n",
    "Train the tuned Random Forest model with the best parameters from grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best model from grid search\n",
    "tuned_rf = grid_search.best_estimator_\n",
    "\n",
    "print(\"Tuned Random Forest model trained successfully!\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b384a40e",
   "metadata": {},
   "source": [
    "### #2.5: Compare Baseline vs Tuned Models\n",
    "Evaluate and compare performance of baseline and tuned models on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e36fc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned model on validation set\n",
    "tuned_val_results = evaluate_model(tuned_rf, X_val, y_val, \"Tuned - Validation Set\")\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n",
    "    'Baseline': [\n",
    "        baseline_val_results['accuracy'],\n",
    "        baseline_val_results['precision'],\n",
    "        baseline_val_results['recall'],\n",
    "        baseline_val_results['f1'],\n",
    "        baseline_val_results['roc_auc']\n",
    "    ],\n",
    "    'Tuned': [\n",
    "        tuned_val_results['accuracy'],\n",
    "        tuned_val_results['precision'],\n",
    "        tuned_val_results['recall'],\n",
    "        tuned_val_results['f1'],\n",
    "        tuned_val_results['roc_auc']\n",
    "    ]\n",
    "})\n",
    "\n",
    "comparison_df['Improvement'] = comparison_df['Tuned'] - comparison_df['Baseline']\n",
    "comparison_df['Improvement %'] = (comparison_df['Improvement'] / comparison_df['Baseline'] * 100).round(2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Baseline vs Tuned Model Comparison (Validation Set)\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2186abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Bar plot comparison\n",
    "metrics = comparison_df['Metric'].tolist()\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, comparison_df['Baseline'], width, label='Baseline', alpha=0.8)\n",
    "axes[0].bar(x + width/2, comparison_df['Tuned'], width, label='Tuned', alpha=0.8)\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Baseline vs Tuned Model Performance')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(metrics, rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Improvement percentage\n",
    "colors = ['green' if x > 0 else 'red' for x in comparison_df['Improvement %']]\n",
    "axes[1].barh(metrics, comparison_df['Improvement %'], color=colors, alpha=0.7)\n",
    "axes[1].set_xlabel('Improvement (%)')\n",
    "axes[1].set_title('Performance Improvement')\n",
    "axes[1].axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f63d742",
   "metadata": {},
   "source": [
    "## #3: Feature Importance Analysis\n",
    "\n",
    "### #3.1: Calculate and Visualize Feature Importance\n",
    "Analyze which features are most important for the tuned model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e1fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': tuned_rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "print(feature_importance.head(20).to_string(index=False))\n",
    "\n",
    "# Plot top 30 features\n",
    "plt.figure(figsize=(12, 10))\n",
    "top_n = 30\n",
    "top_features = feature_importance.head(top_n)\n",
    "plt.barh(range(len(top_features)), top_features['importance'], alpha=0.8)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title(f'Top {top_n} Feature Importances - Tuned Random Forest')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef118c9",
   "metadata": {},
   "source": [
    "### #3.2: Feature Importance Distribution Analysis\n",
    "Analyze the distribution of feature importances and cumulative importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f3c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative importance\n",
    "feature_importance['cumulative_importance'] = feature_importance['importance'].cumsum()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Importance distribution\n",
    "ax1.hist(feature_importance['importance'], bins=50, alpha=0.7, edgecolor='black')\n",
    "ax1.set_xlabel('Feature Importance')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of Feature Importances')\n",
    "ax1.axvline(feature_importance['importance'].mean(), color='red', linestyle='--', \n",
    "            linewidth=2, label=f'Mean: {feature_importance[\"importance\"].mean():.4f}')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative importance\n",
    "ax2.plot(range(len(feature_importance)), feature_importance['cumulative_importance'], 'b-', linewidth=2)\n",
    "ax2.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "ax2.axhline(y=0.90, color='orange', linestyle='--', label='90% threshold')\n",
    "ax2.set_xlabel('Number of Features')\n",
    "ax2.set_ylabel('Cumulative Importance')\n",
    "ax2.set_title('Cumulative Feature Importance')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Report how many features capture 90% and 95% of importance\n",
    "n_features_90 = (feature_importance['cumulative_importance'] <= 0.90).sum() + 1\n",
    "n_features_95 = (feature_importance['cumulative_importance'] <= 0.95).sum() + 1\n",
    "\n",
    "print(f\"\\nFeature Importance Summary:\")\n",
    "print(f\"  Total features: {len(feature_importance)}\")\n",
    "print(f\"  Features capturing 90% importance: {n_features_90} ({n_features_90/len(feature_importance)*100:.1f}%)\")\n",
    "print(f\"  Features capturing 95% importance: {n_features_95} ({n_features_95/len(feature_importance)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb99c1",
   "metadata": {},
   "source": [
    "## #4: Learning Curves\n",
    "\n",
    "### #4.1: Generate Learning Curves for Both Models\n",
    "Plot learning curves to analyze training vs validation performance and identify overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96380185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(model, X, y, title, cv=5):\n",
    "    \"\"\"\n",
    "    Generate and plot learning curves for a model.\n",
    "    \"\"\"\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X, y, \n",
    "        cv=cv,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    val_scores_mean = np.mean(val_scores, axis=1)\n",
    "    val_scores_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"F1-Score\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return train_scores_mean, val_scores_mean\n",
    "\n",
    "# Generate learning curves for baseline model\n",
    "print(\"Generating learning curves for baseline model...\")\n",
    "baseline_train_lc, baseline_val_lc = plot_learning_curve(\n",
    "    baseline_rf, X_train_val, y_train_val, \n",
    "    \"Learning Curve - Baseline Random Forest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42793a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate learning curves for tuned model\n",
    "print(\"Generating learning curves for tuned model...\")\n",
    "tuned_train_lc, tuned_val_lc = plot_learning_curve(\n",
    "    tuned_rf, X_train_val, y_train_val, \n",
    "    \"Learning Curve - Tuned Random Forest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8230690c",
   "metadata": {},
   "source": [
    "## #5: Final Evaluation on Test Set\n",
    "\n",
    "### #5.1: Evaluate Baseline Model on Test Set\n",
    "Test the baseline model on the held-out test set to get final performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9bebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline model on test set\n",
    "baseline_test_results = evaluate_model(baseline_rf, X_test, y_test, \"Baseline - Test Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887e40f",
   "metadata": {},
   "source": [
    "### #5.2: Evaluate Tuned Model on Test Set\n",
    "Test the tuned model on the held-out test set to get final performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7502b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned model on test set\n",
    "tuned_test_results = evaluate_model(tuned_rf, X_test, y_test, \"Tuned - Test Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151aadc3",
   "metadata": {},
   "source": [
    "### #5.3: Final Comparison - Test Set Performance\n",
    "Compare baseline and tuned model performance on the test set with visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25944a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison for test set\n",
    "test_comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n",
    "    'Baseline': [\n",
    "        baseline_test_results['accuracy'],\n",
    "        baseline_test_results['precision'],\n",
    "        baseline_test_results['recall'],\n",
    "        baseline_test_results['f1'],\n",
    "        baseline_test_results['roc_auc']\n",
    "    ],\n",
    "    'Tuned': [\n",
    "        tuned_test_results['accuracy'],\n",
    "        tuned_test_results['precision'],\n",
    "        tuned_test_results['recall'],\n",
    "        tuned_test_results['f1'],\n",
    "        tuned_test_results['roc_auc']\n",
    "    ]\n",
    "})\n",
    "\n",
    "test_comparison_df['Improvement'] = test_comparison_df['Tuned'] - test_comparison_df['Baseline']\n",
    "test_comparison_df['Improvement %'] = (test_comparison_df['Improvement'] / test_comparison_df['Baseline'] * 100).round(2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL COMPARISON: Baseline vs Tuned Model (Test Set)\")\n",
    "print(\"=\"*80)\n",
    "print(test_comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13fc21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test set comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Metrics comparison bar plot\n",
    "metrics = test_comparison_df['Metric'].tolist()\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, test_comparison_df['Baseline'], width, label='Baseline', alpha=0.8)\n",
    "axes[0, 0].bar(x + width/2, test_comparison_df['Tuned'], width, label='Tuned', alpha=0.8)\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].set_title('Test Set: Baseline vs Tuned Model Performance')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(metrics, rotation=45, ha='right')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. Improvement percentage\n",
    "colors = ['green' if x > 0 else 'red' for x in test_comparison_df['Improvement %']]\n",
    "axes[0, 1].barh(metrics, test_comparison_df['Improvement %'], color=colors, alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Improvement (%)')\n",
    "axes[0, 1].set_title('Test Set: Performance Improvement')\n",
    "axes[0, 1].axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Baseline confusion matrix\n",
    "sns.heatmap(baseline_test_results['confusion_matrix'], annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Under 2.5', 'Over 2.5'], yticklabels=['Under 2.5', 'Over 2.5'],\n",
    "            ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Baseline Model - Test Set Confusion Matrix')\n",
    "axes[1, 0].set_ylabel('True Label')\n",
    "axes[1, 0].set_xlabel('Predicted Label')\n",
    "\n",
    "# 4. Tuned confusion matrix\n",
    "sns.heatmap(tuned_test_results['confusion_matrix'], annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Under 2.5', 'Over 2.5'], yticklabels=['Under 2.5', 'Over 2.5'],\n",
    "            ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Tuned Model - Test Set Confusion Matrix')\n",
    "axes[1, 1].set_ylabel('True Label')\n",
    "axes[1, 1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6027e5c",
   "metadata": {},
   "source": [
    "### #5.4: Performance Summary Across All Sets\n",
    "Compare model performance across training, validation, and test sets to check for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b8bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary across all datasets\n",
    "summary_data = {\n",
    "    'Dataset': ['Training', 'Validation', 'Test'] * 2,\n",
    "    'Model': ['Baseline'] * 3 + ['Tuned'] * 3,\n",
    "    'Accuracy': [\n",
    "        baseline_train_results['accuracy'],\n",
    "        baseline_val_results['accuracy'],\n",
    "        baseline_test_results['accuracy'],\n",
    "        None,  # Tuned model not trained on original train set\n",
    "        tuned_val_results['accuracy'],\n",
    "        tuned_test_results['accuracy']\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        baseline_train_results['f1'],\n",
    "        baseline_val_results['f1'],\n",
    "        baseline_test_results['f1'],\n",
    "        None,\n",
    "        tuned_val_results['f1'],\n",
    "        tuned_test_results['f1']\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        baseline_train_results['roc_auc'],\n",
    "        baseline_val_results['roc_auc'],\n",
    "        baseline_test_results['roc_auc'],\n",
    "        None,\n",
    "        tuned_val_results['roc_auc'],\n",
    "        tuned_test_results['roc_auc']\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE SUMMARY ACROSS ALL DATASETS\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Plot performance across datasets\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# F1-Score across datasets\n",
    "baseline_f1 = [baseline_train_results['f1'], baseline_val_results['f1'], baseline_test_results['f1']]\n",
    "tuned_f1 = [None, tuned_val_results['f1'], tuned_test_results['f1']]\n",
    "datasets = ['Training', 'Validation', 'Test']\n",
    "\n",
    "ax1.plot(datasets, baseline_f1, 'o-', linewidth=2, markersize=8, label='Baseline', color='blue')\n",
    "ax1.plot(datasets[1:], [f for f in tuned_f1 if f is not None], 'o-', linewidth=2, markersize=8, label='Tuned', color='green')\n",
    "ax1.set_ylabel('F1-Score')\n",
    "ax1.set_title('F1-Score Across Datasets')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy across datasets\n",
    "baseline_acc = [baseline_train_results['accuracy'], baseline_val_results['accuracy'], baseline_test_results['accuracy']]\n",
    "tuned_acc = [None, tuned_val_results['accuracy'], tuned_test_results['accuracy']]\n",
    "\n",
    "ax2.plot(datasets, baseline_acc, 'o-', linewidth=2, markersize=8, label='Baseline', color='blue')\n",
    "ax2.plot(datasets[1:], [a for a in tuned_acc if a is not None], 'o-', linewidth=2, markersize=8, label='Tuned', color='green')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Across Datasets')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a77f4",
   "metadata": {},
   "source": [
    "### #5.5: Save Best Model and Results\n",
    "Save the tuned model and best parameters for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e9f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tuned model\n",
    "import joblib\n",
    "\n",
    "model_save_path = './processed/tuned_random_forest_model.pkl'\n",
    "joblib.dump(tuned_rf, model_save_path)\n",
    "print(f\"Tuned model saved to: {model_save_path}\")\n",
    "\n",
    "# Save the best parameters\n",
    "params_save_path = './processed/best_rf_parameters.pkl'\n",
    "best_params_final = grid_search.best_params_\n",
    "joblib.dump(best_params_final, params_save_path)\n",
    "print(f\"Best parameters saved to: {params_save_path}\")\n",
    "\n",
    "# Save all results\n",
    "results_save_path = './processed/baseline_rf_results.pkl'\n",
    "results_dict = {\n",
    "    'baseline_val': baseline_val_results,\n",
    "    'baseline_test': baseline_test_results,\n",
    "    'tuned_val': tuned_val_results,\n",
    "    'tuned_test': tuned_test_results,\n",
    "    'best_params': best_params_final,\n",
    "    'feature_importance': feature_importance,\n",
    "    'optuna_study': study\n",
    "}\n",
    "joblib.dump(results_dict, results_save_path)\n",
    "print(f\"All results saved to: {results_save_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST MODEL CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "for key, value in best_params_final.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c04a381",
   "metadata": {},
   "source": [
    "### #5.6: Final Summary and Conclusions\n",
    "Display key findings and insights from the baseline Random Forest analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b346ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE RANDOM FOREST - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n1. MODEL PERFORMANCE ON TEST SET:\")\n",
    "print(f\"   Baseline Model:\")\n",
    "print(f\"     - Accuracy:  {baseline_test_results['accuracy']:.4f}\")\n",
    "print(f\"     - F1-Score:  {baseline_test_results['f1']:.4f}\")\n",
    "print(f\"     - ROC-AUC:   {baseline_test_results['roc_auc']:.4f}\")\n",
    "print(f\"\\n   Tuned Model:\")\n",
    "print(f\"     - Accuracy:  {tuned_test_results['accuracy']:.4f}\")\n",
    "print(f\"     - F1-Score:  {tuned_test_results['f1']:.4f}\")\n",
    "print(f\"     - ROC-AUC:   {tuned_test_results['roc_auc']:.4f}\")\n",
    "\n",
    "improvement_f1 = ((tuned_test_results['f1'] - baseline_test_results['f1']) / baseline_test_results['f1'] * 100)\n",
    "improvement_acc = ((tuned_test_results['accuracy'] - baseline_test_results['accuracy']) / baseline_test_results['accuracy'] * 100)\n",
    "\n",
    "print(f\"\\n2. IMPROVEMENT THROUGH HYPERPARAMETER TUNING:\")\n",
    "print(f\"   - Accuracy improved by:  {improvement_acc:+.2f}%\")\n",
    "print(f\"   - F1-Score improved by:  {improvement_f1:+.2f}%\")\n",
    "\n",
    "print(f\"\\n3. TOP 5 MOST IMPORTANT FEATURES:\")\n",
    "for idx, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"   {idx+1}. {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\\n4. HYPERPARAMETER OPTIMIZATION:\")\n",
    "print(f\"   - Optuna trials: 50\")\n",
    "print(f\"   - Best Optuna F1-Score: {study.best_trial.value:.4f}\")\n",
    "print(f\"   - Grid search refinement completed\")\n",
    "print(f\"   - Final model trained on combined train+val set\")\n",
    "\n",
    "print(f\"\\n5. MODEL SAVED:\")\n",
    "print(f\"   - Model: {model_save_path}\")\n",
    "print(f\"   - Parameters: {params_save_path}\")\n",
    "print(f\"   - Results: {results_save_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Analysis Complete!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
