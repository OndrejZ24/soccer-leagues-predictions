{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e467abf",
   "metadata": {},
   "source": [
    "# Baseline Random Forest Classifier\n",
    "\n",
    "This notebook builds and evaluates a baseline Random Forest classifier for soccer match prediction: Over/Under 2.5 goals (binary classification).\n",
    "\n",
    "## Structure:\n",
    "- **#0**: Setup and Data Loading\n",
    "- **#1**: Baseline Model Training\n",
    "- **#2**: Hyperparameter Tuning\n",
    "- **#3**: Feature Importance Analysis\n",
    "- **#4**: Learning Curves\n",
    "- **#5**: Final Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2f7bf8",
   "metadata": {},
   "source": [
    "## #0: Setup and Data Loading\n",
    "\n",
    "### #0.1: Import Required Libraries\n",
    "Import all necessary libraries for model training, evaluation, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3ee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                             roc_auc_score, confusion_matrix, classification_report)\n",
    "from sklearn.model_selection import learning_curve,  cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59d6a55",
   "metadata": {},
   "source": [
    "### #0.2: Load Preprocessed Data\n",
    "Load the baseline preprocessed data containing training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421c1678",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./processed/baseline_preprocessed.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_val = data['X_val']\n",
    "y_val = data['y_val']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc7550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data shapes\n",
    "print(f\"Training set: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Validation set: X={X_val.shape}, y={y_val.shape}\")\n",
    "print(f\"Test set: X={X_test.shape}, y={y_test.shape}\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "class_dist = pd.Series(y_train).value_counts(normalize=True).sort_index()\n",
    "class_names = {0: 'Under 2.5 goals', 1: 'Over 2.5 goals'}\n",
    "for idx, val in class_dist.items():\n",
    "    print(f\"{class_names[idx]}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2ce20",
   "metadata": {},
   "source": [
    "## #1: Baseline Model Training\n",
    "\n",
    "### #1.1: Train Baseline Random Forest\n",
    "Train an untuned Random Forest classifier with default parameters on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a623835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline Random Forest with default parameters\n",
    "baseline_rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training baseline Random Forest...\")\n",
    "baseline_rf.fit(X_train, y_train)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2cb51b",
   "metadata": {},
   "source": [
    "### #1.2: Evaluate Baseline Model on Validation Set\n",
    "Generate predictions and calculate performance metrics on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba2c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, dataset_name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Evaluate model and return comprehensive metrics.\n",
    "    \"\"\"\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_proba = model.predict_proba(X)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # ROC-AUC for binary classification\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y, y_pred_proba[:, 1])\n",
    "    except:\n",
    "        roc_auc = None\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Performance Metrics - {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    if roc_auc:\n",
    "        print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Classification Report\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y, y_pred, target_names=['Under 2.5', 'Over 2.5'], zero_division=0))\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'confusion_matrix': cm,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e180dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "baseline_val_results = evaluate_model(baseline_rf, X_val, y_val, \"Baseline - Validation Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b354e6",
   "metadata": {},
   "source": [
    "### #1.3: Visualize Confusion Matrix\n",
    "Create a heatmap visualization of the confusion matrix for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7db4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix as a heatmap.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Under 2.5', 'Over 2.5'],\n",
    "                yticklabels=['Under 2.5', 'Over 2.5'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(baseline_val_results['confusion_matrix'], \n",
    "                     \"Baseline Model - Validation Set Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac7c0d8",
   "metadata": {},
   "source": [
    "## #2: Hyperparameter Tuning\n",
    "\n",
    "### #2.1: Bayesian Optimization with Optuna\n",
    "Use Optuna to perform Bayesian optimization to find optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857b7045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna hyperparameter optimization.\n",
    "    Uses cross-validation on training set to avoid leakage.\n",
    "    Returns: ROC-AUC score to maximize\n",
    "    \"\"\"\n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    # Create model with suggested hyperparameters\n",
    "    model = RandomForestClassifier(**params)\n",
    "    \n",
    "    # Use cross-validation on training set to evaluate performance\n",
    "    # This avoids leakage - we don't touch validation set during optimization\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train, y_train, \n",
    "        cv=cv, \n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Return mean ROC-AUC across folds\n",
    "    return cv_scores.mean()\n",
    "\n",
    "# Create study and optimize\n",
    "print(\"Starting Bayesian Optimization with Optuna...\")\n",
    "print(\"Using 5-fold cross-validation on training set to avoid leakage...\")\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=TPESampler(seed=42),\n",
    "    study_name='random_forest_optuna'\n",
    ")\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Calculate optimization time\n",
    "optimization_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nOptimization completed in {optimization_time:.1f} seconds\")\n",
    "print(f\"Best ROC-AUC (CV): {study.best_value:.6f}\")\n",
    "print(f\"Best trial: #{study.best_trial.number}\")\n",
    "print(f\"\\nBest hyperparameters:\")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab2b26c",
   "metadata": {},
   "source": [
    "### #2.2: Manual Bayesian Optimization with Gaussian Processes\n",
    "Implement Bayesian optimization from scratch using Gaussian Processes as surrogate model and UCB acquisition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f1c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "#  Define objective function for manual BO\n",
    "# --------------------------------------------\n",
    "def evaluate_rf_manual(params):\n",
    "    \"\"\"\n",
    "    Evaluate Random Forest with given hyperparameters using cross-validation.\n",
    "    Returns: 1 - ROC-AUC (we minimize, so we negate the metric we want to maximize)\n",
    "    \"\"\"\n",
    "    # Handle max_features categorical parameter (encoded as continuous for GP)\n",
    "    max_features_map = {0: 'sqrt', 1: 'log2', 2: None}\n",
    "    max_features_idx = int(round(params[4]))\n",
    "    max_features_idx = np.clip(max_features_idx, 0, 2)  # Ensure valid range\n",
    "    max_features_val = max_features_map[max_features_idx]\n",
    "    \n",
    "    # Handle bootstrap boolean (encoded as continuous for GP)\n",
    "    bootstrap_val = bool(params[5] > 0.5)\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=int(params[0]),\n",
    "        max_depth=int(params[1]),\n",
    "        min_samples_split=int(params[2]),\n",
    "        min_samples_leaf=int(params[3]),\n",
    "        max_features=max_features_val,\n",
    "        bootstrap=bootstrap_val,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Use cross-validation for evaluation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train, y_train,\n",
    "        cv=cv,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    auc = cv_scores.mean()\n",
    "    # Return 1 - AUC because we minimize (GP convention)\n",
    "    return 1 - auc\n",
    "\n",
    "# --------------------------------------------\n",
    "#  Define search space bounds\n",
    "# --------------------------------------------\n",
    "param_bounds = np.array([\n",
    "    [100, 500],      # n_estimators: 100-500\n",
    "    [5, 30],         # max_depth: 5-30\n",
    "    [2, 20],         # min_samples_split: 2-20\n",
    "    [1, 10],         # min_samples_leaf: 1-10\n",
    "    [0, 2],          # max_features: 0='sqrt', 1='log2', 2=None\n",
    "    [0, 1],          # bootstrap: 0=False, 1=True (continuous for GP)\n",
    "])\n",
    "\n",
    "n_params = param_bounds.shape[0]\n",
    "\n",
    "# --------------------------------------------\n",
    "#  Initialize samples (random warm-up phase)\n",
    "# --------------------------------------------\n",
    "np.random.seed(42)\n",
    "N_INIT = 10  # Initial random samples to warm up the GP\n",
    "\n",
    "print(\"üîß Initializing Manual Bayesian Optimization...\")\n",
    "print(f\"Generating {N_INIT} random initial samples...\")\n",
    "\n",
    "X_samples = np.random.uniform(param_bounds[:, 0], param_bounds[:, 1], size=(N_INIT, n_params))\n",
    "Y_samples = np.array([evaluate_rf_manual(x) for x in X_samples])\n",
    "\n",
    "print(f\"Initial samples generated. Best initial AUC: {1 - np.min(Y_samples):.6f}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "#  Fit Gaussian Process surrogate model\n",
    "# --------------------------------------------\n",
    "kernel = Matern(length_scale=1.0, nu=2.5)\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    normalize_y=True,\n",
    "    n_restarts_optimizer=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Normalize features for better GP performance\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_samples)\n",
    "gp.fit(X_scaled, Y_samples)\n",
    "\n",
    "# --------------------------------------------\n",
    "#  Acquisition function (Upper Confidence Bound)\n",
    "# --------------------------------------------\n",
    "def acquisition_ucb(X, model, kappa=2.0):\n",
    "    \"\"\"\n",
    "    Upper Confidence Bound acquisition function.\n",
    "    Since we're minimizing (1 - AUC), we use LCB which is equivalent to UCB for maximization.\n",
    "    \n",
    "    LCB = mu - kappa * sigma (for minimization)\n",
    "    \"\"\"\n",
    "    mu, sigma = model.predict(X, return_std=True)\n",
    "    return mu - kappa * sigma  # Lower is better for minimization\n",
    "\n",
    "def propose_next(model, bounds, scaler, n_candidates=2000):\n",
    "    \"\"\"Propose next point to evaluate using acquisition function\"\"\"\n",
    "    candidates = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_candidates, bounds.shape[0]))\n",
    "    candidates_scaled = scaler.transform(candidates)\n",
    "    acq_vals = acquisition_ucb(candidates_scaled, model)\n",
    "    best_idx = np.argmin(acq_vals)\n",
    "    return candidates[best_idx].reshape(1, -1)\n",
    "\n",
    "# --------------------------------------------\n",
    "#  Bayesian Optimization loop\n",
    "# --------------------------------------------\n",
    "N_ITER = 40  # Number of BO iterations (50 total evaluations = 10 init + 40 iterations)\n",
    "history_best = [np.min(Y_samples)]\n",
    "\n",
    "print(f\"\\nüöÄ Starting Bayesian Optimization loop with {N_ITER} iterations...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(N_ITER):\n",
    "    # Propose next point using acquisition function\n",
    "    x_next = propose_next(gp, param_bounds, scaler)\n",
    "    y_next = evaluate_rf_manual(x_next[0])\n",
    "    \n",
    "    # Update dataset\n",
    "    X_samples = np.vstack([X_samples, x_next])\n",
    "    Y_samples = np.hstack([Y_samples, y_next])\n",
    "    \n",
    "    # Refit Gaussian Process\n",
    "    X_scaled = scaler.fit_transform(X_samples)\n",
    "    gp.fit(X_scaled, Y_samples)\n",
    "    \n",
    "    # Track progress\n",
    "    history_best.append(np.min(Y_samples))\n",
    "    current_best_auc = 1 - np.min(Y_samples)\n",
    "    current_auc = 1 - y_next\n",
    "    \n",
    "    # Print every iteration to see progress\n",
    "    if (i + 1) % 5 == 0 or i < 5:\n",
    "        print(f\"Iteration {i+1:02d}/{N_ITER}: Current AUC = {current_auc:.6f}, Best AUC = {current_best_auc:.6f}\")\n",
    "\n",
    "optimization_time = time.time() - start_time\n",
    "\n",
    "# --------------------------------------------\n",
    "#  Extract best parameters\n",
    "# --------------------------------------------\n",
    "best_idx = np.argmin(Y_samples)\n",
    "best_params_array = X_samples[best_idx]\n",
    "best_auc_manual = 1 - Y_samples[best_idx]\n",
    "\n",
    "# Convert to dictionary format\n",
    "max_features_map = {0: 'sqrt', 1: 'log2', 2: None}\n",
    "manual_bo_params = {\n",
    "    'n_estimators': int(best_params_array[0]),\n",
    "    'max_depth': int(best_params_array[1]),\n",
    "    'min_samples_split': int(best_params_array[2]),\n",
    "    'min_samples_leaf': int(best_params_array[3]),\n",
    "    'max_features': max_features_map[int(round(np.clip(best_params_array[4], 0, 2)))],\n",
    "    'bootstrap': bool(best_params_array[5] > 0.5),\n",
    "}\n",
    "\n",
    "print(f\"\\nManual Bayesian Optimization completed in {optimization_time:.1f} seconds\")\n",
    "print(f\"Best ROC-AUC (CV): {best_auc_manual:.6f}\")\n",
    "print(f\"Total evaluations: {len(Y_samples)}\")\n",
    "print(f\"\\nBest hyperparameters found:\")\n",
    "for name, val in manual_bo_params.items():\n",
    "    print(f\"  {name}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7022d0e",
   "metadata": {},
   "source": [
    "### #2.3: Compare Optimization Methods\n",
    "Compare the performance of Optuna vs Manual Bayesian Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc01c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Optuna and Manual BO results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPERPARAMETER OPTIMIZATION COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüîπ Optuna Optimization:\")\n",
    "print(f\"   Best CV ROC-AUC: {study.best_value:.6f}\")\n",
    "print(f\"   Total evaluations: {len(study.trials)}\")\n",
    "\n",
    "print(f\"\\nüîπ Manual Bayesian Optimization:\")\n",
    "print(f\"   Best CV ROC-AUC: {best_auc_manual:.6f}\")\n",
    "print(f\"   Total evaluations: {len(Y_samples)}\")\n",
    "\n",
    "print(f\"\\nüîπ Difference:\")\n",
    "diff = study.best_value - best_auc_manual\n",
    "print(f\"   Optuna - Manual BO: {diff:+.6f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Visualize optimization convergence\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Optuna optimization history\n",
    "trials_df = study.trials_dataframe()\n",
    "axes[0].plot(trials_df['number'], trials_df['value'], 'b-', alpha=0.3, label='All trials')\n",
    "axes[0].plot(trials_df['number'], trials_df['value'].cummax(), 'r-', linewidth=2, label='Best so far')\n",
    "axes[0].set_xlabel('Trial Number')\n",
    "axes[0].set_ylabel('CV ROC-AUC')\n",
    "axes[0].set_title('Optuna Optimization (TPE Sampler)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Manual BO convergence\n",
    "manual_best_curve = [1 - val for val in history_best]  # Convert back to AUC\n",
    "axes[1].plot(range(len(manual_best_curve)), manual_best_curve, 'g-', linewidth=2, label='Best so far')\n",
    "axes[1].axhline(y=manual_best_curve[-1], color='r', linestyle='--', alpha=0.5, label=f'Final best: {manual_best_curve[-1]:.6f}')\n",
    "axes[1].set_xlabel('Iteration Number')\n",
    "axes[1].set_ylabel('Best CV ROC-AUC')\n",
    "axes[1].set_title('Manual Bayesian Optimization (GP + UCB)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare parameter distributions\n",
    "print(\"\\nüìä Best Hyperparameters Comparison:\")\n",
    "print(f\"{'Parameter':<20} {'Optuna':<20} {'Manual BO':<20}\")\n",
    "print(\"-\" * 60)\n",
    "for param in ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'max_features', 'bootstrap']:\n",
    "    optuna_val = study.best_params.get(param, 'N/A')\n",
    "    manual_val = manual_bo_params.get(param, 'N/A')\n",
    "    print(f\"{param:<20} {str(optuna_val):<20} {str(manual_val):<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198ff48a",
   "metadata": {},
   "source": [
    "### #2.4: Train Optimized Models\n",
    "Train models with best parameters from both Optuna and Manual BO, then evaluate on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with Optuna best parameters\n",
    "print(\"üîπ Training model with Optuna best parameters...\")\n",
    "rf_optuna = RandomForestClassifier(\n",
    "    **study.best_params,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_optuna.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_val_pred_optuna = rf_optuna.predict(X_val)\n",
    "y_val_pred_proba_optuna = rf_optuna.predict_proba(X_val)[:, 1]\n",
    "val_auc_optuna = roc_auc_score(y_val, y_val_pred_proba_optuna)\n",
    "print(f\"   Validation ROC-AUC: {val_auc_optuna:.6f}\")\n",
    "\n",
    "# Train model with Manual BO best parameters\n",
    "print(\"\\nüîπ Training model with Manual BO best parameters...\")\n",
    "rf_manual = RandomForestClassifier(\n",
    "    **manual_bo_params,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_manual.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_val_pred_manual = rf_manual.predict(X_val)\n",
    "y_val_pred_proba_manual = rf_manual.predict_proba(X_val)[:, 1]\n",
    "val_auc_manual = roc_auc_score(y_val, y_val_pred_proba_manual)\n",
    "print(f\"   Validation ROC-AUC: {val_auc_manual:.6f}\")\n",
    "\n",
    "print(\"\\nüìä Validation Set Results:\")\n",
    "print(f\"{'Model':<25} {'CV ROC-AUC':<15} {'Val ROC-AUC':<15}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Baseline RF':<25} {'N/A':<15} {baseline_val_results['roc_auc']:<15.6f}\")\n",
    "print(f\"{'Optuna Optimized':<25} {study.best_value:<15.6f} {val_auc_optuna:<15.6f}\")\n",
    "print(f\"{'Manual BO Optimized':<25} {best_auc_manual:<15.6f} {val_auc_manual:<15.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b384a40e",
   "metadata": {},
   "source": [
    "### #2.5: Compare All Models on Validation Set\n",
    "Comprehensive comparison of Baseline vs Optuna vs Manual BO models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e36fc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models comprehensively\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON - VALIDATION SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Optuna model\n",
    "optuna_val_results = evaluate_model(rf_optuna, X_val, y_val, \"Optuna Optimized - Validation\")\n",
    "\n",
    "# Manual BO model\n",
    "manual_val_results = evaluate_model(rf_manual, X_val, y_val, \"Manual BO Optimized - Validation\")\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n",
    "    'Baseline': [\n",
    "        baseline_val_results['accuracy'],\n",
    "        baseline_val_results['precision'],\n",
    "        baseline_val_results['recall'],\n",
    "        baseline_val_results['f1'],\n",
    "        baseline_val_results['roc_auc']\n",
    "    ],\n",
    "    'Optuna': [\n",
    "        optuna_val_results['accuracy'],\n",
    "        optuna_val_results['precision'],\n",
    "        optuna_val_results['recall'],\n",
    "        optuna_val_results['f1'],\n",
    "        optuna_val_results['roc_auc']\n",
    "    ],\n",
    "    'Manual BO': [\n",
    "        manual_val_results['accuracy'],\n",
    "        manual_val_results['precision'],\n",
    "        manual_val_results['recall'],\n",
    "        manual_val_results['f1'],\n",
    "        manual_val_results['roc_auc']\n",
    "    ]\n",
    "})\n",
    "\n",
    "comparison_df['Optuna Improvement (%)'] = ((comparison_df['Optuna'] - comparison_df['Baseline']) / comparison_df['Baseline'] * 100).round(2)\n",
    "comparison_df['Manual BO Improvement (%)'] = ((comparison_df['Manual BO'] - comparison_df['Baseline']) / comparison_df['Baseline'] * 100).round(2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Baseline vs Optimized Models Comparison (Validation Set)\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2186abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison of all three models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Bar plot comparison\n",
    "metrics = comparison_df['Metric'].tolist()\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "axes[0].bar(x - width, comparison_df['Baseline'], width, label='Baseline', alpha=0.8, color='blue')\n",
    "axes[0].bar(x, comparison_df['Optuna'], width, label='Optuna', alpha=0.8, color='green')\n",
    "axes[0].bar(x + width, comparison_df['Manual BO'], width, label='Manual BO', alpha=0.8, color='orange')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Model Performance Comparison (Validation Set)')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(metrics, rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Improvement percentage comparison\n",
    "improvement_data = pd.DataFrame({\n",
    "    'Metric': metrics,\n",
    "    'Optuna': comparison_df['Optuna Improvement (%)'],\n",
    "    'Manual BO': comparison_df['Manual BO Improvement (%)']\n",
    "})\n",
    "\n",
    "x_pos = np.arange(len(metrics))\n",
    "axes[1].barh(x_pos - 0.2, improvement_data['Optuna'], 0.4, label='Optuna', alpha=0.8, color='green')\n",
    "axes[1].barh(x_pos + 0.2, improvement_data['Manual BO'], 0.4, label='Manual BO', alpha=0.8, color='orange')\n",
    "axes[1].set_yticks(x_pos)\n",
    "axes[1].set_yticklabels(metrics)\n",
    "axes[1].set_xlabel('Improvement over Baseline (%)')\n",
    "axes[1].set_title('Performance Improvement vs Baseline')\n",
    "axes[1].axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f63d742",
   "metadata": {},
   "source": [
    "## #3: Feature Importance Analysis\n",
    "\n",
    "### #3.1: Calculate and Visualize Feature Importance\n",
    "Analyze which features are most important for the tuned model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e1fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature importances across all three models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "top_n = 15\n",
    "\n",
    "# Baseline model\n",
    "feature_importance_baseline = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': baseline_rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "axes[0].barh(range(top_n), feature_importance_baseline.head(top_n)['importance'], alpha=0.8, color='blue')\n",
    "axes[0].set_yticks(range(top_n))\n",
    "axes[0].set_yticklabels(feature_importance_baseline.head(top_n)['feature'])\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title(f'Baseline RF - Top {top_n} Features')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Optuna model\n",
    "feature_importance_optuna = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_optuna.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "axes[1].barh(range(top_n), feature_importance_optuna.head(top_n)['importance'], alpha=0.8, color='green')\n",
    "axes[1].set_yticks(range(top_n))\n",
    "axes[1].set_yticklabels(feature_importance_optuna.head(top_n)['feature'])\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title(f'Optuna RF - Top {top_n} Features')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Manual BO model\n",
    "feature_importance_manual = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_manual.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "axes[2].barh(range(top_n), feature_importance_manual.head(top_n)['importance'], alpha=0.8, color='orange')\n",
    "axes[2].set_yticks(range(top_n))\n",
    "axes[2].set_yticklabels(feature_importance_manual.head(top_n)['feature'])\n",
    "axes[2].set_xlabel('Importance')\n",
    "axes[2].set_title(f'Manual BO RF - Top {top_n} Features')\n",
    "axes[2].invert_yaxis()\n",
    "axes[2].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top features for best model (let's say Optuna for now, but compare both)\n",
    "print(\"\\nTop 10 Most Important Features (Optuna Model):\")\n",
    "print(feature_importance_optuna.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef118c9",
   "metadata": {},
   "source": [
    "### #3.2: Feature Importance Distribution Analysis\n",
    "Analyze the distribution of feature importances and cumulative importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f3c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Optuna model for detailed analysis (best performing)\n",
    "feature_importance = feature_importance_optuna.copy()\n",
    "\n",
    "# Calculate cumulative importance\n",
    "feature_importance['cumulative_importance'] = feature_importance['importance'].cumsum()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Importance distribution\n",
    "ax1.hist(feature_importance['importance'], bins=50, alpha=0.7, edgecolor='black')\n",
    "ax1.set_xlabel('Feature Importance')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of Feature Importances (Optuna Model)')\n",
    "ax1.axvline(feature_importance['importance'].mean(), color='red', linestyle='--', \n",
    "            linewidth=2, label=f'Mean: {feature_importance[\"importance\"].mean():.4f}')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative importance\n",
    "ax2.plot(range(len(feature_importance)), feature_importance['cumulative_importance'], 'b-', linewidth=2)\n",
    "ax2.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "ax2.axhline(y=0.90, color='orange', linestyle='--', label='90% threshold')\n",
    "ax2.set_xlabel('Number of Features')\n",
    "ax2.set_ylabel('Cumulative Importance')\n",
    "ax2.set_title('Cumulative Feature Importance (Optuna Model)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Report how many features capture 90% and 95% of importance\n",
    "n_features_90 = (feature_importance['cumulative_importance'] <= 0.90).sum() + 1\n",
    "n_features_95 = (feature_importance['cumulative_importance'] <= 0.95).sum() + 1\n",
    "\n",
    "print(f\"\\nFeature Importance Summary (Optuna Model):\")\n",
    "print(f\"  Total features: {len(feature_importance)}\")\n",
    "print(f\"  Features capturing 90% importance: {n_features_90} ({n_features_90/len(feature_importance)*100:.1f}%)\")\n",
    "print(f\"  Features capturing 95% importance: {n_features_95} ({n_features_95/len(feature_importance)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb99c1",
   "metadata": {},
   "source": [
    "## #4: Learning Curves\n",
    "\n",
    "### #4.1: Generate Learning Curves for All Models\n",
    "Plot learning curves to analyze training vs validation performance and identify overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96380185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(model, X, y, title, cv=5):\n",
    "    \"\"\"\n",
    "    Generate and plot learning curves for a model.\n",
    "    \"\"\"\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X, y, \n",
    "        cv=cv,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    val_scores_mean = np.mean(val_scores, axis=1)\n",
    "    val_scores_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"F1-Score\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return train_scores_mean, val_scores_mean\n",
    "\n",
    "# Combine training and validation sets for learning curves\n",
    "X_train_val = pd.concat([X_train, X_val])\n",
    "y_train_val = pd.concat([y_train, y_val])\n",
    "\n",
    "# Generate learning curves for baseline model\n",
    "print(\"Generating learning curves for baseline model...\")\n",
    "baseline_train_lc, baseline_val_lc = plot_learning_curve(\n",
    "    baseline_rf, X_train_val, y_train_val, \n",
    "    \"Learning Curve - Baseline Random Forest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42793a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate learning curves for Optuna model\n",
    "print(\"Generating learning curves for Optuna optimized model...\")\n",
    "optuna_train_lc, optuna_val_lc = plot_learning_curve(\n",
    "    rf_optuna, X_train_val, y_train_val, \n",
    "    \"Learning Curve - Optuna Optimized Random Forest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8820dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate learning curves for Manual BO model\n",
    "print(\"Generating learning curves for Manual BO optimized model...\")\n",
    "manual_train_lc, manual_val_lc = plot_learning_curve(\n",
    "    rf_manual, X_train_val, y_train_val, \n",
    "    \"Learning Curve - Manual BO Optimized Random Forest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8230690c",
   "metadata": {},
   "source": [
    "## #5: Final Evaluation on Test Set\n",
    "\n",
    "### #5.1: Evaluate All Models on Test Set\n",
    "Test all three models (Baseline, Optuna, Manual BO) on the held-out test set to get final performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9bebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models on test set\n",
    "print(\"Evaluating all models on test set...\")\n",
    "baseline_test_results = evaluate_model(baseline_rf, X_test, y_test, \"Baseline - Test Set\")\n",
    "optuna_test_results = evaluate_model(rf_optuna, X_test, y_test, \"Optuna Optimized - Test Set\")\n",
    "manual_test_results = evaluate_model(rf_manual, X_test, y_test, \"Manual BO Optimized - Test Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887e40f",
   "metadata": {},
   "source": [
    "### #5.2: Compare All Models on Test Set\n",
    "Compare baseline and both optimized models' performance on the test set with comprehensive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7502b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive test set comparison\n",
    "test_comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n",
    "    'Baseline': [\n",
    "        baseline_test_results['accuracy'],\n",
    "        baseline_test_results['precision'],\n",
    "        baseline_test_results['recall'],\n",
    "        baseline_test_results['f1'],\n",
    "        baseline_test_results['roc_auc']\n",
    "    ],\n",
    "    'Optuna': [\n",
    "        optuna_test_results['accuracy'],\n",
    "        optuna_test_results['precision'],\n",
    "        optuna_test_results['recall'],\n",
    "        optuna_test_results['f1'],\n",
    "        optuna_test_results['roc_auc']\n",
    "    ],\n",
    "    'Manual BO': [\n",
    "        manual_test_results['accuracy'],\n",
    "        manual_test_results['precision'],\n",
    "        manual_test_results['recall'],\n",
    "        manual_test_results['f1'],\n",
    "        manual_test_results['roc_auc']\n",
    "    ]\n",
    "})\n",
    "\n",
    "test_comparison_df['Optuna Improvement (%)'] = ((test_comparison_df['Optuna'] - test_comparison_df['Baseline']) / test_comparison_df['Baseline'] * 100).round(2)\n",
    "test_comparison_df['Manual BO Improvement (%)'] = ((test_comparison_df['Manual BO'] - test_comparison_df['Baseline']) / test_comparison_df['Baseline'] * 100).round(2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SET COMPARISON: Baseline vs Optuna vs Manual BO\")\n",
    "print(\"=\"*80)\n",
    "print(test_comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151aadc3",
   "metadata": {},
   "source": [
    "### #5.3: Visualize Test Set Performance\n",
    "Visualize comparison of all three models on the test set with charts and confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68132342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test set results with all three models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Metrics comparison bar plot\n",
    "metrics = test_comparison_df['Metric'].tolist()\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "axes[0, 0].bar(x - width, test_comparison_df['Baseline'], width, label='Baseline', alpha=0.8, color='blue')\n",
    "axes[0, 0].bar(x, test_comparison_df['Optuna'], width, label='Optuna', alpha=0.8, color='green')\n",
    "axes[0, 0].bar(x + width, test_comparison_df['Manual BO'], width, label='Manual BO', alpha=0.8, color='orange')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].set_title('Test Set: Model Performance Comparison')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(metrics, rotation=45, ha='right')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. Improvement percentage comparison\n",
    "improvement_data = pd.DataFrame({\n",
    "    'Metric': metrics,\n",
    "    'Optuna': test_comparison_df['Optuna Improvement (%)'],\n",
    "    'Manual BO': test_comparison_df['Manual BO Improvement (%)']\n",
    "})\n",
    "\n",
    "x_pos = np.arange(len(metrics))\n",
    "axes[0, 1].barh(x_pos - 0.2, improvement_data['Optuna'], 0.4, label='Optuna', alpha=0.8, color='green')\n",
    "axes[0, 1].barh(x_pos + 0.2, improvement_data['Manual BO'], 0.4, label='Manual BO', alpha=0.8, color='orange')\n",
    "axes[0, 1].set_yticks(x_pos)\n",
    "axes[0, 1].set_yticklabels(metrics)\n",
    "axes[0, 1].set_xlabel('Improvement over Baseline (%)')\n",
    "axes[0, 1].set_title('Test Set: Performance Improvement vs Baseline')\n",
    "axes[0, 1].axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Baseline confusion matrix\n",
    "sns.heatmap(baseline_test_results['confusion_matrix'], annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Under 2.5', 'Over 2.5'], yticklabels=['Under 2.5', 'Over 2.5'],\n",
    "            ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Baseline Model - Test Set Confusion Matrix')\n",
    "axes[1, 0].set_ylabel('True Label')\n",
    "axes[1, 0].set_xlabel('Predicted Label')\n",
    "\n",
    "# 4. Best optimized model confusion matrix (choose best between Optuna and Manual BO)\n",
    "best_model_name = 'Optuna' if optuna_test_results['roc_auc'] >= manual_test_results['roc_auc'] else 'Manual BO'\n",
    "best_model_results = optuna_test_results if best_model_name == 'Optuna' else manual_test_results\n",
    "best_model_color = 'Greens' if best_model_name == 'Optuna' else 'Oranges'\n",
    "\n",
    "sns.heatmap(best_model_results['confusion_matrix'], annot=True, fmt='d', cmap=best_model_color,\n",
    "            xticklabels=['Under 2.5', 'Over 2.5'], yticklabels=['Under 2.5', 'Over 2.5'],\n",
    "            ax=axes[1, 1])\n",
    "axes[1, 1].set_title(f'{best_model_name} Model - Test Set Confusion Matrix')\n",
    "axes[1, 1].set_ylabel('True Label')\n",
    "axes[1, 1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de89af62",
   "metadata": {},
   "source": [
    "### #5.4: Performance Summary Across All Sets\n",
    "Compare model performance across validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b8bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary across all datasets\n",
    "summary_data = {\n",
    "    'Dataset': ['Validation', 'Test'] * 3,\n",
    "    'Model': ['Baseline'] * 2 + ['Optuna'] * 2 + ['Manual BO'] * 2,\n",
    "    'Accuracy': [\n",
    "        baseline_val_results['accuracy'],\n",
    "        baseline_test_results['accuracy'],\n",
    "        optuna_val_results['accuracy'],\n",
    "        optuna_test_results['accuracy'],\n",
    "        manual_val_results['accuracy'],\n",
    "        manual_test_results['accuracy']\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        baseline_val_results['f1'],\n",
    "        baseline_test_results['f1'],\n",
    "        optuna_val_results['f1'],\n",
    "        optuna_test_results['f1'],\n",
    "        manual_val_results['f1'],\n",
    "        manual_test_results['f1']\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        baseline_val_results['roc_auc'],\n",
    "        baseline_test_results['roc_auc'],\n",
    "        optuna_val_results['roc_auc'],\n",
    "        optuna_test_results['roc_auc'],\n",
    "        manual_val_results['roc_auc'],\n",
    "        manual_test_results['roc_auc']\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE SUMMARY - VALIDATION AND TEST SETS\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Plot performance across datasets\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# F1-Score across datasets\n",
    "datasets = ['Validation', 'Test']\n",
    "baseline_f1 = [baseline_val_results['f1'], baseline_test_results['f1']]\n",
    "optuna_f1 = [optuna_val_results['f1'], optuna_test_results['f1']]\n",
    "manual_f1 = [manual_val_results['f1'], manual_test_results['f1']]\n",
    "\n",
    "ax1.plot(datasets, baseline_f1, 'o-', linewidth=2, markersize=8, label='Baseline', color='blue')\n",
    "ax1.plot(datasets, optuna_f1, 'o-', linewidth=2, markersize=8, label='Optuna', color='green')\n",
    "ax1.plot(datasets, manual_f1, 'o-', linewidth=2, markersize=8, label='Manual BO', color='orange')\n",
    "ax1.set_ylabel('F1-Score')\n",
    "ax1.set_title('F1-Score: Validation vs Test Set')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ROC-AUC across datasets\n",
    "baseline_auc = [baseline_val_results['roc_auc'], baseline_test_results['roc_auc']]\n",
    "optuna_auc = [optuna_val_results['roc_auc'], optuna_test_results['roc_auc']]\n",
    "manual_auc = [manual_val_results['roc_auc'], manual_test_results['roc_auc']]\n",
    "\n",
    "ax2.plot(datasets, baseline_auc, 'o-', linewidth=2, markersize=8, label='Baseline', color='blue')\n",
    "ax2.plot(datasets, optuna_auc, 'o-', linewidth=2, markersize=8, label='Optuna', color='green')\n",
    "ax2.plot(datasets, manual_auc, 'o-', linewidth=2, markersize=8, label='Manual BO', color='orange')\n",
    "ax2.set_ylabel('ROC-AUC')\n",
    "ax2.set_title('ROC-AUC: Validation vs Test Set')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a77f4",
   "metadata": {},
   "source": [
    "### #5.5: Save Best Models and Results\n",
    "Save the optimized models and best parameters for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e9f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine best model based on test set ROC-AUC\n",
    "import joblib\n",
    "\n",
    "best_model_name = 'Optuna' if optuna_test_results['roc_auc'] >= manual_test_results['roc_auc'] else 'Manual BO'\n",
    "best_model = rf_optuna if best_model_name == 'Optuna' else rf_manual\n",
    "best_params_final = study.best_params if best_model_name == 'Optuna' else manual_bo_params\n",
    "best_test_auc = optuna_test_results['roc_auc'] if best_model_name == 'Optuna' else manual_test_results['roc_auc']\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name} (Test ROC-AUC: {best_test_auc:.6f})\")\n",
    "\n",
    "# Save the best model\n",
    "model_save_path = f'./processed/best_random_forest_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl'\n",
    "joblib.dump(best_model, model_save_path)\n",
    "print(f\"‚úÖ Best model saved to: {model_save_path}\")\n",
    "\n",
    "# Save both optimized models\n",
    "joblib.dump(rf_optuna, './processed/rf_optuna_model.pkl')\n",
    "joblib.dump(rf_manual, './processed/rf_manual_bo_model.pkl')\n",
    "print(f\"‚úÖ Optuna model saved to: ./processed/rf_optuna_model.pkl\")\n",
    "print(f\"‚úÖ Manual BO model saved to: ./processed/rf_manual_bo_model.pkl\")\n",
    "\n",
    "# Save the best parameters\n",
    "params_save_path = './processed/best_rf_parameters.pkl'\n",
    "joblib.dump({\n",
    "    'best_method': best_model_name,\n",
    "    'optuna_params': study.best_params,\n",
    "    'manual_bo_params': manual_bo_params\n",
    "}, params_save_path)\n",
    "print(f\"‚úÖ Best parameters saved to: {params_save_path}\")\n",
    "\n",
    "# Save all results\n",
    "results_save_path = './processed/baseline_rf_results.pkl'\n",
    "results_dict = {\n",
    "    'baseline_val': baseline_val_results,\n",
    "    'baseline_test': baseline_test_results,\n",
    "    'optuna_val': optuna_val_results,\n",
    "    'optuna_test': optuna_test_results,\n",
    "    'manual_val': manual_val_results,\n",
    "    'manual_test': manual_test_results,\n",
    "    'best_model': best_model_name,\n",
    "    'optuna_params': study.best_params,\n",
    "    'manual_bo_params': manual_bo_params,\n",
    "    'feature_importance': feature_importance,\n",
    "    'optuna_study': study\n",
    "}\n",
    "joblib.dump(results_dict, results_save_path)\n",
    "print(f\"‚úÖ All results saved to: {results_save_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"BEST MODEL CONFIGURATION ({best_model_name})\")\n",
    "print(\"=\"*80)\n",
    "for key, value in best_params_final.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c04a381",
   "metadata": {},
   "source": [
    "### #5.6: Final Summary and Conclusions\n",
    "Display key findings and insights from the baseline Random Forest analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b346ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE RANDOM FOREST - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n1. MODEL PERFORMANCE ON TEST SET:\")\n",
    "print(f\"   Baseline Model:\")\n",
    "print(f\"     - Accuracy:  {baseline_test_results['accuracy']:.4f}\")\n",
    "print(f\"     - F1-Score:  {baseline_test_results['f1']:.4f}\")\n",
    "print(f\"     - ROC-AUC:   {baseline_test_results['roc_auc']:.4f}\")\n",
    "print(f\"\\n   Optuna Optimized Model:\")\n",
    "print(f\"     - Accuracy:  {optuna_test_results['accuracy']:.4f}\")\n",
    "print(f\"     - F1-Score:  {optuna_test_results['f1']:.4f}\")\n",
    "print(f\"     - ROC-AUC:   {optuna_test_results['roc_auc']:.4f}\")\n",
    "print(f\"\\n   Manual BO Optimized Model:\")\n",
    "print(f\"     - Accuracy:  {manual_test_results['accuracy']:.4f}\")\n",
    "print(f\"     - F1-Score:  {manual_test_results['f1']:.4f}\")\n",
    "print(f\"     - ROC-AUC:   {manual_test_results['roc_auc']:.4f}\")\n",
    "\n",
    "# Calculate improvements\n",
    "optuna_improvement_f1 = ((optuna_test_results['f1'] - baseline_test_results['f1']) / baseline_test_results['f1'] * 100)\n",
    "optuna_improvement_auc = ((optuna_test_results['roc_auc'] - baseline_test_results['roc_auc']) / baseline_test_results['roc_auc'] * 100)\n",
    "manual_improvement_f1 = ((manual_test_results['f1'] - baseline_test_results['f1']) / baseline_test_results['f1'] * 100)\n",
    "manual_improvement_auc = ((manual_test_results['roc_auc'] - baseline_test_results['roc_auc']) / baseline_test_results['roc_auc'] * 100)\n",
    "\n",
    "print(f\"\\n2. IMPROVEMENT THROUGH HYPERPARAMETER OPTIMIZATION:\")\n",
    "print(f\"   Optuna Method:\")\n",
    "print(f\"     - F1-Score improved by:  {optuna_improvement_f1:+.2f}%\")\n",
    "print(f\"     - ROC-AUC improved by:   {optuna_improvement_auc:+.2f}%\")\n",
    "print(f\"   Manual Bayesian Optimization:\")\n",
    "print(f\"     - F1-Score improved by:  {manual_improvement_f1:+.2f}%\")\n",
    "print(f\"     - ROC-AUC improved by:   {manual_improvement_auc:+.2f}%\")\n",
    "\n",
    "print(f\"\\n3. TOP 5 MOST IMPORTANT FEATURES (from best model):\")\n",
    "for idx, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"   {idx+1}. {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\\n4. HYPERPARAMETER OPTIMIZATION DETAILS:\")\n",
    "print(f\"   - Optuna: 50 trials with TPE sampler\")\n",
    "print(f\"   - Manual BO: 10 initial samples + 40 GP-guided iterations\")\n",
    "print(f\"   - Both methods used 5-fold CV on training set\")\n",
    "print(f\"   - Best method: {best_model_name}\")\n",
    "\n",
    "print(f\"\\n5. MODELS SAVED:\")\n",
    "print(f\"   - Best model: {model_save_path}\")\n",
    "print(f\"   - Optuna model: ./processed/rf_optuna_model.pkl\")\n",
    "print(f\"   - Manual BO model: ./processed/rf_manual_bo_model.pkl\")\n",
    "print(f\"   - Parameters: {params_save_path}\")\n",
    "print(f\"   - Results: {results_save_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Analysis Complete!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
