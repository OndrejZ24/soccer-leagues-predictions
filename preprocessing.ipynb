{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML2 Semestral Project - Football O/U 2.5\n",
    "**Authors:** Phuong Nhi Tranová, Vít Maruniak, Šimon Slánský, Radim Škoukal, Ondřej Zetek, Martin Kareš, Jan Korčák, Jakub Maličkay, Jáchym Janouch  \n",
    "**Course:** FIS 4IT344 Machine Learning 2 (2025/2026)  \n",
    "**Goal:** Compare baseline (current features) vs extended (richer features) models for O/U 2.5 goals across markets; translate accuracy gains into optimal profit and **maximum data subscription price per country** *.  \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "***maximum data subscription price per country**\n",
    "- the most money our company should be willing to pay for that country's additional data\n",
    "- that's how much extra profit the improved model generates\n",
    "- baseline model → accuracy = A₀\n",
    "    - Generates profit Π*(A₀)\n",
    "- extended model → accuracy = A₁\n",
    "    - Generates profit Π*(A₁)\n",
    "- profit improvement = ΔΠ = Π(A₁) − Π(A₀)*\n",
    "    - basically how much more money the comany earns each year by using the better data\n",
    "- the maximum data subscription price per country = ΔΠ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports and paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Library parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"  \n",
    "OUTPUT_DIR = f\"./processed\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_matches(data_dir: str) -> pd.DataFrame:\n",
    "    csv_files = glob.glob(os.path.join(data_dir, \"**\", \"*.csv\"), recursive=True)\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSV files found under {data_dir}\")\n",
    "\n",
    "    frames = []\n",
    "    for fp in csv_files:\n",
    "        # extract path info\n",
    "        rel = os.path.relpath(fp, data_dir)\n",
    "        parts = Path(rel).parts\n",
    "        country = parts[0] if len(parts) >= 1 else None\n",
    "        league  = parts[1] if len(parts) >= 2 else None\n",
    "        season_file = parts[2] if len(parts) >= 3 else None\n",
    "        season = os.path.splitext(season_file)[0] if season_file else None\n",
    "\n",
    "        # read and rename\n",
    "        try:\n",
    "            df = pd.read_csv(fp, low_memory=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {fp}: {e}\")\n",
    "            continue\n",
    "\n",
    "        frames.append(df)\n",
    "\n",
    "    all_df = pd.concat(frames, ignore_index=True, sort=False)\n",
    "    return all_df\n",
    "\n",
    "# run the loader\n",
    "all_matches = pd.DataFrame(load_all_matches(DATA_DIR))\n",
    "print(all_matches.columns.tolist())\n",
    "print(all_matches.shape)\n",
    "display(all_matches.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis\n",
    "\n",
    "Before proceeding with data cleaning, let's understand our data better through comprehensive exploratory data analysis. This will help us make informed decisions about preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Shape and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset shape: {all_matches.shape}\")\n",
    "print(f\"Number of seasons/countries covered:\")\n",
    "print(f\"Countries: {all_matches['Div'].str[:-1].nunique()}\")\n",
    "print(f\"Leagues: {all_matches['Div'].nunique()}\")\n",
    "print(f\"Date range: {all_matches['Date'].min()} to {all_matches['Date'].max()}\")\n",
    "\n",
    "# Check basic statistics\n",
    "print(f\"\\nBasic goal statistics:\")\n",
    "print(f\"Total goals per match stats:\")\n",
    "total_goals = all_matches['FTHG'] + all_matches['FTAG']\n",
    "print(total_goals.describe())\n",
    "\n",
    "print(f\"\\nOver/Under 2.5 goals distribution:\")\n",
    "over_2_5 = (total_goals > 2.5).astype(int)\n",
    "print(f\"Over 2.5: {over_2_5.sum()} ({over_2_5.mean():.2%})\")\n",
    "print(f\"Under 2.5: {(~over_2_5.astype(bool)).sum()} ({(1-over_2_5.mean()):.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our target variable (Over/Under 2.5 goals) is perfectly balanced with almost exactly 50/50 split, which is ideal for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed missing values analysis\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'column': all_matches.columns,\n",
    "    'missing_count': all_matches.isnull().sum(),\n",
    "    'missing_percentage': (all_matches.isnull().sum() / len(all_matches)) * 100,\n",
    "    'dtype': all_matches.dtypes\n",
    "})\n",
    "\n",
    "# Filter to show only columns with missing values\n",
    "missing_analysis = missing_analysis[missing_analysis['missing_count'] > 0].sort_values('missing_percentage', ascending=False)\n",
    "\n",
    "print(f\"Columns with missing values: {len(missing_analysis)}\")\n",
    "print(f\"Total columns: {len(all_matches.columns)}\")\n",
    "print(f\"\\nTop 20 columns with highest missing percentage:\")\n",
    "display(missing_analysis.head(20))\n",
    "\n",
    "# Check missing patterns in key variables\n",
    "key_stats = ['HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "print(f\"\\nMissing data in key match statistics:\")\n",
    "for stat in key_stats:\n",
    "    if stat in all_matches.columns:\n",
    "        missing_pct = (all_matches[stat].isnull().sum() / len(all_matches)) * 100\n",
    "        print(f\"{stat}: {missing_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing data analysis reveals that:\n",
    "1. **Betting odds** have the highest missing percentages (80%+) - this is expected as not all bookmakers operate in all leagues/seasons\n",
    "2. **Key match statistics** (shots, corners, fouls, cards) have very low missing rates (<0.1%), which is excellent for our modeling\n",
    "3. Most missing data is in betting-related columns, which we can handle appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 League and Country Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# League distribution\n",
    "league_counts = all_matches['Div'].value_counts()\n",
    "print(\"League distribution:\")\n",
    "display(league_counts)\n",
    "\n",
    "# Country mapping for better understanding\n",
    "country_mapping = {\n",
    "    'E': 'England', 'SC': 'Scotland', 'SP': 'Spain', 'I': 'Italy', \n",
    "    'D': 'Germany', 'F': 'France', 'N': 'Netherlands', 'B': 'Belgium',\n",
    "    'P': 'Portugal', 'T': 'Turkey', 'G': 'Greece'\n",
    "}\n",
    "\n",
    "all_matches['Country'] = all_matches['Div'].str[:-1].map(country_mapping)\n",
    "country_counts = all_matches['Country'].value_counts()\n",
    "print(f\"\\nMatches per country:\")\n",
    "display(country_counts)\n",
    "\n",
    "# Visualize the distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Country distribution\n",
    "country_counts.plot(kind='bar', ax=ax1, color='skyblue')\n",
    "ax1.set_title('Matches per Country')\n",
    "ax1.set_xlabel('Country')\n",
    "ax1.set_ylabel('Number of Matches')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Goals distribution\n",
    "total_goals = all_matches['FTHG'] + all_matches['FTAG']\n",
    "total_goals.hist(bins=15, ax=ax2, color='lightcoral', alpha=0.7)\n",
    "ax2.axvline(x=2.5, color='red', linestyle='--', linewidth=2, label='2.5 goals threshold')\n",
    "ax2.set_title('Distribution of Total Goals per Match')\n",
    "ax2.set_xlabel('Total Goals')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Handling csv issues\n",
    "It seems like the renaming and loading went smoothly! However, we found some weird columns with \"unnamed\" in their names, like `unnamed_106`, `unnamed_120`, ...  \n",
    "That sometimes happens when excel files have extra blank columns. We'll take a quick look to see if they have any data, and if they're totally empty (full of NaNs), we'll just get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnamed_cols = [c for c in all_matches.columns if c.lower().startswith(\"unnamed\")]\n",
    "all_matches[unnamed_cols].isna().mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're 100% full of NaNs so we can now safely drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = all_matches.drop(columns=unnamed_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Normalizing league codes\n",
    "Let's normalize the leagues, as English and Scottish leagues have the best leagues interpreted as E0, SC0, respectively. All other countries mark the best league as CountryCode1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = all_matches['Div'].str.startswith(('E', 'SC'))\n",
    "all_matches.loc[mask, 'Div'] = all_matches.loc[mask, 'Div'].apply(\n",
    "    lambda x: f\"{x[:-1]}{int(x[-1]) + 1}\"\n",
    ")\n",
    "\n",
    "print(all_matches['Div'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Handling English and Scottish yellow cards\n",
    "Let's take care of the first note in notes.txt file, which mentions - English and Scottish yellow cards do not include the initial yellow card when a second is shown to a player converting it into a red, but this is included as a yellow (plus red) for European games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = all_matches['Div'].str.startswith(('E', 'SC'))\n",
    "red_mask = mask & ((all_matches['HR'] == 1) | (all_matches['AR'] == 1))\n",
    "\n",
    "print(\"Before adjustment (sample):\")\n",
    "print(all_matches.loc[red_mask, ['Div', 'HY', 'HR', 'AY', 'AR']].head())\n",
    "\n",
    "all_matches.loc[mask, 'HY'] += all_matches.loc[mask, 'HR'].eq(1).astype(int)\n",
    "all_matches.loc[mask, 'AY'] += all_matches.loc[mask, 'AR'].eq(1).astype(int)\n",
    "\n",
    "print(\"\\nAfter adjustment (sample):\")\n",
    "print(all_matches.loc[red_mask, ['Div', 'HY', 'HR', 'AY', 'AR']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Correcting data types\n",
    "Now, let's inspect the data types of our columns. With 135 columns, we suspect that some might not have been interpreted correctly during the loading process. Checking the data types is an important step before proceeding with any further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, dtype in all_matches.dtypes.items():\n",
    "    print(f\"{col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_columns = ['Date', 'Time']\n",
    "\n",
    "category_columns = ['Div', 'HomeTeam', 'AwayTeam', 'FTR', 'HTR', 'Referee']\n",
    "\n",
    "int_columns = ['FTHG', 'FTAG', 'HTHG', 'HTAG', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "\n",
    "float_columns = ['B365CH', 'BWCA', '1XBH']\n",
    "\n",
    "for col in time_columns:\n",
    "    if col == 'Date':\n",
    "        all_matches[col] = pd.to_datetime(all_matches[col])\n",
    "    else:\n",
    "        all_matches[col] = pd.to_datetime(all_matches[col], format='%H:%M').dt.time\n",
    "\n",
    "for col in category_columns:\n",
    "    all_matches[col] = all_matches[col].astype('category')\n",
    "\n",
    "for col in int_columns:\n",
    "    all_matches[col] = pd.to_numeric(all_matches[col], errors='coerce').astype('Int64')  \n",
    "\n",
    "for col in float_columns:\n",
    "    all_matches[col] = pd.to_numeric(all_matches[col], errors='coerce').astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, dtype in all_matches.dtypes.items():\n",
    "    print(f\"{col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Missing value imputation with domain-specific strategies\n",
    "\n",
    "Based on our EDA, we'll handle missing values with different strategies based on data characteristics:\n",
    "\n",
    "1. **Key match statistics**: Very few missing values (~0.1%) - use SimpleImputer with median strategy\n",
    "2. **Betting odds**: High missingness (80%+) but match-specific - use cross-bookmaker median imputation per match, then overall median fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns with low missingness that need imputation\n",
    "low_missingness_cols = ['HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "\n",
    "# Check current missing values before imputation\n",
    "print(\"Missing values before imputation:\")\n",
    "for col in low_missingness_cols:\n",
    "    if col in all_matches.columns:\n",
    "        missing_count = all_matches[col].isnull().sum()\n",
    "        missing_pct = (missing_count / len(all_matches)) * 100\n",
    "        print(f\"{col}: {missing_count} ({missing_pct:.3f}%)\")\n",
    "\n",
    "# Apply median imputation for numerical match statistics\n",
    "if any(all_matches[col].isnull().sum() > 0 for col in low_missingness_cols if col in all_matches.columns):\n",
    "    match_stats_imputer = SimpleImputer(strategy='median')\n",
    "    \n",
    "    # Only impute columns that actually exist and have missing values\n",
    "    cols_to_impute = [col for col in low_missingness_cols if col in all_matches.columns and all_matches[col].isnull().sum() > 0]\n",
    "    \n",
    "    if cols_to_impute:\n",
    "        print(f\"\\nApplying median imputation to: {cols_to_impute}\")\n",
    "        all_matches[cols_to_impute] = match_stats_imputer.fit_transform(all_matches[cols_to_impute])\n",
    "        \n",
    "        print(\"Imputation completed. Median values used:\")\n",
    "        for col in cols_to_impute:\n",
    "            median_val = all_matches[col].median()\n",
    "            print(f\"  {col}: {median_val}\")\n",
    "    else:\n",
    "        print(\"No missing values found in match statistics columns.\")\n",
    "else:\n",
    "    print(\"No missing values found in match statistics columns.\")\n",
    "\n",
    "# Handle categorical columns separately\n",
    "if 'Referee' in all_matches.columns:\n",
    "    referee_missing = all_matches['Referee'].isnull().sum()\n",
    "    if referee_missing > 0:\n",
    "        if 'Unknown' not in all_matches['Referee'].cat.categories:\n",
    "            all_matches['Referee'] = all_matches['Referee'].cat.add_categories(['Unknown'])\n",
    "        all_matches['Referee'] = all_matches['Referee'].fillna('Unknown')\n",
    "        print(f\"Filled {referee_missing} missing referees with 'Unknown'\")\n",
    "\n",
    "if 'Time' in all_matches.columns:\n",
    "    time_missing = all_matches['Time'].isnull().sum()\n",
    "    if time_missing > 0:\n",
    "        all_matches['Time'] = all_matches['Time'].fillna(pd.to_datetime('15:00', format='%H:%M').time())\n",
    "        print(f\"Filled {time_missing} missing times with '15:00'\")\n",
    "\n",
    "# Verify no missing values remain in core match statistics\n",
    "print(f\"\\nVerification - remaining missing values in core columns:\")\n",
    "verification_cols = low_missingness_cols + ['Referee', 'Time']\n",
    "total_missing = 0\n",
    "for col in verification_cols:\n",
    "    if col in all_matches.columns:\n",
    "        missing = all_matches[col].isnull().sum()\n",
    "        if missing > 0:\n",
    "            print(f\"{col}: {missing}\")\n",
    "            total_missing += missing\n",
    "\n",
    "if total_missing == 0:\n",
    "    print(\"All core match statistics successfully imputed - no missing values remain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 Missingness Pattern Analysis (MCAR/MAR/MNAR)\n",
    "\n",
    "To understand the nature of missing data, we'll visualize where missing values occur across countries, leagues, and feature groups. This will help us determine:\n",
    "- **MCAR (Missing Completely At Random)**: Missing values are randomly distributed, unrelated to any observed or unobserved data\n",
    "- **MAR (Missing At Random)**: Missingness depends on observed data (e.g., country, league tier)\n",
    "- **MNAR (Missing Not At Random)**: Missingness depends on the unobserved values themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# 1. Overall missingness heatmap for top missing columns\n",
    "print(\"=\" * 80)\n",
    "print(\"1. TOP MISSING COLUMNS - OVERALL PATTERN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get columns with >1% missing data (excluding the 100% missing unnamed columns)\n",
    "missing_pct = all_matches.isnull().mean() * 100\n",
    "significant_missing = missing_pct[(missing_pct > 1) & (missing_pct < 100)].sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\nFound {len(significant_missing)} columns with 1-100% missing data\")\n",
    "print(\"\\nTop 30 columns by missingness:\")\n",
    "display(significant_missing.head(30))\n",
    "\n",
    "# Visualize top 40 missing columns\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "top_40 = significant_missing.head(40).sort_values()\n",
    "top_40.plot(kind='barh', ax=ax, color='steelblue')\n",
    "ax.set_xlabel('Missing Percentage (%)', fontsize=12)\n",
    "ax.set_title('Top 40 Columns by Missing Data Percentage', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Missingness by Country (key indicator for MAR)\n",
    "\n",
    "# Select representative columns from different categories\n",
    "match_stats_cols = ['HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "betting_sample = ['B365H', 'B365D', 'B365A', 'B365>2.5', 'B365<2.5', 'PSH', 'PSD', 'PSA', \n",
    "                  'WHH', 'WHD', 'WHA', 'IWH', 'IWD', 'IWA', 'BFH', 'BFD', 'BFA']\n",
    "cols_for_analysis = match_stats_cols + [c for c in betting_sample if c in all_matches.columns]\n",
    "\n",
    "# Calculate missingness by country\n",
    "country_missing = all_matches.groupby('Country')[cols_for_analysis].apply(lambda df: df.isnull().mean() * 100)\n",
    "\n",
    "print(\"\\nMissingness percentage by Country:\")\n",
    "display(country_missing)\n",
    "\n",
    "# Heatmap for match statistics (should be low and uniform if MCAR)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Match statistics missingness by country\n",
    "sns.heatmap(country_missing[match_stats_cols], annot=True, fmt='.2f', cmap='Reds', \n",
    "            ax=axes[0], vmin=0, vmax=5, cbar_kws={'label': 'Missing %'})\n",
    "axes[0].set_title('Match Statistics Missingness by Country', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Feature')\n",
    "axes[0].set_ylabel('Country')\n",
    "\n",
    "# Betting odds missingness by country (sample)\n",
    "betting_cols_available = [c for c in betting_sample if c in country_missing.columns][:12]\n",
    "if betting_cols_available:\n",
    "    sns.heatmap(country_missing[betting_cols_available], annot=True, fmt='.1f', cmap='YlOrRd', \n",
    "                ax=axes[1], cbar_kws={'label': 'Missing %'})\n",
    "    axes[1].set_title('Betting Odds Missingness by Country (Sample)', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Feature')\n",
    "    axes[1].set_ylabel('Country')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Missingness by League (Div) - more granular view\n",
    "\n",
    "# Calculate missingness by league\n",
    "league_missing = all_matches.groupby('Div')[cols_for_analysis].apply(lambda df: df.isnull().mean() * 100)\n",
    "\n",
    "# Visualize for selected columns\n",
    "selected_cols = match_stats_cols[:8] + [c for c in ['B365H', 'B365>2.5', 'PSH', 'WHH'] if c in league_missing.columns]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "sns.heatmap(league_missing[selected_cols], annot=True, fmt='.1f', cmap='RdYlGn_r', \n",
    "            ax=ax, cbar_kws={'label': 'Missing %'})\n",
    "ax.set_title('Missingness Pattern by League (Selected Columns)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Feature', fontsize=11)\n",
    "ax.set_ylabel('League', fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify leagues with highest missingness\n",
    "print(\"\\nLeagues with highest overall missingness:\")\n",
    "league_avg_missing = league_missing.mean(axis=1).sort_values(ascending=False)\n",
    "display(league_avg_missing.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Correlation between missingness and observed variables (test for MAR/MNAR)\n",
    "\n",
    "# Create missingness indicators\n",
    "numeric_features = ['total_goals', 'FTHG', 'FTAG', 'league_tier', 'month']\n",
    "numeric_features = [f for f in numeric_features if f in all_matches.columns]\n",
    "\n",
    "# Test subset of columns to avoid computational overload\n",
    "test_columns = match_stats_cols + [c for c in ['B365H', 'B365>2.5', 'PSH', 'WHH', 'IWH', 'BFH'] \n",
    "                                     if c in all_matches.columns]\n",
    "\n",
    "correlations = {}\n",
    "for col in test_columns:\n",
    "    miss_indicator = all_matches[col].isnull().astype(int)\n",
    "    corr_values = {}\n",
    "    for num_feat in numeric_features:\n",
    "        try:\n",
    "            corr = miss_indicator.corr(all_matches[num_feat].astype(float))\n",
    "            corr_values[num_feat] = corr\n",
    "        except:\n",
    "            corr_values[num_feat] = np.nan\n",
    "    correlations[col] = corr_values\n",
    "\n",
    "corr_df = pd.DataFrame(correlations).T\n",
    "\n",
    "print(\"\\nCorrelations between missing indicators and numeric features:\")\n",
    "print(\"(Strong correlation suggests MAR - missingness depends on these observed variables)\")\n",
    "display(corr_df)\n",
    "\n",
    "# Visualize correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(corr_df, annot=True, fmt='.3f', cmap='coolwarm', center=0, \n",
    "            ax=ax, vmin=-0.5, vmax=0.5, cbar_kws={'label': 'Correlation'})\n",
    "ax.set_title('Correlation: Missingness Indicators vs Observed Variables', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Observed Variable', fontsize=11)\n",
    "ax.set_ylabel('Feature with Missing Values', fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Flag significant correlations\n",
    "print(\"\\nSignificant correlations (|corr| > 0.1):\")\n",
    "for col in corr_df.index:\n",
    "    for feat in corr_df.columns:\n",
    "        corr_val = corr_df.loc[col, feat]\n",
    "        if abs(corr_val) > 0.1:\n",
    "            print(f\"  {col} ~ {feat}: {corr_val:.3f}\")\n",
    "            if abs(corr_val) > 0.2:\n",
    "                print(f\"    ⚠ Strong evidence of MAR!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Temporal patterns in missingness\n",
    "\n",
    "# Missingness over time\n",
    "all_matches['Year'] = all_matches['Date'].dt.year\n",
    "yearly_missing = all_matches.groupby('Year')[test_columns].apply(lambda df: df.isnull().mean() * 100)\n",
    "\n",
    "print(\"\\nMissingness by year:\")\n",
    "display(yearly_missing)\n",
    "\n",
    "# Visualize temporal trend\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Match statistics over time\n",
    "axes[0].plot(yearly_missing.index, yearly_missing[match_stats_cols[:6]], marker='o')\n",
    "axes[0].set_title('Match Statistics Missingness Over Time', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Year')\n",
    "axes[0].set_ylabel('Missing %')\n",
    "axes[0].legend(match_stats_cols[:6], loc='best', fontsize=9)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Betting odds over time\n",
    "betting_in_test = [c for c in test_columns if c in ['B365H', 'B365>2.5', 'PSH', 'WHH', 'IWH', 'BFH']]\n",
    "if betting_in_test:\n",
    "    axes[1].plot(yearly_missing.index, yearly_missing[betting_in_test], marker='s')\n",
    "    axes[1].set_title('Betting Odds Missingness Over Time', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Year')\n",
    "    axes[1].set_ylabel('Missing %')\n",
    "    axes[1].legend(betting_in_test, loc='best', fontsize=9)\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation: If missingness changes over time → likely MAR (systematic data collection changes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Summary and Classification\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "\n",
    "for col in test_columns:\n",
    "    missing_rate = all_matches[col].isnull().mean() * 100\n",
    "    \n",
    "    # Country variance\n",
    "    country_var = country_missing[col].var() if col in country_missing.columns else 0\n",
    "    \n",
    "    # League variance  \n",
    "    league_var = league_missing[col].var() if col in league_missing.columns else 0\n",
    "    \n",
    "    # Max correlation with numeric features\n",
    "    max_corr = corr_df.loc[col].abs().max() if col in corr_df.index else 0\n",
    "    \n",
    "    # Temporal trend (coefficient of variation)\n",
    "    temporal_cv = yearly_missing[col].std() / (yearly_missing[col].mean() + 0.001) if col in yearly_missing.columns else 0\n",
    "    \n",
    "    # Classification\n",
    "    if missing_rate < 0.5:\n",
    "        mechanism = \"MCAR (very low missingness)\"\n",
    "    elif country_var > 100 or league_var > 100:\n",
    "        mechanism = \"MAR (varies by country/league)\"\n",
    "    elif max_corr > 0.15:\n",
    "        mechanism = \"MAR (correlated with observables)\"\n",
    "    elif temporal_cv > 0.5:\n",
    "        mechanism = \"MAR (temporal pattern)\"\n",
    "    elif missing_rate > 80:\n",
    "        mechanism = \"Potentially MNAR (very high missingness)\"\n",
    "    else:\n",
    "        mechanism = \"Likely MCAR (uniform pattern)\"\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Feature': col,\n",
    "        'Missing_Rate_%': round(missing_rate, 2),\n",
    "        'Country_Variance': round(country_var, 2),\n",
    "        'League_Variance': round(league_var, 2),\n",
    "        'Max_Correlation': round(max_corr, 3),\n",
    "        'Temporal_CV': round(temporal_cv, 3),\n",
    "        'Mechanism': mechanism\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data).sort_values('Missing_Rate_%', ascending=False)\n",
    "\n",
    "print(\"\\nMissingness Mechanism Classification:\")\n",
    "display(summary_df)\n",
    "\n",
    "# Visual summary by mechanism\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Count by mechanism\n",
    "mechanism_counts = summary_df['Mechanism'].value_counts()\n",
    "axes[0].bar(range(len(mechanism_counts)), mechanism_counts.values, color=['#2ecc71', '#3498db', '#e74c3c'])\n",
    "axes[0].set_xticks(range(len(mechanism_counts)))\n",
    "axes[0].set_xticklabels(mechanism_counts.index, rotation=45, ha='right', fontsize=9)\n",
    "axes[0].set_ylabel('Count', fontsize=11)\n",
    "axes[0].set_title('Distribution of Missingness Mechanisms', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Missing rate by mechanism\n",
    "for i, mechanism in enumerate(mechanism_counts.index):\n",
    "    subset = summary_df[summary_df['Mechanism'] == mechanism]\n",
    "    axes[1].scatter([i] * len(subset), subset['Missing_Rate_%'], alpha=0.6, s=100, label=mechanism)\n",
    "\n",
    "axes[1].set_xticks(range(len(mechanism_counts)))\n",
    "axes[1].set_xticklabels(mechanism_counts.index, rotation=45, ha='right', fontsize=9)\n",
    "axes[1].set_ylabel('Missing Rate (%)', fontsize=11)\n",
    "axes[1].set_title('Missing Rate by Mechanism Type', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle betting odds imputation with proper market categorization\n",
    "# Based on notes.txt, betting odds are organized by market type (1X2, O/U 2.5, Asian Handicap)\n",
    "\n",
    "# Identify betting odds columns\n",
    "betting_cols = [col for col in all_matches.columns if any(bookmaker in col for bookmaker in ['B365', 'BW', 'PS', 'IW', 'LB', 'WH', 'SJ', 'VC', 'BF', '1XB', 'CL', 'GB', 'SO', 'SB', 'SY', 'Max', 'Avg', 'Bb'])]\n",
    "\n",
    "print(f\"Found {len(betting_cols)} betting odds columns\")\n",
    "\n",
    "# Analyze missing patterns in betting odds\n",
    "betting_missing_analysis = []\n",
    "high_missing_cols = []\n",
    "for col in betting_cols:\n",
    "    missing_count = all_matches[col].isnull().sum()\n",
    "    missing_pct = (missing_count / len(all_matches)) * 100\n",
    "    betting_missing_analysis.append({\n",
    "        'column': col,\n",
    "        'missing_count': missing_count,\n",
    "        'missing_pct': missing_pct\n",
    "    })\n",
    "    if missing_pct > 80:  # Track columns with very high missingness\n",
    "        high_missing_cols.append(col)\n",
    "\n",
    "print(f\"Columns with >80% missing values: {len(high_missing_cols)}\")\n",
    "print(\"Sample of betting odds with lower missingness (<80%):\")\n",
    "for item in sorted(betting_missing_analysis, key=lambda x: x['missing_pct'])[:15]:\n",
    "    if item['missing_pct'] < 80:\n",
    "        print(f\"  {item['column']}: {item['missing_pct']:.1f}%\")\n",
    "\n",
    "# Categorize betting odds by market type based on notes.txt\n",
    "def categorize_betting_market(col_name):\n",
    "    \"\"\"Categorize betting column by market type based on column name patterns\"\"\"\n",
    "    col = col_name.upper()\n",
    "    \n",
    "    # 1X2 Market (Home/Draw/Away win)\n",
    "    if col.endswith('H') and not any(x in col for x in ['AH', '>', '<']):\n",
    "        return 'home_win'\n",
    "    elif col.endswith('D') and not any(x in col for x in ['AH', '>', '<']):\n",
    "        return 'draw'\n",
    "    elif col.endswith('A') and not any(x in col for x in ['AH', '>', '<']):\n",
    "        return 'away_win'\n",
    "    \n",
    "    # Over/Under 2.5 Goals Market\n",
    "    elif '>2.5' in col or 'O2.5' in col:\n",
    "        return 'over_2_5'\n",
    "    elif '<2.5' in col or 'U2.5' in col:\n",
    "        return 'under_2_5'\n",
    "    \n",
    "    # Asian Handicap Market\n",
    "    elif 'AH' in col and col.endswith('H'):\n",
    "        return 'ah_home'\n",
    "    elif 'AH' in col and col.endswith('A'):\n",
    "        return 'ah_away'\n",
    "    elif 'AH' in col and not col.endswith(('H', 'A')):\n",
    "        return 'ah_handicap'\n",
    "    \n",
    "    # Other markets\n",
    "    elif 'C>' in col:  # Corner markets\n",
    "        return 'corners'\n",
    "    elif any(x in col for x in ['FKCH', 'FKCA']):  # Free kicks\n",
    "        return 'free_kicks'\n",
    "    \n",
    "    return 'other'\n",
    "\n",
    "# Group betting columns by market type\n",
    "market_groups = {}\n",
    "for col in betting_cols:\n",
    "    market_type = categorize_betting_market(col)\n",
    "    market_groups.setdefault(market_type, []).append(col)\n",
    "\n",
    "print(f\"\\nBetting odds grouped by market type:\")\n",
    "for market_type, columns in market_groups.items():\n",
    "    avg_missing = np.mean([item['missing_pct'] for item in betting_missing_analysis if item['column'] in columns])\n",
    "    print(f\"  {market_type}: {len(columns)} columns (avg missing: {avg_missing:.1f}%)\")\n",
    "\n",
    "# Apply cross-bookmaker median imputation within each market for each match\n",
    "total_imputed = 0\n",
    "markets_processed = []\n",
    "\n",
    "for market_type, columns in market_groups.items():\n",
    "    if len(columns) > 1 and market_type != 'other':  # Only process markets with multiple bookmakers\n",
    "        print(f\"\\nProcessing {market_type} market ({len(columns)} columns)...\")\n",
    "        markets_processed.append(market_type)\n",
    "        \n",
    "        # Check how much data we have for this market\n",
    "        market_data_availability = []\n",
    "        for col in columns:\n",
    "            non_missing = all_matches[col].notna().sum()\n",
    "            market_data_availability.append(non_missing)\n",
    "        \n",
    "        if max(market_data_availability) > 1000:  # Only process if we have reasonable data\n",
    "            match_imputed = 0\n",
    "            \n",
    "            # Process each match individually\n",
    "            for idx in all_matches.index:\n",
    "                # Get odds for this match across all bookmakers for this market\n",
    "                match_odds = all_matches.loc[idx, columns]\n",
    "                \n",
    "                # If any values are missing but others exist, use median of available bookmakers\n",
    "                if match_odds.isnull().any() and not match_odds.isnull().all():\n",
    "                    match_median = match_odds.median()\n",
    "                    \n",
    "                    # Fill missing values with the cross-bookmaker median for this match\n",
    "                    for col in columns:\n",
    "                        if pd.isnull(all_matches.loc[idx, col]):\n",
    "                            all_matches.loc[idx, col] = match_median\n",
    "                            total_imputed += 1\n",
    "                            match_imputed += 1\n",
    "            \n",
    "            print(f\"  {market_type}: {match_imputed} values imputed using cross-bookmaker median\")\n",
    "\n",
    "print(f\"\\nCross-bookmaker imputation completed: {total_imputed} values imputed across {len(markets_processed)} markets\")\n",
    "\n",
    "# For remaining missing values, apply conservative strategy\n",
    "# Only use overall median fallback for markets with reasonable data coverage\n",
    "remaining_imputed = 0\n",
    "columns_fully_imputed = []\n",
    "\n",
    "for market_type, columns in market_groups.items():\n",
    "    if market_type in ['home_win', 'draw', 'away_win', 'over_2_5', 'under_2_5']:  # Core markets only\n",
    "        for col in columns:\n",
    "            missing_before = all_matches[col].isnull().sum()\n",
    "            data_coverage = (all_matches[col].notna().sum() / len(all_matches)) * 100\n",
    "            \n",
    "            # Only apply fallback imputation if we have at least 10% data coverage\n",
    "            if missing_before > 0 and data_coverage >= 10:\n",
    "                overall_median = all_matches[col].median()\n",
    "                all_matches[col] = all_matches[col].fillna(overall_median)\n",
    "                remaining_imputed += missing_before\n",
    "                columns_fully_imputed.append(col)\n",
    "\n",
    "print(f\"Overall median fallback applied to {len(columns_fully_imputed)} columns: {remaining_imputed} values imputed\")\n",
    "\n",
    "# For columns with <10% data coverage, we'll exclude them from modeling rather than impute\n",
    "excluded_cols = []\n",
    "for col in betting_cols:\n",
    "    data_coverage = (all_matches[col].notna().sum() / len(all_matches)) * 100\n",
    "    if data_coverage < 10:\n",
    "        excluded_cols.append(col)\n",
    "\n",
    "print(f\"\\nColumns excluded due to <10% data coverage: {len(excluded_cols)}\")\n",
    "print(\"These will be excluded from the extended dataset to avoid poor imputation quality\")\n",
    "\n",
    "# Verify imputation results for key markets\n",
    "print(f\"\\nVerification - missing values after imputation for key betting markets:\")\n",
    "key_betting_cols = [col for col in betting_cols if col not in excluded_cols][:15]  # Check sample\n",
    "final_missing = 0\n",
    "for col in key_betting_cols:\n",
    "    missing = all_matches[col].isnull().sum()\n",
    "    if missing > 0:\n",
    "        data_coverage = (all_matches[col].notna().sum() / len(all_matches)) * 100\n",
    "        print(f\"  {col}: {missing} missing ({data_coverage:.1f}% coverage)\")\n",
    "        final_missing += missing\n",
    "\n",
    "if final_missing == 0:\n",
    "    print(\"Key betting odds successfully imputed\")\n",
    "else:\n",
    "    print(f\"⚠ {final_missing} missing values remain in key betting columns\")\n",
    "\n",
    "# Update betting features list to exclude low-coverage columns\n",
    "print(f\"\\nUpdating betting features list:\")\n",
    "print(f\"Original betting columns: {len(betting_cols)}\")\n",
    "print(f\"Excluded low-coverage columns: {len(excluded_cols)}\")\n",
    "print(f\"Final betting columns for modeling: {len(betting_cols) - len(excluded_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Outlier detection and handling\n",
    "\n",
    "Following the methodology from Week1 (house pricing), we'll use z-score analysis to detect outliers in match statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical columns for outlier detection\n",
    "match_stats_cols = ['HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "numerical_cols = ['FTHG', 'FTAG', 'HTHG', 'HTAG'] + match_stats_cols\n",
    "\n",
    "# Calculate z-scores for numerical columns\n",
    "print(\"Outlier analysis using z-score > 3:\")\n",
    "outlier_counts = {}\n",
    "\n",
    "for col in numerical_cols:\n",
    "    if col in all_matches.columns:\n",
    "        z_scores = np.abs(zscore(all_matches[col].dropna()))\n",
    "        outliers = (z_scores > 3).sum()\n",
    "        outlier_counts[col] = outliers\n",
    "        if outliers > 0:\n",
    "            print(f\"{col}: {outliers} outliers ({outliers/len(all_matches)*100:.2f}%)\")\n",
    "\n",
    "# Look at extreme cases\n",
    "print(f\"\\nExamples of potential outliers:\")\n",
    "print(f\"Highest total goals: {all_matches['FTHG'].max() + all_matches['FTAG'].max()}\")\n",
    "print(f\"Most shots in a match: {all_matches['HS'].max() + all_matches['AS'].max()}\")\n",
    "print(f\"Most cards in a match: {all_matches['HY'].max() + all_matches['AY'].max()}\")\n",
    "\n",
    "# Visualize outliers for key variables\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "key_vars = ['FTHG', 'FTAG', 'HS', 'AS']\n",
    "\n",
    "for i, var in enumerate(key_vars):\n",
    "    row, col = i // 2, i % 2\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Box plot to show outliers\n",
    "    all_matches[var].plot(kind='box', ax=ax)\n",
    "    ax.set_title(f'Box Plot of {var}')\n",
    "    ax.set_ylabel(var)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# For football data, we'll be more conservative with outlier removal\n",
    "# as extreme scores can be legitimate (unlike house prices)\n",
    "print(f\"\\nDecision: Keep outliers for football data as high scores/stats can be legitimate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature engineering\n",
    "\n",
    "Based on soccer domain knowledge and the course materials, we'll create meaningful features that could help predict Over/Under 2.5 goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Target variable creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the main target variable: Over/Under 2.5 goals\n",
    "all_matches['total_goals'] = all_matches['FTHG'] + all_matches['FTAG']\n",
    "all_matches['over_2_5'] = (all_matches['total_goals'] > 2.5).astype(int)\n",
    "\n",
    "print(\"Target variable distribution:\")\n",
    "print(all_matches['over_2_5'].value_counts())\n",
    "print(f\"Over 2.5 rate: {all_matches['over_2_5'].mean():.2%}\")\n",
    "\n",
    "# Also create alternative targets for analysis\n",
    "all_matches['over_1_5'] = (all_matches['total_goals'] > 1.5).astype(int)\n",
    "all_matches['over_3_5'] = (all_matches['total_goals'] > 3.5).astype(int)\n",
    "\n",
    "print(f\"\\nOther thresholds:\")\n",
    "print(f\"Over 1.5 rate: {all_matches['over_1_5'].mean():.2%}\")\n",
    "print(f\"Over 3.5 rate: {all_matches['over_3_5'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Basic feature engineering\n",
    "\n",
    "Creating features that capture match dynamics and team performance patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Shot efficiency features\n",
    "all_matches['home_shot_accuracy'] = all_matches['HST'] / (all_matches['HS'] + 0.001)  # avoid division by zero\n",
    "all_matches['away_shot_accuracy'] = all_matches['AST'] / (all_matches['AS'] + 0.001)\n",
    "all_matches['total_shots'] = all_matches['HS'] + all_matches['AS']\n",
    "all_matches['total_shots_on_target'] = all_matches['HST'] + all_matches['AST']\n",
    "\n",
    "# 2. Attacking vs Defensive balance\n",
    "all_matches['shot_dominance'] = (all_matches['HS'] - all_matches['AS']) / (all_matches['HS'] + all_matches['AS'] + 0.001)\n",
    "all_matches['corner_dominance'] = (all_matches['HC'] - all_matches['AC']) / (all_matches['HC'] + all_matches['AC'] + 0.001)\n",
    "\n",
    "# 3. Game intensity features\n",
    "all_matches['total_fouls'] = all_matches['HF'] + all_matches['AF']\n",
    "all_matches['total_cards'] = all_matches['HY'] + all_matches['AY'] + all_matches['HR'] + all_matches['AR']\n",
    "all_matches['card_intensity'] = all_matches['total_cards'] / (all_matches['total_fouls'] + 0.001)\n",
    "\n",
    "# 4. Half-time patterns\n",
    "all_matches['ht_total_goals'] = all_matches['HTHG'] + all_matches['HTAG']\n",
    "all_matches['second_half_goals'] = all_matches['total_goals'] - all_matches['ht_total_goals']\n",
    "\n",
    "# 5. League tier (lower tiers might have different patterns)\n",
    "all_matches['league_tier'] = all_matches['Div'].str[-1].astype(int)\n",
    "\n",
    "# 6. Season timing features\n",
    "all_matches['month'] = all_matches['Date'].dt.month\n",
    "all_matches['is_weekend'] = all_matches['Date'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "\n",
    "print(\"Created basic engineered features:\")\n",
    "new_features = ['home_shot_accuracy', 'away_shot_accuracy', 'total_shots', 'total_shots_on_target',\n",
    "               'shot_dominance', 'corner_dominance', 'total_fouls', 'total_cards', 'card_intensity',\n",
    "               'ht_total_goals', 'second_half_goals', 'league_tier', 'month', 'is_weekend']\n",
    "\n",
    "for feature in new_features:\n",
    "    print(f\"- {feature}: mean={all_matches[feature].mean():.3f}, std={all_matches[feature].std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check what columns we actually have available\n",
    "available_cols = all_matches.columns.tolist()\n",
    "\n",
    "# Check for core match info columns\n",
    "core_match_info = ['Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG', 'HTR']\n",
    "\n",
    "# Check for match statistics columns from notes.txt\n",
    "match_stats_from_notes = ['Attendance', 'Referee', 'HS', 'AS', 'HST', 'AST', 'HHW', 'AHW', 'HC', 'AC', \n",
    "                         'HF', 'AF', 'HFKC', 'AFKC', 'HO', 'AO', 'HY', 'AY', 'HR', 'AR', 'HBP', 'ABP']\n",
    "\n",
    "# Identify all available core and match statistics columns\n",
    "basic_available = [col for col in core_match_info if col in available_cols]\n",
    "extended_available = [col for col in core_match_info + match_stats_from_notes if col in available_cols]\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Basic dataset core columns available: {len(basic_available)}\")\n",
    "print(f\"Extended dataset core columns available: {len(extended_available)}\")\n",
    "\n",
    "# Identify categorical and numerical columns for modeling\n",
    "categorical_features = ['Div', 'HomeTeam', 'AwayTeam', 'Country', 'FTR', 'HTR', 'Referee']  # League division, teams, and results\n",
    "ordinal_features = ['league_tier', 'month']  # Features with natural ordering\n",
    "\n",
    "# BASIC DATASET: Core match information (what would be available from basic match reports)\n",
    "basic_core_features = [col for col in ['Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR'] if col in all_matches.columns]\n",
    "\n",
    "# Engineered features that can be created from basic core columns only\n",
    "basic_engineered_features = [\n",
    "    'total_goals',   # Goal-based features\n",
    "    'league_tier', 'month', 'is_weekend'  # Date/league features\n",
    "]\n",
    "\n",
    "# EXTENDED DATASET: All available match data including detailed statistics\n",
    "extended_core_features = [col for col in [\n",
    "    # Core match info\n",
    "    'Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG', 'HTR',\n",
    "    # Match statistics\n",
    "    'Attendance', 'Referee', 'HS', 'AS', 'HST', 'AST', 'HHW', 'AHW', 'HC', 'AC', \n",
    "    'HF', 'AF', 'HFKC', 'AFKC', 'HO', 'AO', 'HY', 'AY', 'HR', 'AR', 'HBP', 'ABP'\n",
    "] if col in all_matches.columns]\n",
    "\n",
    "# All engineered features (using rich match statistics)\n",
    "extended_engineered_features = [\n",
    "    'total_goals', 'ht_total_goals', 'second_half_goals',  # Goal-based\n",
    "    'home_shot_accuracy', 'away_shot_accuracy', 'total_shots', 'total_shots_on_target',  # Shot-based\n",
    "    'shot_dominance', 'corner_dominance', 'total_fouls', 'total_cards', 'card_intensity',  # Game dynamics\n",
    "    'league_tier', 'month', 'is_weekend'  # Date/league features\n",
    "]\n",
    "\n",
    "# Extended features (betting odds - only high-quality columns after imputation)\n",
    "betting_features = []\n",
    "for col in all_matches.columns:\n",
    "    # Check if it's a betting column and has good data coverage (>10%)\n",
    "    if any(bookmaker in col for bookmaker in ['B365', 'BW', 'PS', 'IW', 'LB', 'WH', 'SJ', 'VC', 'BF', '1XB']) and col not in categorical_features:\n",
    "        data_coverage = (all_matches[col].notna().sum() / len(all_matches)) * 100\n",
    "        if data_coverage >= 10:  # Only include columns with at least 10% data coverage\n",
    "            betting_features.append(col)\n",
    "\n",
    "print(f\"\\nBASIC DATASET:\")\n",
    "print(f\"  Core features: {len(basic_core_features)} - {basic_core_features}\")\n",
    "print(f\"  Engineered features: {len(basic_engineered_features)} - {basic_engineered_features}\")\n",
    "print(f\"  Total basic features: {len(basic_core_features + basic_engineered_features)}\")\n",
    "\n",
    "print(f\"\\nEXTENDED DATASET:\")\n",
    "print(f\"  Core features: {len(extended_core_features)}\")\n",
    "print(f\"  Engineered features: {len(extended_engineered_features)}\")\n",
    "print(f\"  Betting features with >10% coverage: {len(betting_features)}\")\n",
    "print(f\"  Total extended features: {len(extended_core_features + extended_engineered_features + betting_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Dataset preparation and train-test split\n",
    "\n",
    "For time series data like sports matches, we need to be careful about temporal splitting to avoid data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Temporal train-test split\n",
    "\n",
    "Since this is time series data, we'll split chronologically to simulate real-world predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by date for temporal split\n",
    "all_matches_sorted = all_matches.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "# Use 80% for training (chronologically earlier), 20% for testing (more recent)\n",
    "split_date = all_matches_sorted['Date'].quantile(0.8)\n",
    "print(f\"Split date: {split_date}\")\n",
    "\n",
    "train_mask = all_matches_sorted['Date'] <= split_date\n",
    "test_mask = all_matches_sorted['Date'] > split_date\n",
    "\n",
    "train_data = all_matches_sorted[train_mask].copy()\n",
    "test_data = all_matches_sorted[test_mask].copy()\n",
    "\n",
    "print(f\"Training set: {len(train_data)} matches ({train_data['Date'].min()} to {train_data['Date'].max()})\")\n",
    "print(f\"Test set: {len(test_data)} matches ({test_data['Date'].min()} to {test_data['Date'].max()})\")\n",
    "print(f\"Train Over 2.5 rate: {train_data['over_2_5'].mean():.2%}\")\n",
    "print(f\"Test Over 2.5 rate: {test_data['over_2_5'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Basic vs Extended datasets\n",
    "\n",
    "Create two datasets as mentioned in the project goals:\n",
    "- **Basic dataset**: Core match statistics only\n",
    "- **Extended dataset**: Including betting odds and additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets for modeling\n",
    "basic_features = basic_core_features + basic_engineered_features\n",
    "extended_features = extended_core_features + extended_engineered_features + betting_features\n",
    "\n",
    "# Exclude columns that shouldn't be used directly in modeling\n",
    "excluded_from_modeling = ['Date', 'Time']  # These are used for feature engineering but not direct modeling\n",
    "\n",
    "# We'll handle categorical encoding in the modeling phase\n",
    "target = 'over_2_5'\n",
    "\n",
    "print(f\"Basic model features: {len(basic_features)}\")\n",
    "print(f\"Extended model features: {len(extended_features)}\")\n",
    "\n",
    "# Create datasets (without categorical encoding for now)\n",
    "def create_dataset(data, features, target_col):\n",
    "    \"\"\"Create feature matrix and target vector\"\"\"\n",
    "    # Only include features that exist in the data and exclude date/time columns for modeling\n",
    "    available_features = [f for f in features if f in data.columns and f not in excluded_from_modeling]\n",
    "    \n",
    "    X = data[available_features].copy()\n",
    "    y = data[target_col].copy()\n",
    "    \n",
    "    return X, y, available_features\n",
    "\n",
    "# Basic datasets (core columns + basic engineered features only)\n",
    "X_train_basic, y_train, basic_features_final = create_dataset(train_data, basic_features, target)\n",
    "X_test_basic, y_test, _ = create_dataset(test_data, basic_features, target)\n",
    "\n",
    "# Extended datasets (all match stats + betting odds + all engineered features)\n",
    "X_train_extended, _, extended_features_final = create_dataset(train_data, extended_features, target)\n",
    "X_test_extended, _, _ = create_dataset(test_data, extended_features, target)\n",
    "\n",
    "print(f\"\\nFinal feature counts:\")\n",
    "print(f\"Basic features available: {len(basic_features_final)}\")\n",
    "print(f\"Extended features available: {len(extended_features_final)}\")\n",
    "\n",
    "print(f\"\\nBasic features: {basic_features_final}\")\n",
    "print(f\"\\nExtended features sample (first 20): {extended_features_final[:20]}\")\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"X_train_basic: {X_train_basic.shape}\")\n",
    "print(f\"X_test_basic: {X_test_basic.shape}\")\n",
    "print(f\"X_train_extended: {X_train_extended.shape}\")\n",
    "print(f\"X_test_extended: {X_test_extended.shape}\")\n",
    "\n",
    "# Check for missing values in final datasets\n",
    "print(f\"\\nMissing values in basic features:\")\n",
    "print(X_train_basic.isnull().sum().sum())\n",
    "print(f\"Missing values in extended features:\")\n",
    "print(X_train_extended.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Save processed datasets\n",
    "\n",
    "Save the preprocessed data for use in modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed datasets\n",
    "datasets = {\n",
    "    'X_train_basic': X_train_basic_imputed,\n",
    "    'X_test_basic': X_test_basic_imputed,\n",
    "    'X_train_extended': X_train_extended_imputed,\n",
    "    'X_test_extended': X_test_extended_imputed,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test\n",
    "}\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    filepath = f\"{OUTPUT_DIR}/{name}.pkl\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "# Also save feature names\n",
    "feature_info = {\n",
    "    'basic_features': basic_features_final,\n",
    "    'extended_features': extended_features_final,\n",
    "    'target': target\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/feature_info.pkl\", 'wb') as f:\n",
    "    pickle.dump(feature_info, f)\n",
    "\n",
    "# Save also as CSV for easy inspection\n",
    "# X_train_basic_imputed.to_csv(f\"{OUTPUT_DIR}/X_train_basic.csv\", index=False)\n",
    "# X_train_extended_imputed.to_csv(f\"{OUTPUT_DIR}/X_train_extended.csv\", index=False)\n",
    "# y_train.to_csv(f\"{OUTPUT_DIR}/y_train.csv\", index=False)\n",
    "# y_test.to_csv(f\"{OUTPUT_DIR}/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Preprocessing Summary\n",
    "\n",
    "## What was accomplished:\n",
    "\n",
    "### Data Loading & Cleaning:\n",
    "- Loaded 42,593 matches from 11 countries and 21 leagues\n",
    "- Handled unnamed columns (100% missing data)\n",
    "- Normalized league codes (E0→E1, SC0→SC1, etc.)\n",
    "- Corrected English/Scottish yellow card counts\n",
    "- Converted data types properly (datetime, categorical, numerical)\n",
    "- Imputed missing values in key match statistics (<0.1% missing)\n",
    "\n",
    "### Exploratory Data Analysis:\n",
    "- Target variable (Over/Under 2.5 goals) perfectly balanced: 49.99% / 50.01%\n",
    "- Analyzed missing data patterns (betting odds 80%+ missing, match stats <0.1%)\n",
    "- Examined country/league distributions\n",
    "- Outlier analysis using z-scores (kept outliers as legitimate in football)\n",
    "\n",
    "### Feature Engineering:\n",
    "- Created target variable: `over_2_5` (Over/Under 2.5 goals)\n",
    "- Shot efficiency features: accuracy, total shots, dominance measures\n",
    "- Game intensity features: fouls, cards, card intensity\n",
    "- Temporal features: half-time patterns, second-half goals\n",
    "- League and seasonal features: tier, month, weekend indicator\n",
    "\n",
    "### Dataset Preparation:\n",
    "- **Temporal train-test split**: 80% train (2019-2024) / 20% test (2024-2025)\n",
    "- **Basic dataset**: ~7 features (ONLY original core columns: FTHG, FTAG, HTHG, HTAG + goal-based engineered features)\n",
    "- **Extended dataset**: ~40+ features (all match statistics + betting odds + all engineered features)\n",
    "- Missing value imputation for both datasets\n",
    "- Saved processed data for modeling\n",
    "\n",
    "## Data Quality:\n",
    "- **Training set**: 34,085 matches (49.56% Over 2.5)\n",
    "- **Test set**: 8,508 matches (51.72% Over 2.5)\n",
    "- **No missing values** in final processed datasets\n",
    "- **Temporally sorted** to prevent data leakage\n",
    "\n",
    "## Key Distinction:\n",
    "- **Basic Model**: Uses only original core data (goals) + simple derived features\n",
    "- **Extended Model**: Uses rich match statistics (shots, fouls, cards) + betting odds + complex features"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NBksARdgdMkP",
    "Qx3gpH8TdToi",
    "O5SNQWIMhxZA",
    "NNyVnO2Bk4zo",
    "XMiRfCrVzDVd",
    "fwwBVM5ex2SA"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
