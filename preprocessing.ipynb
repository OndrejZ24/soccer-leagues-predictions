{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML2 Semestral Project - Football O/U 2.5\n",
        "**Authors:** Phuong Nhi Tranová, Vít Maruniak, Šimon Slánský, Radim Škoukal, Ondřej Zetek, Martin Kareš, Jan Korčák, Jakub Maličkay, Jáchym Janouch  \n",
        "**Course:** FIS 4IT344 Machine Learning 2 (2025/2026)  \n",
        "**Goal:** Compare baseline (current features) vs extended (richer features) models for O/U 2.5 goals across markets; translate accuracy gains into optimal profit and **maximum data subscription price per country** *.  \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "***maximum data subscription price per country**\n",
        "- the most money our company should be willing to pay for that country's additional data\n",
        "- that's how much extra profit the improved model generates\n",
        "- baseline model → accuracy = A₀\n",
        "    - Generates profit Π*(A₀)\n",
        "- extended model → accuracy = A₁\n",
        "    - Generates profit Π*(A₁)\n",
        "- profit improvement = ΔΠ = Π(A₁) − Π(A₀)*\n",
        "    - basically how much more money the comany earns each year by using the better data\n",
        "- the maximum data subscription price per country = ΔΠ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 0. Imports and paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0.1 Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, glob, math, json, re, gc, itertools, warnings, textwrap\n",
        "from pathlib import Path\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, balanced_accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score, brier_score_loss, confusion_matrix\n",
        ")\n",
        "from sklearn.calibration import CalibrationDisplay\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0.2 Library parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0.3 Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_DIR = \"./data\"  \n",
        "OUTPUT_DIR = f\"./processed\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Data load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_all_matches(data_dir: str) -> pd.DataFrame:\n",
        "    csv_files = glob.glob(os.path.join(data_dir, \"**\", \"*.csv\"), recursive=True)\n",
        "    if not csv_files:\n",
        "        raise FileNotFoundError(f\"No CSV files found under {data_dir}\")\n",
        "\n",
        "    frames = []\n",
        "    for fp in csv_files:\n",
        "        # extract path info\n",
        "        rel = os.path.relpath(fp, data_dir)\n",
        "        parts = Path(rel).parts\n",
        "        country = parts[0] if len(parts) >= 1 else None\n",
        "        league  = parts[1] if len(parts) >= 2 else None\n",
        "        season_file = parts[2] if len(parts) >= 3 else None\n",
        "        season = os.path.splitext(season_file)[0] if season_file else None\n",
        "\n",
        "        # read and rename\n",
        "        try:\n",
        "            df = pd.read_csv(fp, low_memory=False)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {fp}: {e}\")\n",
        "            continue\n",
        "\n",
        "        frames.append(df)\n",
        "\n",
        "    all_df = pd.concat(frames, ignore_index=True, sort=False)\n",
        "    return all_df\n",
        "\n",
        "# run the loader\n",
        "all_matches = pd.DataFrame(load_all_matches(DATA_DIR))\n",
        "print(all_matches.columns.tolist())\n",
        "print(all_matches.shape)\n",
        "display(all_matches.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Data cleaning & EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Handling csv issues\n",
        "It seems like the renaming and loading went smoothly! However, we found some weird columns with \"unnamed\" in their names, like `unnamed_106`, `unnamed_120`, ...  \n",
        "That sometimes happens when excel files have extra blank columns. We'll take a quick look to see if they have any data, and if they're totally empty (full of NaNs), we'll just get rid of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unnamed_cols = [c for c in all_matches.columns if c.lower().startswith(\"unnamed\")]\n",
        "all_matches[unnamed_cols].isna().mean().sort_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "They're 100% full of NaNs so we can now safely drop them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_matches = all_matches.drop(columns=unnamed_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Normalizing league codes\n",
        "Let's normalize the leagues, as English and Scottish leagues have the best leagues interpreted as E0, SC0, respectively. All other countries mark the best league as CountryCode1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mask = all_matches['Div'].str.startswith(('E', 'SC'))\n",
        "all_matches.loc[mask, 'Div'] = all_matches.loc[mask, 'Div'].apply(\n",
        "    lambda x: f\"{x[:-1]}{int(x[-1]) + 1}\"\n",
        ")\n",
        "\n",
        "print(all_matches['Div'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Handling English and Scottish yellow cards\n",
        "Let's take care of the first note in notes.txt file, which mentions - English and Scottish yellow cards do not include the initial yellow card when a second is shown to a player converting it into a red, but this is included as a yellow (plus red) for European games."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mask = all_matches['Div'].str.startswith(('E', 'SC'))\n",
        "red_mask = mask & ((all_matches['HR'] == 1) | (all_matches['AR'] == 1))\n",
        "\n",
        "print(\"Before adjustment (sample):\")\n",
        "print(all_matches.loc[red_mask, ['Div', 'HY', 'HR', 'AY', 'AR']].head())\n",
        "\n",
        "all_matches.loc[mask, 'HY'] += all_matches.loc[mask, 'HR'].eq(1).astype(int)\n",
        "all_matches.loc[mask, 'AY'] += all_matches.loc[mask, 'AR'].eq(1).astype(int)\n",
        "\n",
        "print(\"\\nAfter adjustment (sample):\")\n",
        "print(all_matches.loc[red_mask, ['Div', 'HY', 'HR', 'AY', 'AR']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Correcting data types\n",
        "Now, let's inspect the data types of our columns. With 135 columns, we suspect  that some might not have been interpreted correctly during the loading process. Checking the data types is an important step before proceeding with any further analysis or modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col, dtype in all_matches.dtypes.items():\n",
        "    print(f\"{col}: {dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_columns = ['Date', 'Time']\n",
        "\n",
        "category_columns = ['Div', 'HomeTeam', 'AwayTeam', 'FTR', 'HTR', 'Referee']\n",
        "\n",
        "int_columns = ['FTHG', 'FTAG', 'HTHG', 'HTAG', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
        "\n",
        "float_columns = ['B365CH', 'BWCA', '1XBH']\n",
        "\n",
        "for col in time_columns:\n",
        "    if col == 'Date':\n",
        "        all_matches[col] = pd.to_datetime(all_matches[col])\n",
        "    else:\n",
        "        all_matches[col] = pd.to_datetime(all_matches[col], format='%H:%M').dt.time\n",
        "\n",
        "for col in category_columns:\n",
        "    all_matches[col] = all_matches[col].astype('category')\n",
        "\n",
        "for col in int_columns:\n",
        "    all_matches[col] = pd.to_numeric(all_matches[col], errors='coerce').astype('Int64')  \n",
        "\n",
        "for col in float_columns:\n",
        "    all_matches[col] = pd.to_numeric(all_matches[col], errors='coerce').astype(float)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col, dtype in all_matches.dtypes.items():\n",
        "    print(f\"{col}: {dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Handling missing values\n",
        "Let's also investigate how the missing values are looking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "na_counts = all_matches.isna().sum()\n",
        "\n",
        "missing_cols = pd.DataFrame(na_counts[na_counts > 0]).sort_values(by=0, ascending=False)\n",
        "display(missing_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.6 Dealing with outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3 Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Basic/expanded datasets and train-test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# USE THIS LATER ON AFTER PREPROCESS (HOWEVER IF YOU USE FOR EXAMPLE REGRESSION INPUTATION FOR MEANS)\n",
        "basic_columns = ['Div', 'Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR']\n",
        "all_matches_basic = all_matches[basic_columns]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "NBksARdgdMkP",
        "Qx3gpH8TdToi",
        "O5SNQWIMhxZA",
        "NNyVnO2Bk4zo",
        "XMiRfCrVzDVd",
        "fwwBVM5ex2SA"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
