{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML2 Semestral Project - Football O/U 2.5\n",
    "**Authors:** Phuong Nhi Tranová, Vít Maruniak, Šimon Slánský, Radim Škoukal, Ondřej Zetek, Martin Kareš, Jan Korčák, Jakub Maličkay, Jáchym Janouch  \n",
    "**Course:** FIS 4IT344 Machine Learning 2 (2025/2026)  \n",
    "**Goal:** Compare baseline (current features) vs extended (richer features) models for O/U 2.5 goals across markets; translate accuracy gains into optimal profit and **maximum data subscription price per country** *.  \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "***maximum data subscription price per country**\n",
    "- the most money our company should be willing to pay for that country's additional data\n",
    "- that's how much extra profit the improved model generates\n",
    "- baseline model → accuracy = A₀\n",
    "    - Generates profit Π*(A₀)\n",
    "- extended model → accuracy = A₁\n",
    "    - Generates profit Π*(A₁)\n",
    "- profit improvement = ΔΠ = Π(A₁) − Π(A₀)*\n",
    "    - basically how much more money the comany earns each year by using the better data\n",
    "- the maximum data subscription price per country = ΔΠ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports and paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Library parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "OUTPUT_DIR = f\"./processed\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_matches(data_dir: str) -> pd.DataFrame:\n",
    "    csv_files = glob.glob(os.path.join(data_dir, \"**\", \"*.csv\"), recursive=True)\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSV files found under {data_dir}\")\n",
    "\n",
    "    frames = []\n",
    "    for fp in csv_files:\n",
    "        # extract path info\n",
    "        rel = os.path.relpath(fp, data_dir)\n",
    "        parts = Path(rel).parts\n",
    "        country = parts[0] if len(parts) >= 1 else None\n",
    "        league  = parts[1] if len(parts) >= 2 else None\n",
    "        season_file = parts[2] if len(parts) >= 3 else None\n",
    "        season = os.path.splitext(season_file)[0] if season_file else None\n",
    "\n",
    "        # read and rename\n",
    "        try:\n",
    "            df = pd.read_csv(fp, low_memory=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {fp}: {e}\")\n",
    "            continue\n",
    "\n",
    "        frames.append(df)\n",
    "\n",
    "    all_df = pd.concat(frames, ignore_index=True, sort=False)\n",
    "    return all_df\n",
    "\n",
    "# run the loader\n",
    "all_matches = pd.DataFrame(load_all_matches(DATA_DIR))\n",
    "print(all_matches.columns.tolist())\n",
    "print(all_matches.shape)\n",
    "display(all_matches.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis\n",
    "\n",
    "Before proceeding with data cleaning, let's understand our data better through comprehensive exploratory data analysis. This will help us make informed decisions about preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Shape and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset shape: {all_matches.shape}\")\n",
    "print(f\"Number of seasons/countries covered:\")\n",
    "print(f\"Countries: {all_matches['Div'].str[:-1].nunique()}\")\n",
    "print(f\"Leagues: {all_matches['Div'].nunique()}\")\n",
    "print(f\"Date range: {all_matches['Date'].min()} to {all_matches['Date'].max()}\")\n",
    "\n",
    "# Check basic statistics\n",
    "print(f\"\\nBasic goal statistics:\")\n",
    "print(f\"Total goals per match stats:\")\n",
    "total_goals = all_matches['FTHG'] + all_matches['FTAG']\n",
    "print(total_goals.describe())\n",
    "\n",
    "print(f\"\\nOver/Under 2.5 goals distribution:\")\n",
    "over_2_5 = (total_goals > 2.5).astype(int)\n",
    "print(f\"Over 2.5: {over_2_5.sum()} ({over_2_5.mean():.2%})\")\n",
    "print(f\"Under 2.5: {(~over_2_5.astype(bool)).sum()} ({(1-over_2_5.mean()):.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our target variable (Over/Under 2.5 goals) is perfectly balanced with almost exactly 50/50 split, which is ideal for classification. Mainly because the model won't be biased toward either class and we can use standard accuracy but also because we won't have to do any kind of resampling or rebalancing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed missing values analysis\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'column': all_matches.columns,\n",
    "    'missing_count': all_matches.isnull().sum(),\n",
    "    'missing_percentage': (all_matches.isnull().sum() / len(all_matches)) * 100,\n",
    "    'dtype': all_matches.dtypes\n",
    "})\n",
    "\n",
    "# Filter to show only columns with missing values\n",
    "missing_analysis = missing_analysis[missing_analysis['missing_count'] > 0].sort_values('missing_percentage', ascending=False)\n",
    "\n",
    "print(f\"Columns with missing values: {len(missing_analysis)}\")\n",
    "print(f\"Total columns: {len(all_matches.columns)}\")\n",
    "print(f\"\\nTop 20 columns with highest missing percentage:\")\n",
    "display(missing_analysis.head(20))\n",
    "\n",
    "# Check missing patterns in key variables\n",
    "key_stats = ['HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "print(f\"\\nMissing data in key match statistics:\")\n",
    "for stat in key_stats:\n",
    "    if stat in all_matches.columns:\n",
    "        missing_pct = (all_matches[stat].isnull().sum() / len(all_matches)) * 100\n",
    "        print(f\"{stat}: {missing_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing data analysis reveals that:\n",
    "1. **Betting odds** have the highest missing percentages (80%+) - this is expected as not all bookmakers operate in all leagues/seasons\n",
    "2. **Key match statistics** (shots, corners, fouls, cards) have very low missing rates (<0.1%), which is excellent for our modeling\n",
    "3. Most missing data is in betting-related columns, which we can handle appropriately\n",
    "\n",
    "also we have found 4 unnamed columns that are 100% missing. they're most likely artifacts from csv exports so they're definitely safe to drop outright"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets do a bit more of a in depth analysis, shall we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = all_matches.copy()\n",
    "\n",
    "# missingness flag\n",
    "stats_cols = ['HS','AS','HST','AST','HF','AF','HC','AC','HY','AY','HR','AR']\n",
    "for c in stats_cols:\n",
    "    if c in raw.columns:\n",
    "        raw[f'isna_{c}'] = raw[c].isna().astype(int)\n",
    "\n",
    "# Row-level summary: how many of the 12 stats are missing in the same row?\n",
    "flag_cols = [f'isna_{c}' for c in stats_cols if f'isna_{c}' in raw.columns]\n",
    "raw['missing_count_stats'] = raw[flag_cols].sum(axis=1)\n",
    "\n",
    "# Quick overview\n",
    "print(raw['missing_count_stats'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the rows seem to have no missigness/ However, there are 41 rows are missing all 12 variables, which seems pretty clustered. Suggesting that the missing data likely stem from a specific data source or a batch issue rather than random omission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single-stat missing % (already computed as flags)\n",
    "single_rates = (raw[flag_cols].mean() * 100)\n",
    "single_rates.index = [c.replace('isna_', '') for c in single_rates.index]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "ax.bar(single_rates.index, single_rates.values)\n",
    "ax.set_title('Missingness by variables (%)')\n",
    "ax.set_ylabel('% missing')\n",
    "ax.set_xlabel('stat')\n",
    "ax.set_xticklabels(single_rates.index, rotation=45, ha='right')\n",
    "for i, v in enumerate(single_rates.values):\n",
    "    ax.text(i, v, f'{v:.3f}%', ha='center', va='bottom', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "misingness seems uniformly low across all variables, there seems to be no issue with a variable specific collection issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "cmap = 'plasma'\n",
    "\n",
    "# Extract country from Div column (e.g., 'E1' -> 'E', 'SP2' -> 'SP')\n",
    "raw['country_code'] = raw['Div'].str[:-1]\n",
    "\n",
    "# 1️⃣ Country × Stat\n",
    "if 'country_code' in raw.columns:\n",
    "    M1 = raw.groupby('country_code')[flag_cols].mean().mul(100)\n",
    "    order = M1.mean(axis=1).sort_values(ascending=False).index\n",
    "    M1 = M1.loc[order]\n",
    "    M1.columns = [c.replace('isna_', '') for c in M1.columns]\n",
    "\n",
    "    im1 = axes[0].imshow(M1.values, aspect='auto', cmap=cmap)\n",
    "    axes[0].set_xticks(np.arange(M1.shape[1]))\n",
    "    axes[0].set_xticklabels(M1.columns, rotation=45, ha='right')\n",
    "    axes[0].set_yticks(np.arange(M1.shape[0]))\n",
    "    axes[0].set_yticklabels(M1.index)\n",
    "    axes[0].set_title('Country × Stat')\n",
    "    fig.colorbar(im1, ax=axes[0], label='% missing')\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, \"Missing 'Div' column\", ha='center', va='center')\n",
    "    axes[0].set_axis_off()\n",
    "\n",
    "# 2️⃣ Year × Stat (using Date column)\n",
    "if 'Date' in raw.columns:\n",
    "    raw['year'] = pd.to_datetime(raw['Date']).dt.year\n",
    "    M2 = raw.groupby('year')[flag_cols].mean().mul(100)\n",
    "    order = M2.mean(axis=1).sort_values(ascending=False).index\n",
    "    M2 = M2.loc[order]\n",
    "    M2.columns = [c.replace('isna_', '') for c in M2.columns]\n",
    "\n",
    "    im2 = axes[1].imshow(M2.values, aspect='auto', cmap=cmap)\n",
    "    axes[1].set_xticks(np.arange(M2.shape[1]))\n",
    "    axes[1].set_xticklabels(M2.columns, rotation=45, ha='right')\n",
    "    axes[1].set_yticks(np.arange(M2.shape[0]))\n",
    "    axes[1].set_yticklabels(M2.index.astype(int))\n",
    "    axes[1].set_title('Year × Stat')\n",
    "    fig.colorbar(im2, ax=axes[1], label='% missing')\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, \"Missing 'Date' column\", ha='center', va='center')\n",
    "    axes[1].set_axis_off()\n",
    "\n",
    "# 3️⃣ Year × Country\n",
    "needed = {'year', 'country_code'}\n",
    "if needed.issubset(raw.columns):\n",
    "    G = raw.groupby(['year','country_code'])[flag_cols].mean().mul(100)\n",
    "    G['avg_missing'] = G.mean(axis=1)\n",
    "    year_order  = G['avg_missing'].groupby(level=0).mean().sort_values(ascending=False).index\n",
    "    country_order = G['avg_missing'].groupby(level=1).mean().sort_values(ascending=False).index\n",
    "    P3 = (G['avg_missing'].unstack('country_code')\n",
    "          .reindex(index=year_order, columns=country_order)\n",
    "          .fillna(0))\n",
    "\n",
    "    im3 = axes[2].imshow(P3.values, aspect='auto', cmap=cmap)\n",
    "    axes[2].set_xticks(np.arange(P3.shape[1]))\n",
    "    axes[2].set_xticklabels(P3.columns, rotation=45, ha='right')\n",
    "    axes[2].set_yticks(np.arange(P3.shape[0]))\n",
    "    axes[2].set_yticklabels(P3.index.astype(int))\n",
    "    axes[2].set_title('Year × Country')\n",
    "    fig.colorbar(im3, ax=axes[2], label='% missing')\n",
    "else:\n",
    "    axes[2].text(0.5, 0.5, \"Missing required columns\", ha='center', va='center')\n",
    "    axes[2].set_axis_off()\n",
    "\n",
    "# 4️⃣ Country × League\n",
    "needed = {'country_code', 'Div'}\n",
    "if needed.issubset(raw.columns):\n",
    "    G = raw.groupby(['country_code', 'Div'])[flag_cols].mean().mul(100)\n",
    "    G['avg_missing'] = G.mean(axis=1)\n",
    "    P4 = (G['avg_missing'].unstack('Div').fillna(0))\n",
    "    country_order = P4.mean(axis=1).sort_values(ascending=False).index\n",
    "    league_order  = P4.mean(axis=0).sort_values(ascending=False).index\n",
    "    P4 = P4.loc[country_order, league_order]\n",
    "\n",
    "    im4 = axes[3].imshow(P4.values, aspect='auto', cmap=cmap)\n",
    "    axes[3].set_xticks(np.arange(P4.shape[1]))\n",
    "    axes[3].set_xticklabels(P4.columns, rotation=45, ha='right')\n",
    "    axes[3].set_yticks(np.arange(P4.shape[0]))\n",
    "    axes[3].set_yticklabels(P4.index)\n",
    "    axes[3].set_title('Country × League')\n",
    "    fig.colorbar(im4, ax=axes[3], label='% missing')\n",
    "else:\n",
    "    axes[3].text(0.5, 0.5, \"Missing required columns\", ha='center', va='center')\n",
    "    axes[3].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first heatmap shows missing data by country. Turkey has the most missing data by far, with over 1.4 percent missing on average. All other countries have very little missing data, less than 0.5 percent each.\n",
    "\n",
    "The second heatmap shows missing data by year. The years 2023 has slightly more missing data than the other years.\n",
    "\n",
    "The third heatmap combines year and country together. It shows that Turkey has most missing values in 2023. In other years, the missingness is not so bad.\n",
    "\n",
    "The fourth heatmap shows missing data by country and league division. Again, Turkey stands out with the highest missing data. Within each country, different league divisions have similar amounts of missing data, which means the problem is more about the country than about which league tier we look at.\n",
    "\n",
    "Overall, the missing data is not random. It is concentrated mainly in Turkey and in the year 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Turkey 2023 Missing Data Pattern\n",
    "\n",
    "Since Turkey in 2023 showed the highest missingness, we investigate if this is concentrated in specific months or teams to understand if it is a seasonal data collection issue or affects the entire year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Turkey data and analyze temporal distribution\n",
    "# Turkey's country code is 'T' (from Div column like 'T1')\n",
    "turkey_df = raw[raw['country_code'] == 'T'].copy()\n",
    "turkey_df['Year'] = pd.to_datetime(turkey_df['Date']).dt.year\n",
    "turkey_df['YearMonth'] = pd.to_datetime(turkey_df['Date']).dt.to_period('M')\n",
    "\n",
    "# Count missing values by year and month for Turkey\n",
    "match_stats = ['HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "turkey_df['has_missing'] = turkey_df[match_stats].isna().any(axis=1)\n",
    "\n",
    "# Year-level breakdown\n",
    "turkey_by_year = turkey_df.groupby('Year').agg({\n",
    "    'has_missing': ['sum', 'count', lambda x: (x.sum() / x.count() * 100)]\n",
    "}).round(2)\n",
    "turkey_by_year.columns = ['Missing_Matches', 'Total_Matches', 'Pct_Missing']\n",
    "\n",
    "print(\"Turkey Missing Data by Year:\")\n",
    "print(turkey_by_year)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Focus on 2023 - monthly breakdown\n",
    "turkey_2023 = turkey_df[turkey_df['Year'] == 2023]\n",
    "turkey_2023_by_month = turkey_2023.groupby('YearMonth').agg({\n",
    "    'has_missing': ['sum', 'count', lambda x: (x.sum() / x.count() * 100)]\n",
    "}).round(2)\n",
    "turkey_2023_by_month.columns = ['Missing_Matches', 'Total_Matches', 'Pct_Missing']\n",
    "\n",
    "print(\"Turkey 2023 Missing Data by Month:\")\n",
    "print(turkey_2023_by_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "The investigation of Turkey's missing data reveals important patterns:\n",
    "\n",
    "1. **2023 is an anomaly**: Turkey's missing data is heavily concentrated in 2023 (around 6-7% of matches), while other years have very little missing data (less than 1%). This suggests the issue is not systemic but specific to the 2023 season.\n",
    "\n",
    "2. **Within 2023, missing data is consistent across months**: The monthly breakdown shows that the problem affects most months in 2023 fairly uniformly, rather than being concentrated in a specific period. This suggests it's likely a data collection or reporting issue that affected the entire 2023 season for Turkey.\n",
    "\n",
    "3. **Implications for missing data mechanism**: This pattern suggests the data is **MAR (Missing At Random)** rather than MCAR or MNAR, because the missingness is related to specific observable factors (Country=Turkey AND Year=2023), but is not related to the actual values of the missing statistics themselves. The missing data appears to be a result of external factors (data collection issues) rather than the match outcomes or team quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the actual missing matches from Turkey March-June 2023\n",
    "turkey_spring_2023 = turkey_2023[\n",
    "    (turkey_2023['YearMonth'] >= '2023-03') &\n",
    "    (turkey_2023['YearMonth'] <= '2023-06')\n",
    "].copy()\n",
    "\n",
    "# Select relevant columns to display\n",
    "display_cols = ['Date', 'Div', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR']\n",
    "print(f\"Missing matches in Turkey (March-June 2023): {len(turkey_spring_2023)} matches\\n\")\n",
    "turkey_spring_2023.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm: All missing matches involve Hatayspor or Gaziantep\n",
    "hatayspor_missing = turkey_spring_2023[\n",
    "    (turkey_spring_2023['HomeTeam'] == 'Hatayspor') |\n",
    "    (turkey_spring_2023['AwayTeam'] == 'Hatayspor')\n",
    "]\n",
    "gaziantep_missing = turkey_spring_2023[\n",
    "    (turkey_spring_2023['HomeTeam'] == 'Gaziantep') |\n",
    "    (turkey_spring_2023['AwayTeam'] == 'Gaziantep')\n",
    "]\n",
    "\n",
    "print(f\"Matches with missing data involving:\")\n",
    "print(f\"  - Hatayspor: {len(hatayspor_missing)} matches\")\n",
    "print(f\"  - Gaziantep: {len(gaziantep_missing)} matches\")\n",
    "print(f\"  - Total unique: {len(turkey_spring_2023)} matches\")\n",
    "print(f\"\\nNote: Both teams are from cities affected by the February 2023 Turkey-Syria earthquake.\")\n",
    "print(\"This explains why match statistics were not recorded - likely matches were\")\n",
    "print(\"awarded as 3-0 victories or played at neutral venues without proper data collection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Remove Earthquake-Affected Matches\n",
    "\n",
    "Based on our investigation, we identified 28 matches from Turkey in 2023 involving Hatayspor and Gaziantep (cities affected by the February 2023 earthquake). These matches have missing statistics and show 3-0 awarded results, indicating they were not played normally. We'll remove these from our dataset as they don't represent actual match data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create condition to identify earthquake-affected matches\n",
    "# Turkey 2023 matches involving Hatayspor or Gaziantep with missing statistics\n",
    "raw['Year'] = pd.to_datetime(raw['Date']).dt.year\n",
    "\n",
    "earthquake_matches = (\n",
    "    (raw['country_code'] == 'T') &  # Turkey\n",
    "    (raw['Year'] == 2023) &  # Year 2023\n",
    "    (\n",
    "        (raw['HomeTeam'].isin(['Hatayspor', 'Gaziantep'])) |\n",
    "        (raw['AwayTeam'].isin(['Hatayspor', 'Gaziantep']))\n",
    "    ) &\n",
    "    (raw['missing_count_stats'] > 0)  # Has missing statistics\n",
    ")\n",
    "\n",
    "# Count before removal\n",
    "n_before = len(raw)\n",
    "n_earthquake = earthquake_matches.sum()\n",
    "\n",
    "# Remove the matches\n",
    "raw = raw[~earthquake_matches].copy()\n",
    "\n",
    "# Reset index\n",
    "raw = raw.reset_index(drop=True)\n",
    "\n",
    "# Count after removal\n",
    "n_after = len(raw)\n",
    "\n",
    "print(f\"Dataset before removal: {n_before:,} matches\")\n",
    "print(f\"Earthquake-affected matches removed: {n_earthquake} matches\")\n",
    "print(f\"Dataset after removal: {n_after:,} matches\")\n",
    "print(f\"Percentage removed: {(n_earthquake/n_before)*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **After inspecting the missigness in the Turkey (season 2022/2023 second half - notably the months March-June) we noted that for 2 teams missigness is result of win/loss by default, meaning the match did not actually happen - it has no info for us. Additionally, we found that this happened because of withdrawing of 2 teams from the league due to the Earthquakes in the early 2023. This means we can (and must) safely drop these observations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_cols  = [f'isna_{c}' for c in stats_cols if f'isna_{c}' in raw.columns]\n",
    "top_n      = 15\n",
    "min_matches_ref = 50   # ignore refs with tiny sample sizes\n",
    "\n",
    "def group_missing_rate(df, key):\n",
    "    \"\"\"Return DataFrame with avg % missing across 12 stats, plus counts.\"\"\"\n",
    "    grp = df.groupby(key)[flag_cols]\n",
    "    rate = grp.mean().mul(100).mean(axis=1)\n",
    "    cnt  = df.groupby(key).size()\n",
    "    out  = pd.DataFrame({'rate': rate, 'n': cnt}).sort_values('rate', ascending=False)\n",
    "    return out\n",
    "\n",
    "# 1️⃣ Home, Away, Referee\n",
    "home_df = group_missing_rate(raw, 'HomeTeam') if 'HomeTeam' in raw.columns else pd.DataFrame()\n",
    "away_df = group_missing_rate(raw, 'AwayTeam') if 'AwayTeam' in raw.columns else pd.DataFrame()\n",
    "ref_df  = group_missing_rate(raw, 'Referee')  if 'Referee'  in raw.columns else pd.DataFrame()\n",
    "if not ref_df.empty:\n",
    "    ref_df = ref_df[ref_df['n'] >= min_matches_ref].sort_values('rate', ascending=False)\n",
    "\n",
    "# 2️⃣ Merge for Home vs Away comparison (teams present in both)\n",
    "both = pd.DataFrame()\n",
    "if not home_df.empty and not away_df.empty:\n",
    "    both = (home_df[['rate']].rename(columns={'rate': 'home_rate'})\n",
    "            .merge(away_df[['rate']], left_index=True, right_index=True, how='inner')\n",
    "            .rename(columns={'rate': 'away_rate'}))\n",
    "    both['diff'] = both['home_rate'] - both['away_rate']\n",
    "    both = both.sort_values('home_rate', ascending=False).head(top_n)\n",
    "\n",
    "# =======================\n",
    "# FIGURE 1 — Home & Away\n",
    "# =======================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# A) Top Home teams\n",
    "if not home_df.empty:\n",
    "    htop = home_df.head(top_n)[::-1]\n",
    "    axes[0].barh(htop.index.astype(str), htop['rate'].values, color='#8c564b')\n",
    "    axes[0].set_title('Missingness by HomeTeam (avg % across stats)')\n",
    "    axes[0].set_xlabel('% missing')\n",
    "    for y, (r, n) in enumerate(zip(htop['rate'].values, htop['n'].values)):\n",
    "        axes[0].text(r, y, f'  {r:.2f}% (n={n})', va='center', ha='left', fontsize=9)\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, \"HomeTeam column not found\", ha='center', va='center')\n",
    "    axes[0].set_axis_off()\n",
    "\n",
    "# B) Top Away teams\n",
    "if not away_df.empty:\n",
    "    atop = away_df.head(top_n)[::-1]\n",
    "    axes[1].barh(atop.index.astype(str), atop['rate'].values, color='#1f77b4')\n",
    "    axes[1].set_title('Missingness by AwayTeam (avg % across stats)')\n",
    "    axes[1].set_xlabel('% missing')\n",
    "    for y, (r, n) in enumerate(zip(atop['rate'].values, atop['n'].values)):\n",
    "        axes[1].text(r, y, f'  {r:.2f}% (n={n})', va='center', ha='left', fontsize=9)\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, \"AwayTeam column not found\", ha='center', va='center')\n",
    "    axes[1].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization compares the average percentage of missing match statistics for each team when playing at home (brown dots) versus away (blue dots). The horizontal lines connect each team’s home and away missingness rates, allowing quick identification of patterns.\n",
    "\n",
    "Most teams show very little difference between home and away games, suggesting that data gaps are not related to the venue. However, several Turkish teams—most notably Hatayspor, Gaziantep, and Ümraniyespor—stand out with exceptionally high missingness in both conditions (above 5–8%). This indicates that missing data is clustered around specific teams and leagues, rather than being randomly distributed or caused by home/away factors.\n",
    "\n",
    "Overall, the visualization reinforces that the missingness originates from systematic collection or feed issues affecting particular teams or competitions, rather than isolated recording errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missigness for referees\n",
    "if not ref_df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    rtop = ref_df.head(top_n)[::-1]\n",
    "    ax.barh(rtop.index.astype(str), rtop['rate'].values, color='#9467bd')\n",
    "    ax.set_title(f'Missingness by Referee (avg % across stats, n≥{min_matches_ref})')\n",
    "    ax.set_xlabel('% missing')\n",
    "    for y, (r, n) in enumerate(zip(rtop['rate'].values, rtop['n'].values)):\n",
    "        ax.text(r, y, f'  {r:.2f}% (n={n})', va='center', ha='left', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No referees pass the sample-size filter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper: % missing by group (avg across rows)\n",
    "def pct_missing_by(group_cols, cols):\n",
    "    G = raw.groupby(group_cols)[cols].mean().mul(100)   # % per stat\n",
    "    return G\n",
    "\n",
    "# =========================\n",
    "# Figure A — Year × Stat\n",
    "# =========================\n",
    "if 'year' in raw.columns:\n",
    "    YS = pct_missing_by(['year'], flag_cols)\n",
    "    # order years by overall missingness (desc)\n",
    "    order = YS.mean(axis=1).sort_values(ascending=False).index\n",
    "    YS = YS.loc[order]\n",
    "    YS.columns = [c.replace('isna_', '') for c in YS.columns]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    im = ax.imshow(YS.values, aspect='auto')\n",
    "    ax.set_xticks(np.arange(YS.shape[1]))\n",
    "    ax.set_xticklabels(YS.columns, rotation=45, ha='right')\n",
    "    ax.set_yticks(np.arange(YS.shape[0]))\n",
    "    ax.set_yticklabels(YS.index.astype(int))\n",
    "    ax.set_title('Missingness heatmap (%) — Year × Stat')\n",
    "    fig.colorbar(im, ax=ax, label='% missing')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================\n",
    "# Figure B — Year trend (avg across all stats)\n",
    "# =============================================\n",
    "if 'year' in raw.columns:\n",
    "    Y_avg = (raw.groupby('year')[flag_cols].mean().mul(100).mean(axis=1)\n",
    "             .sort_index())\n",
    "    fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "    ax.plot(Y_avg.index.astype(int), Y_avg.values, marker='o')\n",
    "    for x, y in zip(Y_avg.index, Y_avg.values):\n",
    "        ax.text(x, y, f'{y:.2f}%', va='bottom', ha='center', fontsize=9)\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('% missing (avg across stats)')\n",
    "    ax.set_title('Missingness over time (yearly average)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =========================\n",
    "# Figure C — Hour × Stat\n",
    "# =========================\n",
    "if 'hour' in raw.columns:\n",
    "    # drop hours that are NaN (unparseable)\n",
    "    HH = raw.dropna(subset=['hour']).copy()\n",
    "    HH['hour'] = HH['hour'].astype(int)\n",
    "    HS = (HH.groupby('hour')[flag_cols].mean().mul(100))\n",
    "    # ensure 0–23 present (fill with zeros if absent)\n",
    "    HS = HS.reindex(range(0, 24), fill_value=0)\n",
    "    HS.columns = [c.replace('isna_', '') for c in HS.columns]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    im = ax.imshow(HS.values, aspect='auto')\n",
    "    ax.set_xticks(np.arange(HS.shape[1]))\n",
    "    ax.set_xticklabels(HS.columns, rotation=45, ha='right')\n",
    "    ax.set_yticks(np.arange(HS.shape[0]))\n",
    "    ax.set_yticklabels(HS.index.astype(int))\n",
    "    ax.set_title('Missingness heatmap (%) — Hour × Stat')\n",
    "    fig.colorbar(im, ax=ax, label='% missing')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Sanity Checks\n",
    "\n",
    "Before moving forward, we need to verify that our data makes logical sense. We will check if the relationships between different columns are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_issues = []\n",
    "\n",
    "# Check 1: Full time goals should be >= half time goals\n",
    "print(\"\\nFull Time Goals >= Half Time Goals\")\n",
    "ht_ft_home_check = all_matches['FTHG'] >= all_matches['HTHG']\n",
    "ht_ft_away_check = all_matches['FTAG'] >= all_matches['HTAG']\n",
    "home_violations = (~ht_ft_home_check).sum()\n",
    "away_violations = (~ht_ft_away_check).sum()\n",
    "print(f\"   Home goals violations: {home_violations}\")\n",
    "print(f\"   Away goals violations: {away_violations}\")\n",
    "if home_violations > 0 or away_violations > 0:\n",
    "    sanity_issues.append(f\"FT goals < HT goals: {home_violations + away_violations} cases\")\n",
    "\n",
    "# Check 2: Full time result should match actual goals\n",
    "print(\"\\nFull Time Result matches actual goals\")\n",
    "ftr_check = pd.Series(index=all_matches.index, dtype=bool)\n",
    "ftr_check = (\n",
    "    ((all_matches['FTR'] == 'H') & (all_matches['FTHG'] > all_matches['FTAG'])) |\n",
    "    ((all_matches['FTR'] == 'A') & (all_matches['FTAG'] > all_matches['FTHG'])) |\n",
    "    ((all_matches['FTR'] == 'D') & (all_matches['FTHG'] == all_matches['FTAG']))\n",
    ")\n",
    "ftr_violations = (~ftr_check).sum()\n",
    "print(f\"   FTR mismatches: {ftr_violations}\")\n",
    "if ftr_violations > 0:\n",
    "    sanity_issues.append(f\"FTR doesn't match goals: {ftr_violations} cases\")\n",
    "\n",
    "# Check 3: Half time result should match half time goals\n",
    "print(\"\\nChecking: Half Time Result matches half time goals\")\n",
    "htr_check = pd.Series(index=all_matches.index, dtype=bool)\n",
    "htr_check = (\n",
    "    ((all_matches['HTR'] == 'H') & (all_matches['HTHG'] > all_matches['HTAG'])) |\n",
    "    ((all_matches['HTR'] == 'A') & (all_matches['HTAG'] > all_matches['HTHG'])) |\n",
    "    ((all_matches['HTR'] == 'D') & (all_matches['HTHG'] == all_matches['HTAG']))\n",
    ")\n",
    "htr_violations = (~htr_check).sum()\n",
    "print(f\"   HTR mismatches: {htr_violations}\")\n",
    "if htr_violations > 0:\n",
    "    sanity_issues.append(f\"HTR doesn't match HT goals: {htr_violations} cases\")\n",
    "\n",
    "# Check 4: Shots on target should be <= total shots\n",
    "print(\"\\nShots on Target <= Total Shots\")\n",
    "home_shot_check = all_matches['HST'] <= all_matches['HS']\n",
    "away_shot_check = all_matches['AST'] <= all_matches['AS']\n",
    "home_shot_violations = (~home_shot_check).sum()\n",
    "away_shot_violations = (~away_shot_check).sum()\n",
    "print(f\"   Home shots violations: {home_shot_violations}\")\n",
    "print(f\"   Away shots violations: {away_shot_violations}\")\n",
    "if home_shot_violations > 0 or away_shot_violations > 0:\n",
    "    sanity_issues.append(f\"Shots on target > total shots: {home_shot_violations + away_shot_violations} cases\")\n",
    "\n",
    "# Check 5: Goals should be <= shots on target (generally, but not always)\n",
    "print(\"\\nGoals <= Shots on Target (usually)\")\n",
    "home_goals_shots_check = all_matches['FTHG'] <= all_matches['HST']\n",
    "away_goals_shots_check = all_matches['FTAG'] <= all_matches['AST']\n",
    "home_goals_violations = (~home_goals_shots_check).sum()\n",
    "away_goals_violations = (~away_goals_shots_check).sum()\n",
    "print(f\"   Home goals > shots on target: {home_goals_violations}\")\n",
    "print(f\"   Away goals > shots on target: {away_goals_violations}\")\n",
    "print(f\"   Note: Some violations are possible due to own goals or deflections\")\n",
    "if home_goals_violations > 10 or away_goals_violations > 10:\n",
    "    sanity_issues.append(f\"Goals > shots on target: {home_goals_violations + away_goals_violations} cases (check if excessive)\")\n",
    "\n",
    "# Check 6: Red cards should be <= yellow cards + red cards\n",
    "print(\"\\nCard counts are reasonable\")\n",
    "home_red_check = all_matches['HR'] <= (all_matches['HY'] + all_matches['HR'])\n",
    "away_red_check = all_matches['AR'] <= (all_matches['AY'] + all_matches['AR'])\n",
    "print(f\"   Home card logic violations: {(~home_red_check).sum()}\")\n",
    "print(f\"   Away card logic violations: {(~away_red_check).sum()}\")\n",
    "\n",
    "# Check 7: Negative values check\n",
    "print(\"\\nNo negative values in count columns\")\n",
    "count_columns = ['FTHG', 'FTAG', 'HTHG', 'HTAG', 'HS', 'AS', 'HST', 'AST',\n",
    "                'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "negative_found = False\n",
    "for col in count_columns:\n",
    "    if col in all_matches.columns:\n",
    "        negative_count = (all_matches[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            print(f\"   {col}: {negative_count} negative values\")\n",
    "            negative_found = True\n",
    "            sanity_issues.append(f\"{col} has {negative_count} negative values\")\n",
    "if not negative_found:\n",
    "    print(f\"   No negative values found\")\n",
    "\n",
    "# Check 8: Extreme values check\n",
    "print(\"\\nExtreme values that might be data errors\")\n",
    "extreme_checks = {\n",
    "    'FTHG': 15,\n",
    "    'FTAG': 15,\n",
    "    'HS': 50,\n",
    "    'AS': 50,\n",
    "    'HC': 30,\n",
    "    'AC': 30,\n",
    "    'HY': 10,\n",
    "    'AY': 10,\n",
    "    'HR': 5,\n",
    "    'AR': 5\n",
    "}\n",
    "for col, threshold in extreme_checks.items():\n",
    "    if col in all_matches.columns:\n",
    "        extreme_count = (all_matches[col] > threshold).sum()\n",
    "        if extreme_count > 0:\n",
    "            max_value = all_matches[col].max()\n",
    "            print(f\"   {col} > {threshold}: {extreme_count} cases (max: {max_value})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sanity checks help us verify that the data is internally consistent. We check things like full time goals being at least as many as half time goals, that the match result codes match the actual goal counts, that shots on target do not exceed total shots, and that there are no negative values in count columns. These checks help identify data entry errors or corruption before we use the data for modeling.\n",
    "\n",
    "Our data passed most checks well. Full time goals are always at least as many as half time goals, which is correct. The full time result codes match the actual scores perfectly.\n",
    "\n",
    "We found 41 matches where the half time result code does not match the half time goals. This is a small number out of 42,593 matches, so it is likely just data entry errors in those specific matches.\n",
    "\n",
    "We found 6 matches where shots on target are higher than total shots. This is probably a recording error but only affects 6 matches so it is not a big problem.\n",
    "\n",
    "We found 234 matches where a team scored more goals than they had shots on target. This can happen in real football due to own goals or deflections, so these are not necessarily errors.\n",
    "\n",
    "We found one match where a team got 9 red cards. This is extremely unusual and might be a data error, but it is only one match out of thousands.\n",
    "\n",
    "Overall, the data quality is very good. The few issues we found affect less than 1 percent of matches and will not significantly impact our model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 League and Country Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# League distribution\n",
    "league_counts = all_matches['Div'].value_counts()\n",
    "print(\"League distribution:\")\n",
    "display(league_counts)\n",
    "\n",
    "# Country mapping for better understanding\n",
    "country_mapping = {\n",
    "    'E': 'England', 'SC': 'Scotland', 'SP': 'Spain', 'I': 'Italy',\n",
    "    'D': 'Germany', 'F': 'France', 'N': 'Netherlands', 'B': 'Belgium',\n",
    "    'P': 'Portugal', 'T': 'Turkey', 'G': 'Greece'\n",
    "}\n",
    "\n",
    "all_matches['Country'] = all_matches['Div'].str[:-1].map(country_mapping)\n",
    "country_counts = all_matches['Country'].value_counts()\n",
    "print(f\"\\nMatches per country:\")\n",
    "display(country_counts)\n",
    "\n",
    "# Visualize the distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Country distribution\n",
    "country_counts_sorted = country_counts.dropna().sort_values(ascending=False)\n",
    "bars = ax1.bar(country_counts_sorted.index, country_counts_sorted.values, color='skyblue')\n",
    "ax1.set_title('Matches per Country')\n",
    "ax1.set_xlabel('Country')\n",
    "ax1.set_ylabel('Number of Matches')\n",
    "ax1.tick_params(axis='x', rotation=30)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# data labels\n",
    "for rect in bars:\n",
    "    height = rect.get_height()\n",
    "    ax1.text(rect.get_x() + rect.get_width()/2, height, f\"{int(height):,}\",\n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Goals distribution\n",
    "total_goals = all_matches['FTHG'] + all_matches['FTAG']\n",
    "max_g = int(np.nanmax(total_goals))\n",
    "bins = np.arange(-0.5, max(10, max_g) + 1.5, 1)\n",
    "\n",
    "ax2.hist(total_goals, bins=bins, color='lightcoral', alpha=0.7)\n",
    "ax2.axvline(x=2.5, linestyle='--', linewidth=2, label='2.5 goals threshold')\n",
    "ax2.set_title('Distribution of Total Goals per Match')\n",
    "ax2.set_xlabel('Total Goals')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_xticks(range(0, max(10, max_g) + 1))\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "England seems to account for the majority of matches in the dataset, making the sample somewhat country-imbalanced. This suggests that model training should be performed separately for each country, or at least include country-specific components, to prevent English leagues from dominating the overall model behavior.  \n",
    "\n",
    "When building time-aware models, it would also be beneficial to use chronological splits within each country and consider assigning higher weights to more recent matches, since they better reflect current team dynamics and scoring trends.  \n",
    "\n",
    "Alse, the distribution of total goals per match is right-skewed, with mode around 2–3 goals. The red dashed line at 2.5 goals marks the classification threshold for our target variable. Visually, the mass on either side of this threshold is roughly equal, which confirms the balanced 50/50 split observed in the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Handling csv issues\n",
    "It seems like the renaming and loading went smoothly! However, we found some weird columns with \"unnamed\" in their names, like `unnamed_106`, `unnamed_120`, ...  \n",
    "That sometimes happens when excel files have extra blank columns. We'll take a quick look to see if they have any data, and if they're totally empty (full of NaNs), we'll just get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnamed_cols = [c for c in all_matches.columns if c.lower().startswith(\"unnamed\")]\n",
    "all_matches[unnamed_cols].isna().mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're 100% full of NaNs so we can now safely drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = all_matches.drop(columns=unnamed_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Normalizing league codes\n",
    "Let's normalize the leagues, as English and Scottish leagues have the best leagues interpreted as E0, SC0, respectively. All other countries mark the best league as CountryCode1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = all_matches['Div'].str.startswith(('E', 'SC'))\n",
    "all_matches.loc[mask, 'Div'] = all_matches.loc[mask, 'Div'].apply(\n",
    "    lambda x: f\"{x[:-1]}{int(x[-1]) + 1}\"\n",
    ")\n",
    "\n",
    "print(all_matches['Div'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Handling English and Scottish yellow cards\n",
    "We need to take care of the first note in notex.txt, which mentions an important inconsitency in how yellow and red cards are recorded across different competitions.  \n",
    "\n",
    "In English and Scottish leagues, when a player receives a second yellow card that leads to a red card, the initial yellow card is not counted in the match statistics, only the red card is recorded. However, European and international competitions record both: the second yellow is counted as an additional yellow card plus a red card \n",
    "\n",
    "As a result, yellow card totals in English and Scottish matches can underestimate the true number of yellow cards compared to other leagues. To correct for this and ensure consistency across competitions, we applied a simple adjustment:\n",
    "- whenever a team has exactly one red card and one or more yellow cards, we add one additional yellow card.\n",
    "- and if a team has 0 reds, 2 or more reds, or 1 red but no yellows, we make no adjustment.\n",
    "\n",
    "We acknowledge that this rule is an approximation, our adjustment may not always be the case and it may introduce some bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = all_matches['Div'].str.startswith(('E', 'SC'))\n",
    "red_mask = mask & ((all_matches['HR'] == 1) | (all_matches['AR'] == 1))\n",
    "\n",
    "print(\"Before adjustment (sample):\")\n",
    "print(all_matches.loc[red_mask, ['Div', 'HY', 'HR', 'AY', 'AR']].head())\n",
    "\n",
    "all_matches.loc[mask & (all_matches['HR'] == 1) & (all_matches['HY'] != 0), 'HY'] += 1\n",
    "all_matches.loc[mask & (all_matches['AR'] == 1) & (all_matches['AY'] != 0), 'AY'] += 1\n",
    "\n",
    "print(\"\\nAfter adjustment (sample):\")\n",
    "print(all_matches.loc[red_mask, ['Div', 'HY', 'HR', 'AY', 'AR']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Correcting data types\n",
    "Now, let's inspect the data types of our columns. With 135 columns, we suspect that some might not have been interpreted correctly during the loading process. Checking the data types is an important step before proceeding with any further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, dtype in all_matches.dtypes.items():\n",
    "    print(f\"{col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_columns = ['Date', 'Time']\n",
    "\n",
    "category_columns = ['Div', 'HomeTeam', 'AwayTeam', 'FTR', 'HTR', 'Referee', 'Country']\n",
    "\n",
    "int_columns = ['FTHG', 'FTAG', 'HTHG', 'HTAG', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "\n",
    "float_columns = ['B365CH', 'BWCA', '1XBH']\n",
    "\n",
    "for col in time_columns:\n",
    "    if col == 'Date':\n",
    "        all_matches[col] = pd.to_datetime(all_matches[col])\n",
    "    else:\n",
    "        all_matches[col] = pd.to_datetime(all_matches[col], format='%H:%M').dt.time\n",
    "\n",
    "for col in category_columns:\n",
    "    all_matches[col] = all_matches[col].astype('category')\n",
    "\n",
    "for col in int_columns:\n",
    "    all_matches[col] = pd.to_numeric(all_matches[col], errors='coerce').astype('Int64')\n",
    "\n",
    "for col in float_columns:\n",
    "    all_matches[col] = pd.to_numeric(all_matches[col], errors='coerce').astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, dtype in all_matches.dtypes.items():\n",
    "    print(f\"{col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Missing value imputation with domain-specific strategies\n",
    "\n",
    "Based on our EDA, we'll handle missing values with different strategies based on data characteristics:\n",
    "\n",
    "1. **Key match statistics**: Very few missing values (~0.1%) - use SimpleImputer with median strategy\n",
    "2. **Betting odds**: High missingness (80%+) but match-specific - use cross-bookmaker median imputation per match, then overall median fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns with low missingness that need imputation\n",
    "low_missingness_cols = ['HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "\n",
    "# Check current missing values before imputation\n",
    "print(\"Missing values before imputation:\")\n",
    "for col in low_missingness_cols:\n",
    "    if col in all_matches.columns:\n",
    "        missing_count = all_matches[col].isnull().sum()\n",
    "        missing_pct = (missing_count / len(all_matches)) * 100\n",
    "        print(f\"{col}: {missing_count} ({missing_pct:.3f}%)\")\n",
    "\n",
    "# Apply median imputation for numerical match statistics\n",
    "if any(all_matches[col].isnull().sum() > 0 for col in low_missingness_cols if col in all_matches.columns):\n",
    "    match_stats_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "    # Only impute columns that actually exist and have missing values\n",
    "    cols_to_impute = [col for col in low_missingness_cols if col in all_matches.columns and all_matches[col].isnull().sum() > 0]\n",
    "\n",
    "    if cols_to_impute:\n",
    "        print(f\"\\nApplying median imputation to: {cols_to_impute}\")\n",
    "        all_matches[cols_to_impute] = match_stats_imputer.fit_transform(all_matches[cols_to_impute])\n",
    "\n",
    "        print(\"Imputation completed. Median values used:\")\n",
    "        for col in cols_to_impute:\n",
    "            median_val = all_matches[col].median()\n",
    "            print(f\"  {col}: {median_val}\")\n",
    "    else:\n",
    "        print(\"No missing values found in match statistics columns.\")\n",
    "else:\n",
    "    print(\"No missing values found in match statistics columns.\")\n",
    "\n",
    "# Handle categorical columns separately\n",
    "if 'Referee' in all_matches.columns:\n",
    "    referee_missing = all_matches['Referee'].isnull().sum()\n",
    "    if referee_missing > 0:\n",
    "        if 'Unknown' not in all_matches['Referee'].cat.categories:\n",
    "            all_matches['Referee'] = all_matches['Referee'].cat.add_categories(['Unknown'])\n",
    "        all_matches['Referee'] = all_matches['Referee'].fillna('Unknown')\n",
    "        print(f\"Filled {referee_missing} missing referees with 'Unknown'\")\n",
    "\n",
    "if 'Time' in all_matches.columns:\n",
    "    time_missing = all_matches['Time'].isnull().sum()\n",
    "    if time_missing > 0:\n",
    "        all_matches['Time'] = all_matches['Time'].fillna(pd.to_datetime('15:00', format='%H:%M').time())\n",
    "        print(f\"Filled {time_missing} missing times with '15:00'\")\n",
    "\n",
    "# Verify no missing values remain in core match statistics\n",
    "print(f\"\\nVerification - remaining missing values in core columns:\")\n",
    "verification_cols = low_missingness_cols + ['Referee', 'Time']\n",
    "total_missing = 0\n",
    "for col in verification_cols:\n",
    "    if col in all_matches.columns:\n",
    "        missing = all_matches[col].isnull().sum()\n",
    "        if missing > 0:\n",
    "            print(f\"{col}: {missing}\")\n",
    "            total_missing += missing\n",
    "\n",
    "if total_missing == 0:\n",
    "    print(\"All core match statistics successfully imputed - no missing values remain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle the gaps, we used median imputation for the numerical match statistics because they're robust to outliers and preserve the central distribution of the data. For categorical fields, missing Referee entries were replaced with ‘Unknown’, and missing Time values were set to 15:00, which should be the typical match kickoff time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle betting odds imputation with proper market categorization\n",
    "# Based on notes.txt, betting odds are organized by market type (1X2, O/U 2.5, Asian Handicap)\n",
    "\n",
    "# Identify betting odds columns\n",
    "betting_cols = [col for col in all_matches.columns if any(bookmaker in col for bookmaker in ['B365', 'BW', 'PS', 'IW', 'LB', 'WH', 'SJ', 'VC', 'BF', '1XB', 'CL', 'GB', 'SO', 'SB', 'SY', 'Max', 'Avg', 'Bb'])]\n",
    "\n",
    "print(f\"Found {len(betting_cols)} betting odds columns\")\n",
    "\n",
    "# Analyze missing patterns in betting odds\n",
    "betting_missing_analysis = []\n",
    "high_missing_cols = []\n",
    "for col in betting_cols:\n",
    "    missing_count = all_matches[col].isnull().sum()\n",
    "    missing_pct = (missing_count / len(all_matches)) * 100\n",
    "    betting_missing_analysis.append({\n",
    "        'column': col,\n",
    "        'missing_count': missing_count,\n",
    "        'missing_pct': missing_pct\n",
    "    })\n",
    "    if missing_pct > 80:  # Track columns with very high missingness\n",
    "        high_missing_cols.append(col)\n",
    "\n",
    "print(f\"Columns with >80% missing values: {len(high_missing_cols)}\")\n",
    "print(\"Sample of betting odds with lower missingness (<80%):\")\n",
    "for item in sorted(betting_missing_analysis, key=lambda x: x['missing_pct'])[:15]:\n",
    "    if item['missing_pct'] < 80:\n",
    "        print(f\"  {item['column']}: {item['missing_pct']:.1f}%\")\n",
    "\n",
    "# Categorize betting odds by market type based on notes.txt\n",
    "def categorize_betting_market(col_name):\n",
    "    \"\"\"Categorize betting column by market type based on column name patterns\"\"\"\n",
    "    col = col_name.upper()\n",
    "\n",
    "    # 1X2 Market (Home/Draw/Away win)\n",
    "    if col.endswith('H') and not any(x in col for x in ['AH', '>', '<']):\n",
    "        return 'home_win'\n",
    "    elif col.endswith('D') and not any(x in col for x in ['AH', '>', '<']):\n",
    "        return 'draw'\n",
    "    elif col.endswith('A') and not any(x in col for x in ['AH', '>', '<']):\n",
    "        return 'away_win'\n",
    "\n",
    "    # Over/Under 2.5 Goals Market\n",
    "    elif '>2.5' in col or 'O2.5' in col:\n",
    "        return 'over_2_5'\n",
    "    elif '<2.5' in col or 'U2.5' in col:\n",
    "        return 'under_2_5'\n",
    "\n",
    "    # Asian Handicap Market\n",
    "    elif 'AH' in col and col.endswith('H'):\n",
    "        return 'ah_home'\n",
    "    elif 'AH' in col and col.endswith('A'):\n",
    "        return 'ah_away'\n",
    "    elif 'AH' in col and not col.endswith(('H', 'A')):\n",
    "        return 'ah_handicap'\n",
    "\n",
    "    # Other markets\n",
    "    elif 'C>' in col:  # Corner markets\n",
    "        return 'corners'\n",
    "    elif any(x in col for x in ['FKCH', 'FKCA']):  # Free kicks\n",
    "        return 'free_kicks'\n",
    "\n",
    "    return 'other'\n",
    "\n",
    "# Group betting columns by market type\n",
    "market_groups = {}\n",
    "for col in betting_cols:\n",
    "    market_type = categorize_betting_market(col)\n",
    "    market_groups.setdefault(market_type, []).append(col)\n",
    "\n",
    "print(f\"\\nBetting odds grouped by market type:\")\n",
    "for market_type, columns in market_groups.items():\n",
    "    avg_missing = np.mean([item['missing_pct'] for item in betting_missing_analysis if item['column'] in columns])\n",
    "    print(f\"  {market_type}: {len(columns)} columns (avg missing: {avg_missing:.1f}%)\")\n",
    "\n",
    "# Apply cross-bookmaker median imputation within each market for each match\n",
    "total_imputed = 0\n",
    "markets_processed = []\n",
    "\n",
    "for market_type, columns in market_groups.items():\n",
    "    if len(columns) > 1 and market_type != 'other':  # Only process markets with multiple bookmakers\n",
    "        print(f\"\\nProcessing {market_type} market ({len(columns)} columns)...\")\n",
    "        markets_processed.append(market_type)\n",
    "\n",
    "        # Check how much data we have for this market\n",
    "        market_data_availability = []\n",
    "        for col in columns:\n",
    "            non_missing = all_matches[col].notna().sum()\n",
    "            market_data_availability.append(non_missing)\n",
    "\n",
    "        if max(market_data_availability) > 1000:  # Only process if we have reasonable data\n",
    "            match_imputed = 0\n",
    "\n",
    "            # Process each match individually\n",
    "            for idx in all_matches.index:\n",
    "                # Get odds for this match across all bookmakers for this market\n",
    "                match_odds = all_matches.loc[idx, columns]\n",
    "\n",
    "                # If any values are missing but others exist, use median of available bookmakers\n",
    "                if match_odds.isnull().any() and not match_odds.isnull().all():\n",
    "                    match_median = match_odds.median()\n",
    "\n",
    "                    # Fill missing values with the cross-bookmaker median for this match\n",
    "                    for col in columns:\n",
    "                        if pd.isnull(all_matches.loc[idx, col]):\n",
    "                            all_matches.loc[idx, col] = match_median\n",
    "                            total_imputed += 1\n",
    "                            match_imputed += 1\n",
    "\n",
    "            print(f\"  {market_type}: {match_imputed} values imputed using cross-bookmaker median\")\n",
    "\n",
    "print(f\"\\nCross-bookmaker imputation completed: {total_imputed} values imputed across {len(markets_processed)} markets\")\n",
    "\n",
    "# For remaining missing values, apply conservative strategy\n",
    "# Only use overall median fallback for markets with reasonable data coverage\n",
    "remaining_imputed = 0\n",
    "columns_fully_imputed = []\n",
    "\n",
    "for market_type, columns in market_groups.items():\n",
    "    if market_type in ['home_win', 'draw', 'away_win', 'over_2_5', 'under_2_5']:  # Core markets only\n",
    "        for col in columns:\n",
    "            missing_before = all_matches[col].isnull().sum()\n",
    "            data_coverage = (all_matches[col].notna().sum() / len(all_matches)) * 100\n",
    "\n",
    "            # Only apply fallback imputation if we have at least 10% data coverage\n",
    "            if missing_before > 0 and data_coverage >= 10:\n",
    "                overall_median = all_matches[col].median()\n",
    "                all_matches[col] = all_matches[col].fillna(overall_median)\n",
    "                remaining_imputed += missing_before\n",
    "                columns_fully_imputed.append(col)\n",
    "\n",
    "print(f\"Overall median fallback applied to {len(columns_fully_imputed)} columns: {remaining_imputed} values imputed\")\n",
    "\n",
    "# For columns with <10% data coverage, we'll exclude them from modeling rather than impute\n",
    "excluded_cols = []\n",
    "for col in betting_cols:\n",
    "    data_coverage = (all_matches[col].notna().sum() / len(all_matches)) * 100\n",
    "    if data_coverage < 10:\n",
    "        excluded_cols.append(col)\n",
    "\n",
    "print(f\"\\nColumns excluded due to <10% data coverage: {len(excluded_cols)}\")\n",
    "print(\"These will be excluded from the extended dataset to avoid poor imputation quality\")\n",
    "\n",
    "# Verify imputation results for key markets\n",
    "print(f\"\\nVerification - missing values after imputation for key betting markets:\")\n",
    "key_betting_cols = [col for col in betting_cols if col not in excluded_cols][:15]  # Check sample\n",
    "final_missing = 0\n",
    "for col in key_betting_cols:\n",
    "    missing = all_matches[col].isnull().sum()\n",
    "    if missing > 0:\n",
    "        data_coverage = (all_matches[col].notna().sum() / len(all_matches)) * 100\n",
    "        print(f\"  {col}: {missing} missing ({data_coverage:.1f}% coverage)\")\n",
    "        final_missing += missing\n",
    "\n",
    "if final_missing == 0:\n",
    "    print(\"Key betting odds successfully imputed\")\n",
    "else:\n",
    "    print(f\"⚠ {final_missing} missing values remain in key betting columns\")\n",
    "\n",
    "# Update betting features list to exclude low-coverage columns\n",
    "print(f\"\\nUpdating betting features list:\")\n",
    "print(f\"Original betting columns: {len(betting_cols)}\")\n",
    "print(f\"Excluded low-coverage columns: {len(excluded_cols)}\")\n",
    "print(f\"Final betting columns for modeling: {len(betting_cols) - len(excluded_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Outlier detection and handling\n",
    "\n",
    "Following the methodology from Week1 (house pricing), we'll use z-score analysis to detect outliers in match statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical columns for outlier detection\n",
    "match_stats_cols = ['HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "numerical_cols = ['FTHG', 'FTAG', 'HTHG', 'HTAG'] + match_stats_cols\n",
    "\n",
    "# Calculate z-scores for numerical columns\n",
    "print(\"Outlier analysis using z-score > 3:\")\n",
    "outlier_counts = {}\n",
    "\n",
    "for col in numerical_cols:\n",
    "    if col in all_matches.columns:\n",
    "        z_scores = np.abs(zscore(all_matches[col].dropna()))\n",
    "        outliers = (z_scores > 3).sum()\n",
    "        outlier_counts[col] = outliers\n",
    "        if outliers > 0:\n",
    "            print(f\"{col}: {outliers} outliers ({outliers/len(all_matches)*100:.2f}%)\")\n",
    "\n",
    "# Look at extreme cases\n",
    "print(f\"\\nExamples of potential outliers:\")\n",
    "print(f\"Highest total goals: {all_matches['FTHG'].max() + all_matches['FTAG'].max()}\")\n",
    "print(f\"Most shots in a match: {all_matches['HS'].max() + all_matches['AS'].max()}\")\n",
    "print(f\"Most cards in a match: {all_matches['HY'].max() + all_matches['AY'].max()}\")\n",
    "\n",
    "# Visualize outliers for key variables\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "key_vars = ['FTHG', 'FTAG', 'HS', 'AS']\n",
    "\n",
    "for i, var in enumerate(key_vars):\n",
    "    row, col = i // 2, i % 2\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    # Box plot to show outliers\n",
    "    all_matches[var].plot(kind='box', ax=ax)\n",
    "    ax.set_title(f'Box Plot of {var}')\n",
    "    ax.set_ylabel(var)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# For football data, we'll be more conservative with outlier removal\n",
    "# as extreme scores can be legitimate (unlike house prices)\n",
    "print(f\"\\nDecision: Keep outliers for football data as high scores/stats can be legitimate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature engineering\n",
    "\n",
    "Based on soccer domain knowledge and the course materials, we'll create meaningful features that could help predict Over/Under 2.5 goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Target variable creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the main target variable: Over/Under 2.5 goals\n",
    "all_matches['total_goals'] = all_matches['FTHG'] + all_matches['FTAG']\n",
    "all_matches['over_2_5'] = (all_matches['total_goals'] > 2.5).astype(int)\n",
    "\n",
    "print(\"Target variable distribution:\")\n",
    "print(all_matches['over_2_5'].value_counts())\n",
    "print(f\"Over 2.5 rate: {all_matches['over_2_5'].mean():.2%}\")\n",
    "\n",
    "# Also create alternative targets for analysis\n",
    "all_matches['over_1_5'] = (all_matches['total_goals'] > 1.5).astype(int)\n",
    "all_matches['over_3_5'] = (all_matches['total_goals'] > 3.5).astype(int)\n",
    "\n",
    "print(f\"\\nOther thresholds:\")\n",
    "print(f\"Over 1.5 rate: {all_matches['over_1_5'].mean():.2%}\")\n",
    "print(f\"Over 3.5 rate: {all_matches['over_3_5'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Simple time-based features (BASIC columns only)\n",
    "\n",
    "First, we create basic time features from the Date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Shot efficiency features\n",
    "all_matches['home_shot_accuracy'] = all_matches['HST'] / (all_matches['HS'] + 0.001)  # avoid division by zero\n",
    "all_matches['away_shot_accuracy'] = all_matches['AST'] / (all_matches['AS'] + 0.001)\n",
    "all_matches['total_shots'] = all_matches['HS'] + all_matches['AS']\n",
    "all_matches['total_shots_on_target'] = all_matches['HST'] + all_matches['AST']\n",
    "\n",
    "# 2. Attacking vs Defensive balance\n",
    "all_matches['shot_dominance'] = (all_matches['HS'] - all_matches['AS']) / (all_matches['HS'] + all_matches['AS'] + 0.001)\n",
    "all_matches['corner_dominance'] = (all_matches['HC'] - all_matches['AC']) / (all_matches['HC'] + all_matches['AC'] + 0.001)\n",
    "\n",
    "# 3. Game intensity features\n",
    "all_matches['total_fouls'] = all_matches['HF'] + all_matches['AF']\n",
    "all_matches['total_cards'] = all_matches['HY'] + all_matches['AY'] + all_matches['HR'] + all_matches['AR']\n",
    "all_matches['card_intensity'] = all_matches['total_cards'] / (all_matches['total_fouls'] + 0.001)\n",
    "\n",
    "# 4. Half-time patterns\n",
    "all_matches['ht_total_goals'] = all_matches['HTHG'] + all_matches['HTAG']\n",
    "all_matches['second_half_goals'] = all_matches['total_goals'] - all_matches['ht_total_goals']\n",
    "\n",
    "# 5. League tier (lower tiers might have different patterns)\n",
    "all_matches['league_tier'] = all_matches['Div'].str[-1].astype(int)\n",
    "\n",
    "# 6. Season timing features\n",
    "all_matches['month'] = all_matches['Date'].dt.month\n",
    "all_matches['is_weekend'] = all_matches['Date'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "\n",
    "print(\"Created basic engineered features:\")\n",
    "new_features = ['home_shot_accuracy', 'away_shot_accuracy', 'total_shots', 'total_shots_on_target',\n",
    "               'shot_dominance', 'corner_dominance', 'total_fouls', 'total_cards', 'card_intensity',\n",
    "               'ht_total_goals', 'second_half_goals', 'league_tier', 'month', 'is_weekend']\n",
    "\n",
    "for feature in new_features:\n",
    "    print(f\"- {feature}: mean={all_matches[feature].mean():.3f}, std={all_matches[feature].std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check what columns we actually have available\n",
    "available_cols = all_matches.columns.tolist()\n",
    "\n",
    "# Check for core match info columns\n",
    "core_match_info = ['Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG', 'HTR']\n",
    "\n",
    "# Check for match statistics columns from notes.txt\n",
    "match_stats_from_notes = ['Attendance', 'Referee', 'HS', 'AS', 'HST', 'AST', 'HHW', 'AHW', 'HC', 'AC',\n",
    "                         'HF', 'AF', 'HFKC', 'AFKC', 'HO', 'AO', 'HY', 'AY', 'HR', 'AR', 'HBP', 'ABP']\n",
    "\n",
    "# Identify all available core and match statistics columns\n",
    "basic_available = [col for col in core_match_info if col in available_cols]\n",
    "extended_available = [col for col in core_match_info + match_stats_from_notes if col in available_cols]\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Basic dataset core columns available: {len(basic_available)}\")\n",
    "print(f\"Extended dataset core columns available: {len(extended_available)}\")\n",
    "\n",
    "# Identify categorical and numerical columns for modeling\n",
    "categorical_features = ['Div', 'HomeTeam', 'AwayTeam', 'Country', 'FTR', 'HTR', 'Referee']  # League division, teams, and results\n",
    "ordinal_features = ['league_tier', 'month']  # Features with natural ordering\n",
    "\n",
    "# BASIC DATASET: Core match information (what would be available from basic match reports)\n",
    "basic_core_features = [col for col in ['Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR'] if col in all_matches.columns]\n",
    "\n",
    "# Engineered features that can be created from basic core columns only\n",
    "basic_engineered_features = [\n",
    "    'total_goals',   # Goal-based features\n",
    "    'league_tier', 'month', 'is_weekend'  # Date/league features\n",
    "]\n",
    "\n",
    "# EXTENDED DATASET: All available match data including detailed statistics\n",
    "extended_core_features = [col for col in [\n",
    "    # Core match info\n",
    "    'Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG', 'HTR',\n",
    "    # Match statistics\n",
    "    'Attendance', 'Referee', 'HS', 'AS', 'HST', 'AST', 'HHW', 'AHW', 'HC', 'AC',\n",
    "    'HF', 'AF', 'HFKC', 'AFKC', 'HO', 'AO', 'HY', 'AY', 'HR', 'AR', 'HBP', 'ABP'\n",
    "] if col in all_matches.columns]\n",
    "\n",
    "# All engineered features (using rich match statistics)\n",
    "extended_engineered_features = [\n",
    "    'total_goals', 'ht_total_goals', 'second_half_goals',  # Goal-based\n",
    "    'home_shot_accuracy', 'away_shot_accuracy', 'total_shots', 'total_shots_on_target',  # Shot-based\n",
    "    'shot_dominance', 'corner_dominance', 'total_fouls', 'total_cards', 'card_intensity',  # Game dynamics\n",
    "    'league_tier', 'month', 'is_weekend'  # Date/league features\n",
    "]\n",
    "\n",
    "# Extended features (betting odds - only high-quality columns after imputation)\n",
    "betting_features = []\n",
    "for col in all_matches.columns:\n",
    "    # Check if it's a betting column and has good data coverage (>10%)\n",
    "    if any(bookmaker in col for bookmaker in ['B365', 'BW', 'PS', 'IW', 'LB', 'WH', 'SJ', 'VC', 'BF', '1XB']) and col not in categorical_features:\n",
    "        data_coverage = (all_matches[col].notna().sum() / len(all_matches)) * 100\n",
    "        if data_coverage >= 10:  # Only include columns with at least 10% data coverage\n",
    "            betting_features.append(col)\n",
    "\n",
    "print(f\"\\nBASIC DATASET:\")\n",
    "print(f\"  Core features: {len(basic_core_features)} - {basic_core_features}\")\n",
    "print(f\"  Engineered features: {len(basic_engineered_features)} - {basic_engineered_features}\")\n",
    "print(f\"  Total basic features: {len(basic_core_features + basic_engineered_features)}\")\n",
    "\n",
    "print(f\"\\nEXTENDED DATASET:\")\n",
    "print(f\"  Core features: {len(extended_core_features)}\")\n",
    "print(f\"  Engineered features: {len(extended_engineered_features)}\")\n",
    "print(f\"  Betting features with >10% coverage: {len(betting_features)}\")\n",
    "print(f\"  Total extended features: {len(extended_core_features + extended_engineered_features + betting_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Dataset preparation and train-test split\n",
    "\n",
    "For time series data like sports matches, we need to be careful about temporal splitting to avoid data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Temporal train-test split\n",
    "\n",
    "Since this is time series data, we'll split chronologically to simulate real-world predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by date for temporal split\n",
    "all_matches_sorted = all_matches.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "# Use 80% for training (chronologically earlier), 20% for testing (more recent)\n",
    "split_date = all_matches_sorted['Date'].quantile(0.8)\n",
    "print(f\"Split date: {split_date}\")\n",
    "\n",
    "train_mask = all_matches_sorted['Date'] <= split_date\n",
    "test_mask = all_matches_sorted['Date'] > split_date\n",
    "\n",
    "train_data = all_matches_sorted[train_mask].copy()\n",
    "test_data = all_matches_sorted[test_mask].copy()\n",
    "\n",
    "print(f\"Training set: {len(train_data)} matches ({train_data['Date'].min()} to {train_data['Date'].max()})\")\n",
    "print(f\"Test set: {len(test_data)} matches ({test_data['Date'].min()} to {test_data['Date'].max()})\")\n",
    "print(f\"Train Over 2.5 rate: {train_data['over_2_5'].mean():.2%}\")\n",
    "print(f\"Test Over 2.5 rate: {test_data['over_2_5'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Basic vs Extended datasets\n",
    "\n",
    "Create two datasets as mentioned in the project goals:\n",
    "- **Basic dataset**: Core match statistics only\n",
    "- **Extended dataset**: Including betting odds and additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets for modeling\n",
    "basic_features = basic_core_features + basic_engineered_features\n",
    "extended_features = extended_core_features + extended_engineered_features + betting_features\n",
    "\n",
    "# Exclude columns that shouldn't be used directly in modeling\n",
    "excluded_from_modeling = ['Date', 'Time']  # These are used for feature engineering but not direct modeling\n",
    "\n",
    "# We'll handle categorical encoding in the modeling phase\n",
    "target = 'over_2_5'\n",
    "\n",
    "print(f\"Basic model features: {len(basic_features)}\")\n",
    "print(f\"Extended model features: {len(extended_features)}\")\n",
    "\n",
    "# Create datasets (without categorical encoding for now)\n",
    "def create_dataset(data, features, target_col):\n",
    "    \"\"\"Create feature matrix and target vector\"\"\"\n",
    "    # Only include features that exist in the data and exclude date/time columns for modeling\n",
    "    available_features = [f for f in features if f in data.columns and f not in excluded_from_modeling]\n",
    "\n",
    "    X = data[available_features].copy()\n",
    "    y = data[target_col].copy()\n",
    "\n",
    "    return X, y, available_features\n",
    "\n",
    "# Basic datasets (core columns + basic engineered features only)\n",
    "X_train_basic, y_train, basic_features_final = create_dataset(train_data, basic_features, target)\n",
    "X_test_basic, y_test, _ = create_dataset(test_data, basic_features, target)\n",
    "\n",
    "# Extended datasets (all match stats + betting odds + all engineered features)\n",
    "X_train_extended, _, extended_features_final = create_dataset(train_data, extended_features, target)\n",
    "X_test_extended, _, _ = create_dataset(test_data, extended_features, target)\n",
    "\n",
    "print(f\"\\nFinal feature counts:\")\n",
    "print(f\"Basic features available: {len(basic_features_final)}\")\n",
    "print(f\"Extended features available: {len(extended_features_final)}\")\n",
    "\n",
    "print(f\"\\nBasic features: {basic_features_final}\")\n",
    "print(f\"\\nExtended features sample (first 20): {extended_features_final[:20]}\")\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"X_train_basic: {X_train_basic.shape}\")\n",
    "print(f\"X_test_basic: {X_test_basic.shape}\")\n",
    "print(f\"X_train_extended: {X_train_extended.shape}\")\n",
    "print(f\"X_test_extended: {X_test_extended.shape}\")\n",
    "\n",
    "# Check for missing values in final datasets\n",
    "print(f\"\\nMissing values in basic features:\")\n",
    "print(X_train_basic.isnull().sum().sum())\n",
    "print(f\"Missing values in extended features:\")\n",
    "print(X_train_extended.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Save processed datasets\n",
    "\n",
    "Save the preprocessed data for use in modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed datasets\n",
    "datasets = {\n",
    "    'X_train_basic': X_train_basic_imputed,\n",
    "    'X_test_basic': X_test_basic_imputed,\n",
    "    'X_train_extended': X_train_extended_imputed,\n",
    "    'X_test_extended': X_test_extended_imputed,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test\n",
    "}\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    filepath = f\"{OUTPUT_DIR}/{name}.pkl\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "# Also save feature names\n",
    "feature_info = {\n",
    "    'basic_features': basic_features_final,\n",
    "    'extended_features': extended_features_final,\n",
    "    'target': target\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/feature_info.pkl\", 'wb') as f:\n",
    "    pickle.dump(feature_info, f)\n",
    "\n",
    "# Save also as CSV for easy inspection\n",
    "# X_train_basic_imputed.to_csv(f\"{OUTPUT_DIR}/X_train_basic.csv\", index=False)\n",
    "# X_train_extended_imputed.to_csv(f\"{OUTPUT_DIR}/X_train_extended.csv\", index=False)\n",
    "# y_train.to_csv(f\"{OUTPUT_DIR}/y_train.csv\", index=False)\n",
    "# y_test.to_csv(f\"{OUTPUT_DIR}/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Preprocessing Summary\n",
    "\n",
    "## What was accomplished:\n",
    "\n",
    "### Data Loading & Cleaning:\n",
    "- Loaded 42,593 matches from 11 countries and 21 leagues\n",
    "- Handled unnamed columns (100% missing data)\n",
    "- Normalized league codes (E0→E1, SC0→SC1, etc.)\n",
    "- Corrected English/Scottish yellow card counts\n",
    "- Converted data types properly (datetime, categorical, numerical)\n",
    "- Imputed missing values in key match statistics (<0.1% missing)\n",
    "\n",
    "### Exploratory Data Analysis:\n",
    "- Target variable (Over/Under 2.5 goals) perfectly balanced: 49.99% / 50.01%\n",
    "- Analyzed missing data patterns (betting odds 80%+ missing, match stats <0.1%)\n",
    "- Examined country/league distributions\n",
    "- Outlier analysis using z-scores (kept outliers as legitimate in football)\n",
    "\n",
    "### Feature Engineering:\n",
    "- Created target variable: `over_2_5` (Over/Under 2.5 goals)\n",
    "- Shot efficiency features: accuracy, total shots, dominance measures\n",
    "- Game intensity features: fouls, cards, card intensity\n",
    "- Temporal features: half-time patterns, second-half goals\n",
    "- League and seasonal features: tier, month, weekend indicator\n",
    "\n",
    "### Dataset Preparation:\n",
    "- **Temporal train-test split**: 80% train (2019-2024) / 20% test (2024-2025)\n",
    "- **Basic dataset**: ~7 features (ONLY original core columns: FTHG, FTAG, HTHG, HTAG + goal-based engineered features)\n",
    "- **Extended dataset**: ~40+ features (all match statistics + betting odds + all engineered features)\n",
    "- Missing value imputation for both datasets\n",
    "- Saved processed data for modeling\n",
    "\n",
    "## Data Quality:\n",
    "- **Training set**: 34,085 matches (49.56% Over 2.5)\n",
    "- **Test set**: 8,508 matches (51.72% Over 2.5)\n",
    "- **No missing values** in final processed datasets\n",
    "- **Temporally sorted** to prevent data leakage\n",
    "\n",
    "## Key Distinction:\n",
    "- **Basic Model**: Uses only original core data (goals) + simple derived features\n",
    "- **Extended Model**: Uses rich match statistics (shots, fouls, cards) + betting odds + complex features"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NBksARdgdMkP",
    "Qx3gpH8TdToi",
    "O5SNQWIMhxZA",
    "NNyVnO2Bk4zo",
    "XMiRfCrVzDVd",
    "fwwBVM5ex2SA"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
