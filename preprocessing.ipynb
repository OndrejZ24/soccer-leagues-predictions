{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML2 Semestral Project - Football O/U 2.5\n",
    "**Authors:** Phuong Nhi Tranová, Vít Maruniak, Šimon Slánský, Radim Škoukal, Ondřej Zetek, Martin Kareš, Jan Korčák, Jakub Maličkay, Jáchym Janouch  \n",
    "**Course:** FIS 4IT344 Machine Learning 2 (2025/2026)  \n",
    "**Goal:** Compare baseline (current features) vs extended (richer features) models for O/U 2.5 goals across markets; translate accuracy gains into optimal profit and **maximum data subscription price per country** *.  \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "***maximum data subscription price per country**\n",
    "- the most money our company should be willing to pay for that country's additional data\n",
    "- that's how much extra profit the improved model generates\n",
    "- baseline model → accuracy = A₀\n",
    "    - Generates profit Π*(A₀)\n",
    "- extended model → accuracy = A₁\n",
    "    - Generates profit Π*(A₁)\n",
    "- profit improvement = ΔΠ = Π(A₁) − Π(A₀)*\n",
    "    - basically how much more money the comany earns each year by using the better data\n",
    "- the maximum data subscription price per country = ΔΠ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports and paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore, chi2_contingency\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Library parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "OUTPUT_DIR = f\"./processed\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_matches(data_dir: str) -> pd.DataFrame:\n",
    "    csv_files = glob.glob(os.path.join(data_dir, \"**\", \"*.csv\"), recursive=True)\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSV files found under {data_dir}\")\n",
    "\n",
    "    frames = []\n",
    "    for fp in csv_files:\n",
    "        # extract path info\n",
    "        rel = os.path.relpath(fp, data_dir)\n",
    "        parts = Path(rel).parts\n",
    "        country = parts[0] if len(parts) >= 1 else None\n",
    "        league  = parts[1] if len(parts) >= 2 else None\n",
    "        season_file = parts[2] if len(parts) >= 3 else None\n",
    "        season_code = os.path.splitext(season_file)[0] if season_file else None\n",
    "\n",
    "        # read and rename\n",
    "        try:\n",
    "            df = pd.read_csv(fp, low_memory=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {fp}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Format season as YYYY/YYYY format\n",
    "        if season_code and len(season_code) == 4 and season_code.isdigit():\n",
    "            # Handle formats like \"1920\" or \"2021\"\n",
    "            year1 = int(season_code[:2])\n",
    "            year2 = int(season_code[2:])\n",
    "\n",
    "            # Determine century based on year range\n",
    "            if year1 >= 19 and year1 <= 24:  # 19-24 maps to 2019-2024\n",
    "                year1_full = 2000 + year1\n",
    "            else:\n",
    "                year1_full = 1900 + year1\n",
    "\n",
    "            if year2 >= 19 and year2 <= 99:\n",
    "                if year2 < year1:  # Next year (e.g., 19->20, 23->24)\n",
    "                    year2_full = 2000 + year2\n",
    "                else:\n",
    "                    year2_full = 2000 + year2\n",
    "            else:\n",
    "                year2_full = 1900 + year2\n",
    "\n",
    "            season_formatted = f\"{year1_full}/{year2_full}\"\n",
    "        else:\n",
    "            season_formatted = season_code  # Fallback to original if format is unexpected\n",
    "\n",
    "        # Add Season column right after Div (if Div exists)\n",
    "        if 'Div' in df.columns:\n",
    "            div_idx = df.columns.get_loc('Div')\n",
    "            df.insert(div_idx + 1, 'Season', season_formatted)\n",
    "        else:\n",
    "            df['Season'] = season_formatted\n",
    "\n",
    "        frames.append(df)\n",
    "\n",
    "    all_df = pd.concat(frames, ignore_index=True, sort=False)\n",
    "    return all_df\n",
    "\n",
    "# run the loader\n",
    "all_matches = pd.DataFrame(load_all_matches(DATA_DIR))\n",
    "print(all_matches.columns.tolist())\n",
    "print(all_matches.shape)\n",
    "display(all_matches.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis\n",
    "\n",
    "Before proceeding with data cleaning, let's understand our data better through comprehensive exploratory data analysis. This will help us make informed decisions about preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Shape and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset shape: {all_matches.shape}\")\n",
    "print(f\"Number of seasons/countries covered:\")\n",
    "print(f\"Countries: {all_matches['Div'].str[:-1].nunique()}\")\n",
    "print(f\"Leagues: {all_matches['Div'].nunique()}\")\n",
    "print(f\"Date range: {all_matches['Date'].min()} to {all_matches['Date'].max()}\")\n",
    "\n",
    "# Check basic statistics\n",
    "print(f\"\\nBasic goal statistics:\")\n",
    "print(f\"Total goals per match stats:\")\n",
    "total_goals = all_matches['FTHG'] + all_matches['FTAG']\n",
    "print(total_goals.describe())\n",
    "\n",
    "print(f\"\\nOver/Under 2.5 goals distribution:\")\n",
    "over_2_5 = (total_goals > 2.5).astype(int)\n",
    "print(f\"Over 2.5: {over_2_5.sum()} ({over_2_5.mean():.2%})\")\n",
    "print(f\"Under 2.5: {(~over_2_5.astype(bool)).sum()} ({(1-over_2_5.mean()):.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our target variable (Over/Under 2.5 goals) is perfectly balanced with almost exactly 50/50 split, which is ideal for classification. Mainly because the model won't be biased toward either class and we can use standard accuracy but also because we won't have to do any kind of resampling or rebalancing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed missing values analysis\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'column': all_matches.columns,\n",
    "    'missing_count': all_matches.isnull().sum(),\n",
    "    'missing_percentage': (all_matches.isnull().sum() / len(all_matches)) * 100,\n",
    "    'dtype': all_matches.dtypes\n",
    "})\n",
    "\n",
    "# Filter to show only columns with missing values\n",
    "missing_analysis = missing_analysis[missing_analysis['missing_count'] > 0].sort_values('missing_percentage', ascending=False)\n",
    "\n",
    "print(f\"Columns with missing values: {len(missing_analysis)}\")\n",
    "print(f\"Total columns: {len(all_matches.columns)}\")\n",
    "print(f\"\\nTop 20 columns with highest missing percentage:\")\n",
    "display(missing_analysis.head(20))\n",
    "\n",
    "# Check missing patterns in key variables\n",
    "key_stats = ['HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "print(f\"\\nMissing data in key match statistics:\")\n",
    "for stat in key_stats:\n",
    "    if stat in all_matches.columns:\n",
    "        missing_pct = (all_matches[stat].isnull().sum() / len(all_matches)) * 100\n",
    "        print(f\"{stat}: {missing_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing data analysis reveals that:\n",
    "1. **Betting odds** have the highest missing percentages (80%+) - this is expected as not all bookmakers operate in all leagues/seasons\n",
    "2. **Key match statistics** (shots, corners, fouls, cards) have very low missing rates (<0.1%), which is excellent for our modeling\n",
    "3. Most missing data is in betting-related columns, which we can handle appropriately\n",
    "\n",
    "also we have found 4 unnamed columns that are 100% missing. they're most likely artifacts from csv exports so they're definitely safe to drop outright"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets do a bit more of a in depth analysis, shall we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = all_matches.copy()\n",
    "\n",
    "# missingness flag\n",
    "stats_cols = ['HS','AS','HST','AST','HF','AF','HC','AC','HY','AY','HR','AR']\n",
    "for c in stats_cols:\n",
    "    if c in raw.columns:\n",
    "        raw[f'isna_{c}'] = raw[c].isna().astype(int)\n",
    "\n",
    "# Row-level summary: how many of the 12 stats are missing in the same row?\n",
    "flag_cols = [f'isna_{c}' for c in stats_cols if f'isna_{c}' in raw.columns]\n",
    "raw['missing_count_stats'] = raw[flag_cols].sum(axis=1)\n",
    "\n",
    "# Quick overview\n",
    "print(raw['missing_count_stats'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the rows seem to have no missigness/ However, there are 41 rows that have are missing all 12 variables, which seems pretty clustered. Suggesting that the missing data likely stem from a specific data source or a batch issue rather than random omission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single-stat missing % (already computed as flags)\n",
    "single_rates = (raw[flag_cols].mean() * 100)\n",
    "single_rates.index = [c.replace('isna_', '') for c in single_rates.index]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "ax.bar(single_rates.index, single_rates.values)\n",
    "ax.set_title('Missingness by variables (%)')\n",
    "ax.set_ylabel('% missing')\n",
    "ax.set_xlabel('stat')\n",
    "ax.set_xticklabels(single_rates.index, rotation=45, ha='right')\n",
    "for i, v in enumerate(single_rates.values):\n",
    "    ax.text(i, v, f'{v:.3f}%', ha='center', va='bottom', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "misigness seems uniformly low across all variables, there seems to be no issue with a variable specific collection issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "cmap = 'plasma'\n",
    "\n",
    "# Extract country from Div column (e.g., 'E1' -> 'E', 'SP2' -> 'SP')\n",
    "raw['country_code'] = raw['Div'].str[:-1]\n",
    "\n",
    "# 1️⃣ Country × Stat\n",
    "if 'country_code' in raw.columns:\n",
    "    M1 = raw.groupby('country_code')[flag_cols].mean().mul(100)\n",
    "    order = M1.mean(axis=1).sort_values(ascending=False).index\n",
    "    M1 = M1.loc[order]\n",
    "    M1.columns = [c.replace('isna_', '') for c in M1.columns]\n",
    "\n",
    "    im1 = axes[0].imshow(M1.values, aspect='auto', cmap=cmap)\n",
    "    axes[0].set_xticks(np.arange(M1.shape[1]))\n",
    "    axes[0].set_xticklabels(M1.columns, rotation=45, ha='right')\n",
    "    axes[0].set_yticks(np.arange(M1.shape[0]))\n",
    "    axes[0].set_yticklabels(M1.index)\n",
    "    axes[0].set_title('Country × Stat')\n",
    "    fig.colorbar(im1, ax=axes[0], label='% missing')\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, \"Missing 'Div' column\", ha='center', va='center')\n",
    "    axes[0].set_axis_off()\n",
    "\n",
    "# 2️⃣ Year × Stat (using Date column)\n",
    "if 'Date' in raw.columns:\n",
    "    raw['year'] = pd.to_datetime(raw['Date']).dt.year\n",
    "    M2 = raw.groupby('year')[flag_cols].mean().mul(100)\n",
    "    order = M2.mean(axis=1).sort_values(ascending=False).index\n",
    "    M2 = M2.loc[order]\n",
    "    M2.columns = [c.replace('isna_', '') for c in M2.columns]\n",
    "\n",
    "    im2 = axes[1].imshow(M2.values, aspect='auto', cmap=cmap)\n",
    "    axes[1].set_xticks(np.arange(M2.shape[1]))\n",
    "    axes[1].set_xticklabels(M2.columns, rotation=45, ha='right')\n",
    "    axes[1].set_yticks(np.arange(M2.shape[0]))\n",
    "    axes[1].set_yticklabels(M2.index.astype(int))\n",
    "    axes[1].set_title('Year × Stat')\n",
    "    fig.colorbar(im2, ax=axes[1], label='% missing')\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, \"Missing 'Date' column\", ha='center', va='center')\n",
    "    axes[1].set_axis_off()\n",
    "\n",
    "# 3️⃣ Year × Country\n",
    "needed = {'year', 'country_code'}\n",
    "if needed.issubset(raw.columns):\n",
    "    G = raw.groupby(['year','country_code'])[flag_cols].mean().mul(100)\n",
    "    G['avg_missing'] = G.mean(axis=1)\n",
    "    year_order  = G['avg_missing'].groupby(level=0).mean().sort_values(ascending=False).index\n",
    "    country_order = G['avg_missing'].groupby(level=1).mean().sort_values(ascending=False).index\n",
    "    P3 = (G['avg_missing'].unstack('country_code')\n",
    "          .reindex(index=year_order, columns=country_order)\n",
    "          .fillna(0))\n",
    "\n",
    "    im3 = axes[2].imshow(P3.values, aspect='auto', cmap=cmap)\n",
    "    axes[2].set_xticks(np.arange(P3.shape[1]))\n",
    "    axes[2].set_xticklabels(P3.columns, rotation=45, ha='right')\n",
    "    axes[2].set_yticks(np.arange(P3.shape[0]))\n",
    "    axes[2].set_yticklabels(P3.index.astype(int))\n",
    "    axes[2].set_title('Year × Country')\n",
    "    fig.colorbar(im3, ax=axes[2], label='% missing')\n",
    "else:\n",
    "    axes[2].text(0.5, 0.5, \"Missing required columns\", ha='center', va='center')\n",
    "    axes[2].set_axis_off()\n",
    "\n",
    "# 4️⃣ Country × League\n",
    "needed = {'country_code', 'Div'}\n",
    "if needed.issubset(raw.columns):\n",
    "    G = raw.groupby(['country_code', 'Div'])[flag_cols].mean().mul(100)\n",
    "    G['avg_missing'] = G.mean(axis=1)\n",
    "    P4 = (G['avg_missing'].unstack('Div').fillna(0))\n",
    "    country_order = P4.mean(axis=1).sort_values(ascending=False).index\n",
    "    league_order  = P4.mean(axis=0).sort_values(ascending=False).index\n",
    "    P4 = P4.loc[country_order, league_order]\n",
    "\n",
    "    im4 = axes[3].imshow(P4.values, aspect='auto', cmap=cmap)\n",
    "    axes[3].set_xticks(np.arange(P4.shape[1]))\n",
    "    axes[3].set_xticklabels(P4.columns, rotation=45, ha='right')\n",
    "    axes[3].set_yticks(np.arange(P4.shape[0]))\n",
    "    axes[3].set_yticklabels(P4.index)\n",
    "    axes[3].set_title('Country × League')\n",
    "    fig.colorbar(im4, ax=axes[3], label='% missing')\n",
    "else:\n",
    "    axes[3].text(0.5, 0.5, \"Missing required columns\", ha='center', va='center')\n",
    "    axes[3].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first heatmap shows missing data by country. Turkey has the most missing data by far, with over 1.4 percent missing on average. All other countries have very little missing data, less than 0.5 percent each.\n",
    "\n",
    "The second heatmap shows missing data by year. The years 2023 has slightly more missing data than the other years.\n",
    "\n",
    "The third heatmap combines year and country together. It shows that Turkey has most missing values in 2023. In other years, the missingness is not so bad.\n",
    "\n",
    "The fourth heatmap shows missing data by country and league division. Again, Turkey stands out with the highest missing data. Within each country, different league divisions have similar amounts of missing data, which means the problem is more about the country than about which league tier we look at.\n",
    "\n",
    "Overall, the missing data is not random. It is concentrated mainly in Turkey and in the year 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_cols  = [f'isna_{c}' for c in stats_cols if f'isna_{c}' in raw.columns]\n",
    "top_n      = 15\n",
    "min_matches_ref = 50   # ignore refs with tiny sample sizes\n",
    "\n",
    "def group_missing_rate(df, key):\n",
    "    \"\"\"Return DataFrame with avg % missing across 12 stats, plus counts.\"\"\"\n",
    "    grp = df.groupby(key)[flag_cols]\n",
    "    rate = grp.mean().mul(100).mean(axis=1)\n",
    "    cnt  = df.groupby(key).size()\n",
    "    out  = pd.DataFrame({'rate': rate, 'n': cnt}).sort_values('rate', ascending=False)\n",
    "    return out\n",
    "\n",
    "# 1️⃣ Home, Away, Referee\n",
    "home_df = group_missing_rate(raw, 'HomeTeam') if 'HomeTeam' in raw.columns else pd.DataFrame()\n",
    "away_df = group_missing_rate(raw, 'AwayTeam') if 'AwayTeam' in raw.columns else pd.DataFrame()\n",
    "ref_df  = group_missing_rate(raw, 'Referee')  if 'Referee'  in raw.columns else pd.DataFrame()\n",
    "if not ref_df.empty:\n",
    "    ref_df = ref_df[ref_df['n'] >= min_matches_ref].sort_values('rate', ascending=False)\n",
    "\n",
    "# 2️⃣ Merge for Home vs Away comparison (teams present in both)\n",
    "both = pd.DataFrame()\n",
    "if not home_df.empty and not away_df.empty:\n",
    "    both = (home_df[['rate']].rename(columns={'rate': 'home_rate'})\n",
    "            .merge(away_df[['rate']], left_index=True, right_index=True, how='inner')\n",
    "            .rename(columns={'rate': 'away_rate'}))\n",
    "    both['diff'] = both['home_rate'] - both['away_rate']\n",
    "    both = both.sort_values('home_rate', ascending=False).head(top_n)\n",
    "\n",
    "# =======================\n",
    "# FIGURE 1 — Home & Away\n",
    "# =======================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# A) Top Home teams\n",
    "if not home_df.empty:\n",
    "    htop = home_df.head(top_n)[::-1]\n",
    "    axes[0].barh(htop.index.astype(str), htop['rate'].values, color='#8c564b')\n",
    "    axes[0].set_title('Missingness by HomeTeam (avg % across stats)')\n",
    "    axes[0].set_xlabel('% missing')\n",
    "    for y, (r, n) in enumerate(zip(htop['rate'].values, htop['n'].values)):\n",
    "        axes[0].text(r, y, f'  {r:.2f}% (n={n})', va='center', ha='left', fontsize=9)\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, \"HomeTeam column not found\", ha='center', va='center')\n",
    "    axes[0].set_axis_off()\n",
    "\n",
    "# B) Top Away teams\n",
    "if not away_df.empty:\n",
    "    atop = away_df.head(top_n)[::-1]\n",
    "    axes[1].barh(atop.index.astype(str), atop['rate'].values, color='#1f77b4')\n",
    "    axes[1].set_title('Missingness by AwayTeam (avg % across stats)')\n",
    "    axes[1].set_xlabel('% missing')\n",
    "    for y, (r, n) in enumerate(zip(atop['rate'].values, atop['n'].values)):\n",
    "        axes[1].text(r, y, f'  {r:.2f}% (n={n})', va='center', ha='left', fontsize=9)\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, \"AwayTeam column not found\", ha='center', va='center')\n",
    "    axes[1].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization compares the average percentage of missing match statistics for each team when playing at home (brown dots) versus away (blue dots). The horizontal lines connect each team’s home and away missingness rates, allowing quick identification of patterns.\n",
    "\n",
    "Most teams show very little difference between home and away games, suggesting that data gaps are not related to the venue. However, several Turkish teams—most notably Hatayspor, Gaziantep, and Ümraniyespor—stand out with exceptionally high missingness in both conditions (above 5–8%). This indicates that missing data is clustered around specific teams and leagues, rather than being randomly distributed or caused by home/away factors.\n",
    "\n",
    "Overall, the visualization reinforces that the missingness originates from systematic collection or feed issues affecting particular teams or competitions, rather than isolated recording errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missigness for referees\n",
    "if not ref_df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    rtop = ref_df.head(top_n)[::-1]\n",
    "    ax.barh(rtop.index.astype(str), rtop['rate'].values, color='#9467bd')\n",
    "    ax.set_title(f'Missingness by Referee (avg % across stats, n≥{min_matches_ref})')\n",
    "    ax.set_xlabel('% missing')\n",
    "    for y, (r, n) in enumerate(zip(rtop['rate'].values, rtop['n'].values)):\n",
    "        ax.text(r, y, f'  {r:.2f}% (n={n})', va='center', ha='left', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No referees pass the sample-size filter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper: % missing by group (avg across rows)\n",
    "def pct_missing_by(group_cols, cols):\n",
    "    G = raw.groupby(group_cols)[cols].mean().mul(100)   # % per stat\n",
    "    return G\n",
    "\n",
    "# =========================\n",
    "# Figure A — Year × Stat\n",
    "# =========================\n",
    "if 'year' in raw.columns:\n",
    "    YS = pct_missing_by(['year'], flag_cols)\n",
    "    # order years by overall missingness (desc)\n",
    "    order = YS.mean(axis=1).sort_values(ascending=False).index\n",
    "    YS = YS.loc[order]\n",
    "    YS.columns = [c.replace('isna_', '') for c in YS.columns]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    im = ax.imshow(YS.values, aspect='auto')\n",
    "    ax.set_xticks(np.arange(YS.shape[1]))\n",
    "    ax.set_xticklabels(YS.columns, rotation=45, ha='right')\n",
    "    ax.set_yticks(np.arange(YS.shape[0]))\n",
    "    ax.set_yticklabels(YS.index.astype(int))\n",
    "    ax.set_title('Missingness heatmap (%) — Year × Stat')\n",
    "    fig.colorbar(im, ax=ax, label='% missing')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================\n",
    "# Figure B — Year trend (avg across all stats)\n",
    "# =============================================\n",
    "if 'year' in raw.columns:\n",
    "    Y_avg = (raw.groupby('year')[flag_cols].mean().mul(100).mean(axis=1)\n",
    "             .sort_index())\n",
    "    fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "    ax.plot(Y_avg.index.astype(int), Y_avg.values, marker='o')\n",
    "    for x, y in zip(Y_avg.index, Y_avg.values):\n",
    "        ax.text(x, y, f'{y:.2f}%', va='bottom', ha='center', fontsize=9)\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('% missing (avg across stats)')\n",
    "    ax.set_title('Missingness over time (yearly average)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =========================\n",
    "# Figure C — Hour × Stat\n",
    "# =========================\n",
    "if 'hour' in raw.columns:\n",
    "    # drop hours that are NaN (unparseable)\n",
    "    HH = raw.dropna(subset=['hour']).copy()\n",
    "    HH['hour'] = HH['hour'].astype(int)\n",
    "    HS = (HH.groupby('hour')[flag_cols].mean().mul(100))\n",
    "    # ensure 0–23 present (fill with zeros if absent)\n",
    "    HS = HS.reindex(range(0, 24), fill_value=0)\n",
    "    HS.columns = [c.replace('isna_', '') for c in HS.columns]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    im = ax.imshow(HS.values, aspect='auto')\n",
    "    ax.set_xticks(np.arange(HS.shape[1]))\n",
    "    ax.set_xticklabels(HS.columns, rotation=45, ha='right')\n",
    "    ax.set_yticks(np.arange(HS.shape[0]))\n",
    "    ax.set_yticklabels(HS.index.astype(int))\n",
    "    ax.set_title('Missingness heatmap (%) — Hour × Stat')\n",
    "    fig.colorbar(im, ax=ax, label='% missing')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Sanity Checks\n",
    "\n",
    "Before moving forward, we need to verify that our data makes logical sense. We will check if the relationships between different columns are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_issues = []\n",
    "\n",
    "# Check 1: Full time goals should be >= half time goals\n",
    "print(\"\\nFull Time Goals >= Half Time Goals\")\n",
    "ht_ft_home_check = all_matches['FTHG'] >= all_matches['HTHG']\n",
    "ht_ft_away_check = all_matches['FTAG'] >= all_matches['HTAG']\n",
    "home_violations = (~ht_ft_home_check).sum()\n",
    "away_violations = (~ht_ft_away_check).sum()\n",
    "print(f\"Home goals violations: {home_violations}\")\n",
    "print(f\"Away goals violations: {away_violations}\")\n",
    "if home_violations > 0 or away_violations > 0:\n",
    "    sanity_issues.append(f\"FT goals < HT goals: {home_violations + away_violations} cases\")\n",
    "\n",
    "# Check 2: Full time result should match actual goals\n",
    "print(\"\\nFull Time Result matches actual goals\")\n",
    "ftr_check = pd.Series(index=all_matches.index, dtype=bool)\n",
    "ftr_check = (\n",
    "    ((all_matches['FTR'] == 'H') & (all_matches['FTHG'] > all_matches['FTAG'])) |\n",
    "    ((all_matches['FTR'] == 'A') & (all_matches['FTAG'] > all_matches['FTHG'])) |\n",
    "    ((all_matches['FTR'] == 'D') & (all_matches['FTHG'] == all_matches['FTAG']))\n",
    ")\n",
    "ftr_violations = (~ftr_check).sum()\n",
    "print(f\"FTR mismatches: {ftr_violations}\")\n",
    "if ftr_violations > 0:\n",
    "    sanity_issues.append(f\"FTR doesn't match goals: {ftr_violations} cases\")\n",
    "\n",
    "# Check 3: Half time result should match half time goals\n",
    "print(\"\\nChecking: Half Time Result matches half time goals\")\n",
    "htr_check = pd.Series(index=all_matches.index, dtype=bool)\n",
    "htr_check = (\n",
    "    ((all_matches['HTR'] == 'H') & (all_matches['HTHG'] > all_matches['HTAG'])) |\n",
    "    ((all_matches['HTR'] == 'A') & (all_matches['HTAG'] > all_matches['HTHG'])) |\n",
    "    ((all_matches['HTR'] == 'D') & (all_matches['HTHG'] == all_matches['HTAG']))\n",
    ")\n",
    "htr_violations = (~htr_check).sum()\n",
    "print(f\"HTR mismatches: {htr_violations}\")\n",
    "if htr_violations > 0:\n",
    "    sanity_issues.append(f\"HTR doesn't match HT goals: {htr_violations} cases\")\n",
    "\n",
    "# Check 4: Shots on target should be <= total shots\n",
    "print(\"\\nShots on Target <= Total Shots\")\n",
    "home_shot_check = all_matches['HST'] <= all_matches['HS']\n",
    "away_shot_check = all_matches['AST'] <= all_matches['AS']\n",
    "home_shot_violations = (~home_shot_check).sum()\n",
    "away_shot_violations = (~away_shot_check).sum()\n",
    "print(f\"Home shots violations: {home_shot_violations}\")\n",
    "print(f\"Away shots violations: {away_shot_violations}\")\n",
    "if home_shot_violations > 0 or away_shot_violations > 0:\n",
    "    sanity_issues.append(f\"Shots on target > total shots: {home_shot_violations + away_shot_violations} cases\")\n",
    "\n",
    "# Check 5: Goals should be <= shots on target (generally, but not always)\n",
    "print(\"\\nGoals <= Shots on Target (usually)\")\n",
    "home_goals_shots_check = all_matches['FTHG'] <= all_matches['HST']\n",
    "away_goals_shots_check = all_matches['FTAG'] <= all_matches['AST']\n",
    "home_goals_violations = (~home_goals_shots_check).sum()\n",
    "away_goals_violations = (~away_goals_shots_check).sum()\n",
    "print(f\"Home goals > shots on target: {home_goals_violations}\")\n",
    "print(f\"Away goals > shots on target: {away_goals_violations}\")\n",
    "print(f\"Note: Some violations are possible due to own goals or deflections\")\n",
    "if home_goals_violations > 10 or away_goals_violations > 10:\n",
    "    sanity_issues.append(f\"Goals > shots on target: {home_goals_violations + away_goals_violations} cases (check if excessive)\")\n",
    "\n",
    "# Check 6: Red cards should be <= yellow cards + red cards\n",
    "print(\"\\nCard counts are reasonable\")\n",
    "home_red_check = all_matches['HR'] <= (all_matches['HY'] + all_matches['HR'])\n",
    "away_red_check = all_matches['AR'] <= (all_matches['AY'] + all_matches['AR'])\n",
    "print(f\"Home card logic violations: {(~home_red_check).sum()}\")\n",
    "print(f\"Away card logic violations: {(~away_red_check).sum()}\")\n",
    "\n",
    "# Check 7: Negative values check\n",
    "print(\"\\nNo negative values in count columns\")\n",
    "count_columns = ['FTHG', 'FTAG', 'HTHG', 'HTAG', 'HS', 'AS', 'HST', 'AST',\n",
    "                'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "negative_found = False\n",
    "for col in count_columns:\n",
    "    if col in all_matches.columns:\n",
    "        negative_count = (all_matches[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            print(f\"{col}: {negative_count} negative values\")\n",
    "            negative_found = True\n",
    "            sanity_issues.append(f\"{col} has {negative_count} negative values\")\n",
    "if not negative_found:\n",
    "    print(f\"No negative values found\")\n",
    "\n",
    "# Check 8: Extreme values check\n",
    "print(\"\\nExtreme values that might be data errors\")\n",
    "extreme_checks = {\n",
    "    'FTHG': 15,\n",
    "    'FTAG': 15,\n",
    "    'HS': 50,\n",
    "    'AS': 50,\n",
    "    'HC': 30,\n",
    "    'AC': 30,\n",
    "    'HY': 10,\n",
    "    'AY': 10,\n",
    "    'HR': 5,\n",
    "    'AR': 5\n",
    "}\n",
    "for col, threshold in extreme_checks.items():\n",
    "    if col in all_matches.columns:\n",
    "        extreme_count = (all_matches[col] > threshold).sum()\n",
    "        if extreme_count > 0:\n",
    "            max_value = all_matches[col].max()\n",
    "            print(f\"{col} > {threshold}: {extreme_count} cases (max: {max_value})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sanity checks help us verify that the data is internally consistent. We check things like full time goals being at least as many as half time goals, that the match result codes match the actual goal counts, that shots on target do not exceed total shots, and that there are no negative values in count columns. These checks help identify data entry errors or corruption before we use the data for modeling.\n",
    "\n",
    "Our data passed most checks well. Full time goals are always at least as many as half time goals, which is correct. The full time result codes match the actual scores perfectly.\n",
    "\n",
    "We found 41 matches where the half time result code does not match the half time goals. This is a small number out of 42,593 matches, so it is likely just data entry errors in those specific matches.\n",
    "\n",
    "We found 6 matches where shots on target are higher than total shots. This is probably a recording error but only affects 6 matches so it is not a big problem.\n",
    "\n",
    "We found 234 matches where a team scored more goals than they had shots on target. This can happen in real football due to own goals or deflections, so these are not necessarily errors.\n",
    "\n",
    "We found one match where a team got 9 red cards. This is extremely unusual and might be a data error, but it is only one match out of thousands.\n",
    "\n",
    "Overall, the data quality is very good. The few issues we found affect less than 1 percent of matches and will not significantly impact our model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 League and Country Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# League distribution\n",
    "league_counts = all_matches['Div'].value_counts()\n",
    "print(\"League distribution:\")\n",
    "display(league_counts)\n",
    "\n",
    "# Country mapping for better understanding\n",
    "country_mapping = {\n",
    "    'E': 'England', 'SC': 'Scotland', 'SP': 'Spain', 'I': 'Italy',\n",
    "    'D': 'Germany', 'F': 'France', 'N': 'Netherlands', 'B': 'Belgium',\n",
    "    'P': 'Portugal', 'T': 'Turkey', 'G': 'Greece'\n",
    "}\n",
    "\n",
    "all_matches['Country'] = all_matches['Div'].str[:-1].map(country_mapping)\n",
    "country_counts = all_matches['Country'].value_counts()\n",
    "print(f\"\\nMatches per country:\")\n",
    "display(country_counts)\n",
    "\n",
    "# Visualize the distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Country distribution\n",
    "country_counts_sorted = country_counts.dropna().sort_values(ascending=False)\n",
    "bars = ax1.bar(country_counts_sorted.index, country_counts_sorted.values, color='skyblue')\n",
    "ax1.set_title('Matches per Country')\n",
    "ax1.set_xlabel('Country')\n",
    "ax1.set_ylabel('Number of Matches')\n",
    "ax1.tick_params(axis='x', rotation=30)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# data labels\n",
    "for rect in bars:\n",
    "    height = rect.get_height()\n",
    "    ax1.text(rect.get_x() + rect.get_width()/2, height, f\"{int(height):,}\",\n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Goals distribution\n",
    "total_goals = all_matches['FTHG'] + all_matches['FTAG']\n",
    "max_g = int(np.nanmax(total_goals))\n",
    "bins = np.arange(-0.5, max(10, max_g) + 1.5, 1)\n",
    "\n",
    "ax2.hist(total_goals, bins=bins, color='lightcoral', alpha=0.7)\n",
    "ax2.axvline(x=2.5, linestyle='--', linewidth=2, label='2.5 goals threshold')\n",
    "ax2.set_title('Distribution of Total Goals per Match')\n",
    "ax2.set_xlabel('Total Goals')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_xticks(range(0, max(10, max_g) + 1))\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "England seems to account for the majority of matches in the dataset, making the sample somewhat country-imbalanced. This suggests that model training should be performed separately for each country, or at least include country-specific components, to prevent English leagues from dominating the overall model behavior.  \n",
    "\n",
    "When building time-aware models, it would also be beneficial to use chronological splits within each country and consider assigning higher weights to more recent matches, since they better reflect current team dynamics and scoring trends.  \n",
    "\n",
    "Alse, the distribution of total goals per match is right-skewed, with mode around 2–3 goals. The red dashed line at 2.5 goals marks the classification threshold for our target variable. Visually, the mass on either side of this threshold is roughly equal, which confirms the balanced 50/50 split observed in the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Handling csv issues\n",
    "It seems like the renaming and loading went smoothly! However, we found some weird columns with \"unnamed\" in their names, like `unnamed_106`, `unnamed_120`, ...  \n",
    "That sometimes happens when excel files have extra blank columns. We'll take a quick look to see if they have any data, and if they're totally empty (full of NaNs), we'll just get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnamed_cols = [c for c in all_matches.columns if c.lower().startswith(\"unnamed\")]\n",
    "all_matches[unnamed_cols].isna().mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're 100% full of NaNs so we can now safely drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = all_matches.drop(columns=unnamed_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Normalizing league codes\n",
    "Let's normalize the leagues, as English and Scottish leagues have the best leagues interpreted as E0, SC0, respectively. All other countries mark the best league as CountryCode1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = all_matches['Div'].str.startswith(('E', 'SC'))\n",
    "all_matches.loc[mask, 'Div'] = all_matches.loc[mask, 'Div'].apply(\n",
    "    lambda x: f\"{x[:-1]}{int(x[-1]) + 1}\"\n",
    ")\n",
    "\n",
    "print(all_matches['Div'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Handling English and Scottish yellow cards\n",
    "We need to take care of the first note in notex.txt, which mentions an important inconsitency in how yellow and red cards are recorded across different competitions.  \n",
    "\n",
    "In English and Scottish leagues, when a player receives a second yellow card that leads to a red card, the initial yellow card is not counted in the match statistics, only the red card is recorded. However, European and international competitions record both: the second yellow is counted as an additional yellow card plus a red card \n",
    "\n",
    "As a result, yellow card totals in English and Scottish matches can underestimate the true number of yellow cards compared to other leagues. To correct for this and ensure consistency across competitions, we applied a simple adjustment:\n",
    "- whenever a team has exactly one red card and one yellow card, we add one additional yellow card.\n",
    "- and if a team has 0 reds, 2 or more reds, or 1 red but no yellows, we make no adjustment.\n",
    "\n",
    "We acknowledge that this rule is an approximation, our adjustment may not always be the case and it may introduce some bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = all_matches['Div'].str.startswith(('E', 'SC'))\n",
    "red_mask = mask & ((all_matches['HR'] == 1) | (all_matches['AR'] == 1))\n",
    "\n",
    "print(\"Before adjustment (sample):\")\n",
    "print(all_matches.loc[red_mask, ['Div', 'HY', 'HR', 'AY', 'AR']].head())\n",
    "\n",
    "all_matches.loc[mask & (all_matches['HR'] == 1) & (all_matches['HY'] == 0), 'HY'] += 1\n",
    "all_matches.loc[mask & (all_matches['AR'] == 1) & (all_matches['AY'] == 0), 'AY'] += 1\n",
    "\n",
    "print(\"\\nAfter adjustment (sample):\")\n",
    "print(all_matches.loc[red_mask, ['Div', 'HY', 'HR', 'AY', 'AR']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Correcting data types\n",
    "Now, let's inspect the data types of our columns. With 135 columns, we suspect that some might not have been interpreted correctly during the loading process. Checking the data types is an important step before proceeding with any further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, dtype in all_matches.dtypes.items():\n",
    "    print(f\"{col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_columns = ['Date', 'Time']\n",
    "\n",
    "category_columns = ['Div', 'HomeTeam', 'AwayTeam', 'FTR', 'HTR', 'Referee', 'Country']\n",
    "\n",
    "int_columns = ['FTHG', 'FTAG', 'HTHG', 'HTAG', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "\n",
    "float_columns = ['B365CH', 'BWCA', '1XBH']\n",
    "\n",
    "for col in time_columns:\n",
    "    if col == 'Date':\n",
    "        all_matches[col] = pd.to_datetime(all_matches[col])\n",
    "    else:\n",
    "        all_matches[col] = pd.to_datetime(all_matches[col], format='%H:%M').dt.time\n",
    "\n",
    "for col in category_columns:\n",
    "    if col in all_matches.columns:\n",
    "        all_matches[col] = all_matches[col].astype('category')\n",
    "\n",
    "for col in int_columns:\n",
    "    all_matches[col] = pd.to_numeric(all_matches[col], errors='coerce').astype('Int64')\n",
    "\n",
    "for col in float_columns:\n",
    "    all_matches[col] = pd.to_numeric(all_matches[col], errors='coerce').astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, dtype in all_matches.dtypes.items():\n",
    "    print(f\"{col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Outlier detection and handling\n",
    "\n",
    "Following the methodology from Week1 (house pricing), we'll use z-score analysis to detect outliers in match statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical columns for outlier detection\n",
    "match_stats_cols = ['HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "numerical_cols = ['FTHG', 'FTAG', 'HTHG', 'HTAG'] + match_stats_cols\n",
    "\n",
    "# Calculate z-scores for numerical columns\n",
    "print(\"Outlier analysis using z-score > 3:\")\n",
    "outlier_counts = {}\n",
    "\n",
    "for col in numerical_cols:\n",
    "    if col in all_matches.columns:\n",
    "        z_scores = np.abs(zscore(all_matches[col].dropna()))\n",
    "        outliers = (z_scores > 3).sum()\n",
    "        outlier_counts[col] = outliers\n",
    "        if outliers > 0:\n",
    "            print(f\"{col}: {outliers} outliers ({outliers/len(all_matches)*100:.2f}%)\")\n",
    "\n",
    "# Look at extreme cases\n",
    "print(f\"\\nExamples of potential outliers:\")\n",
    "print(f\"Highest total goals: {all_matches['FTHG'].max() + all_matches['FTAG'].max()}\")\n",
    "print(f\"Most shots in a match: {all_matches['HS'].max() + all_matches['AS'].max()}\")\n",
    "print(f\"Most cards in a match: {all_matches['HY'].max() + all_matches['AY'].max()}\")\n",
    "\n",
    "# Visualize outliers for key variables\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "key_vars = ['FTHG', 'FTAG', 'HS', 'AS']\n",
    "\n",
    "for i, var in enumerate(key_vars):\n",
    "    row, col = i // 2, i % 2\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    # Box plot to show outliers\n",
    "    all_matches[var].plot(kind='box', ax=ax)\n",
    "    ax.set_title(f'Box Plot of {var}')\n",
    "    ax.set_ylabel(var)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# For football data, we'll be more conservative with outlier removal\n",
    "# as extreme scores can be legitimate (unlike house prices)\n",
    "print(f\"\\nDecision: Keep outliers for football data as high scores/stats can be legitimate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature engineering\n",
    "\n",
    "Based on soccer domain knowledge and the course materials, we'll create meaningful features that could help predict Over/Under 2.5 goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Target variable creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the main target variable: Over/Under 2.5 goals\n",
    "all_matches['total_goals'] = all_matches['FTHG'] + all_matches['FTAG']\n",
    "all_matches['over_2_5'] = (all_matches['total_goals'] > 2.5).astype(int)\n",
    "\n",
    "print(\"Target variable distribution:\")\n",
    "print(all_matches['over_2_5'].value_counts())\n",
    "print(f\"Over 2.5 rate: {all_matches['over_2_5'].mean():.2%}\")\n",
    "\n",
    "# Also create alternative targets for analysis\n",
    "all_matches['over_1_5'] = (all_matches['total_goals'] > 1.5).astype(int)\n",
    "all_matches['over_3_5'] = (all_matches['total_goals'] > 3.5).astype(int)\n",
    "\n",
    "print(f\"\\nOther thresholds:\")\n",
    "print(f\"Over 1.5 rate: {all_matches['over_1_5'].mean():.2%}\")\n",
    "print(f\"Over 3.5 rate: {all_matches['over_3_5'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Basic feature engineering\n",
    "\n",
    "Creating features that capture match dynamics and team performance patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTANT: These are TEMPORARY INTERMEDIATE FEATURES\n",
    "# ============================================================================\n",
    "# These features are created from post-match statistics (HS, AS, HST, etc.)\n",
    "# that are only known AFTER a match is played. They are NOT used directly\n",
    "# in the final model.\n",
    "#\n",
    "# PURPOSE: These intermediate features are used to calculate:\n",
    "# 1. Moving averages (MA5) for extended dataset features\n",
    "# 2. Seasonal patterns and team statistics\n",
    "#\n",
    "# These raw features will be REMOVED in Section 4.6 before model training.\n",
    "# Only their aggregated historical versions (MA5, seasonal stats) will remain,\n",
    "# which ARE valid predictors as they represent past performance.\n",
    "# ============================================================================\n",
    "\n",
    "# Basic engineered features (TEMPORARY - for intermediate calculations only)\n",
    "all_matches['home_shot_accuracy'] = all_matches['HST'] / (all_matches['HS'] + 0.001)\n",
    "all_matches['away_shot_accuracy'] = all_matches['AST'] / (all_matches['AS'] + 0.001)\n",
    "all_matches['total_shots'] = all_matches['HS'] + all_matches['AS']\n",
    "all_matches['total_shots_on_target'] = all_matches['HST'] + all_matches['AST']\n",
    "\n",
    "# 2. Attacking vs Defensive balance (TEMPORARY)\n",
    "all_matches['shot_dominance'] = (all_matches['HS'] - all_matches['AS']) / (all_matches['HS'] + all_matches['AS'] + 0.001)\n",
    "all_matches['corner_dominance'] = (all_matches['HC'] - all_matches['AC']) / (all_matches['HC'] + all_matches['AC'] + 0.001)\n",
    "\n",
    "# 3. Game intensity features (TEMPORARY)\n",
    "all_matches['total_fouls'] = all_matches['HF'] + all_matches['AF']\n",
    "all_matches['total_cards'] = all_matches['HY'] + all_matches['AY'] + all_matches['HR'] + all_matches['AR']\n",
    "all_matches['card_intensity'] = all_matches['total_cards'] / (all_matches['total_fouls'] + 0.001)\n",
    "\n",
    "# 4. Half-time patterns (TEMPORARY)\n",
    "all_matches['ht_total_goals'] = all_matches['HTHG'] + all_matches['HTAG']\n",
    "all_matches['second_half_goals'] = all_matches['total_goals'] - all_matches['ht_total_goals']\n",
    "\n",
    "# 5. League tier (can be used directly - known before match)\n",
    "all_matches['league_tier'] = all_matches['Div'].str[-1].astype(int)\n",
    "\n",
    "# 6. Season timing features (can be used directly - known before match)\n",
    "all_matches['month'] = all_matches['Date'].dt.month\n",
    "all_matches['is_weekend'] = all_matches['Date'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "\n",
    "print(\"Basic engineered features created (note: post-match stats are temporary).\")\n",
    "print(\"These will be used to calculate historical aggregates, then removed before modeling.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Base feature df engineering\n",
    "\n",
    "Start with core + basic engineered features as the foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with core and basic engineered features\n",
    "df_basic = all_matches[['Div', 'Season', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR',\n",
    "                   'total_goals', 'league_tier', 'month', 'is_weekend', \"over_2_5\"]].copy()\n",
    "# by date for time-based features\n",
    "df_basic = df_basic.sort_values(['Div', 'Date']).reset_index(drop=True)\n",
    "print(f\"Columns: {df_basic.columns.tolist()}\")\n",
    "df_basic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Days since last match\n",
    "\n",
    "\n",
    "**Calculate days since last match for both home and away teams.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days since last match for each team\n",
    "df_basic['home_days_since_last'] = np.nan\n",
    "df_basic['away_days_since_last'] = np.nan\n",
    "for team in df_basic['HomeTeam'].unique():\n",
    "    home_mask = df_basic['HomeTeam'] == team\n",
    "    away_mask = df_basic['AwayTeam'] == team\n",
    "    team_matches = df_basic[home_mask | away_mask].sort_values('Date')\n",
    "    #days between matches\n",
    "    team_matches['days_diff'] = team_matches['Date'].diff().dt.days\n",
    "    # to home/away columns\n",
    "    for idx, row in team_matches.iterrows():\n",
    "        if df_basic.loc[idx, 'HomeTeam'] == team:\n",
    "            df_basic.loc[idx, 'home_days_since_last'] = row['days_diff']\n",
    "        else:\n",
    "            df_basic.loc[idx, 'away_days_since_last'] = row['days_diff']\n",
    "# first matches filling with median\n",
    "df_basic['home_days_since_last'].fillna(df_basic['home_days_since_last'].median(), inplace=True)\n",
    "df_basic['away_days_since_last'].fillna(df_basic['away_days_since_last'].median(), inplace=True)\n",
    "\n",
    "print(f\"Home days since last - mean: {df_basic['home_days_since_last'].mean():.1f}, median: {df_basic['home_days_since_last'].median():.1f}\")\n",
    "print(f\"Away days since last - mean: {df_basic['away_days_since_last'].mean():.1f}, median: {df_basic['away_days_since_last'].median():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Everything seems ok, the usual pause is one week (expected), but they are visible pauses between individual seasons.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 5-match Moving Averages\n",
    "\n",
    "**Calculate 5-match moving averages for goals scored and conceded.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basic['home_goals_ma5'] = np.nan\n",
    "df_basic['home_conceded_ma5'] = np.nan\n",
    "df_basic['away_goals_ma5'] = np.nan\n",
    "df_basic['away_conceded_ma5'] = np.nan\n",
    "\n",
    "for team in df_basic['HomeTeam'].unique():\n",
    "\n",
    "    # Home matches\n",
    "    home_mask = df_basic['HomeTeam'] == team\n",
    "    home_dates = df_basic[home_mask].sort_values('Date').index\n",
    "    for i, idx in enumerate(home_dates):\n",
    "        if i >= 5:\n",
    "            last_5_home = df_basic.loc[home_dates[i-5:i]]\n",
    "            df_basic.loc[idx, 'home_goals_ma5'] = last_5_home['FTHG'].mean()\n",
    "            df_basic.loc[idx, 'home_conceded_ma5'] = last_5_home['FTAG'].mean()\n",
    "\n",
    "    # Away matches\n",
    "    away_mask = df_basic['AwayTeam'] == team\n",
    "    away_dates = df_basic[away_mask].sort_values('Date').index\n",
    "\n",
    "    for i, idx in enumerate(away_dates):\n",
    "        if i >= 5:\n",
    "            last_5_away = df_basic.loc[away_dates[i-5:i]]\n",
    "            df_basic.loc[idx, 'away_goals_ma5'] = last_5_away['FTAG'].mean()\n",
    "            df_basic.loc[idx, 'away_conceded_ma5'] = last_5_away['FTHG'].mean()\n",
    "\n",
    "# NaN values remain for teams' first 5 matches - will be handled by model\n",
    "# or imputed during train/test split using only training data\n",
    "print(f\"\\nMA5 Statistics (NaN preserved for early-season matches):\")\n",
    "print(f\"home_goals_ma5    - mean: {df_basic['home_goals_ma5'].mean():.2f}, missing: {df_basic['home_goals_ma5'].isna().sum()}\")\n",
    "print(f\"away_goals_ma5    - mean: {df_basic['away_goals_ma5'].mean():.2f}, missing: {df_basic['away_goals_ma5'].isna().sum()}\")\n",
    "print(f\"home_conceded_ma5 - mean: {df_basic['home_conceded_ma5'].mean():.2f}, missing: {df_basic['home_conceded_ma5'].isna().sum()}\")\n",
    "print(f\"away_conceded_ma5 - mean: {df_basic['away_conceded_ma5'].mean():.2f}, missing: {df_basic['away_conceded_ma5'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Fixed: Data leakage removed**\n",
    "- NaN values for early-season matches (first 5 games) are now preserved\n",
    "- No longer filling with overall mean (which would use future data)\n",
    "- Tree-based models can handle NaN values natively\n",
    "- Alternative: Impute during train/test split using only training data statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Promoted/Demoted\n",
    "\n",
    "**Detect teams that changed leagues between seasons using the Season column (e.g., 2019/2020 → 2020/2021).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# promotion/demotion flags\n",
    "df_basic['home_promoted'] = 0\n",
    "df_basic['home_demoted'] = 0\n",
    "df_basic['away_promoted'] = 0\n",
    "df_basic['away_demoted'] = 0\n",
    "\n",
    "# For each team, check if they changed tier between seasons\n",
    "for team in df_basic['HomeTeam'].unique():\n",
    "    team_data = df_basic[(df_basic['HomeTeam'] == team) | (df_basic['AwayTeam'] == team)].sort_values('Date')\n",
    "\n",
    "    # Grouping by season and get the league tier for each season\n",
    "    season_tiers = team_data.groupby('Season')['league_tier'].first()\n",
    "\n",
    "    for i in range(1, len(season_tiers)):\n",
    "        season = season_tiers.index[i]\n",
    "        prev_tier = season_tiers.iloc[i-1]\n",
    "        curr_tier = season_tiers.iloc[i]\n",
    "\n",
    "        if curr_tier < prev_tier:  # Lower tier number = higher division\n",
    "            promoted = 1\n",
    "            demoted = 0\n",
    "        elif curr_tier > prev_tier:\n",
    "            promoted = 0\n",
    "            demoted = 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        season_mask = (df_basic['Season'] == season)\n",
    "        home_mask = season_mask & (df_basic['HomeTeam'] == team)\n",
    "        away_mask = season_mask & (df_basic['AwayTeam'] == team)\n",
    "\n",
    "        df_basic.loc[home_mask, 'home_promoted'] = promoted\n",
    "        df_basic.loc[home_mask, 'home_demoted'] = demoted\n",
    "        df_basic.loc[away_mask, 'away_promoted'] = promoted\n",
    "        df_basic.loc[away_mask, 'away_demoted'] = demoted\n",
    "\n",
    "print(f\"Home teams promoted: {df_basic['home_promoted'].sum()}\")\n",
    "print(f\"Home teams demoted: {df_basic['home_demoted'].sum()}\")\n",
    "print(f\"Away teams promoted: {df_basic['away_promoted'].sum()}\")\n",
    "print(f\"Away teams demoted: {df_basic['away_demoted'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Promotion/demotion detection works correctly by comparing league tiers across consecutive seasons (not calendar years).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4 Historical Season Positions & Goal Patterns\n",
    "\n",
    "**Calculate team standings and goal-scoring patterns with round-based tracking and home/away splits.**\n",
    "\n",
    "**Methodology:**\n",
    "- **Past seasons:** Use final end-of-season standings (complete data)\n",
    "- **Current season:** Use standings after last completed round (prevents data leakage)\n",
    "  - Round = all teams played same number of matches\n",
    "  - Match uses standings from previous completed round only\n",
    "  - Example: If Team A has played 5 matches but round 5 not complete → use round 4 standings\n",
    "- **Position features:** Overall league position only (no home/away split)\n",
    "- **Goal pattern features:** Track separately for overall, home-only, and away-only\n",
    "  - Home team gets: Overall stats + Home-specific stats\n",
    "  - Away team gets: Overall stats + Away-specific stats\n",
    "  - Both absolute counts AND percentages (counts show sample size/reliability)\n",
    "- Use percentiles (0-100) for cross-league comparability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate league standings: final for past seasons, round-by-round for current season\n",
    "def calculate_standings_with_rounds(df):\n",
    "    \"\"\"\n",
    "    Calculate standings with home/away splits:\n",
    "    - Position: Overall only\n",
    "    - Goal patterns: Overall + Home-specific + Away-specific (both counts and percentages)\n",
    "    \"\"\"\n",
    "    standings_list = []\n",
    "    match_to_round = {}\n",
    "\n",
    "    for (season, div), group in df.groupby(['Season', 'Div']):\n",
    "        group = group.sort_values('Date').reset_index(drop=True)\n",
    "        \n",
    "        # Initialize tracking for overall, home, away\n",
    "        teams = set(group['HomeTeam'].unique()) | set(group['AwayTeam'].unique())\n",
    "        team_stats_overall = {team: {'points': 0, 'gf': 0, 'ga': 0, 'matches': 0} for team in teams}\n",
    "        team_stats_home = {team: {'gf': 0, 'ga': 0, 'matches': 0} for team in teams}\n",
    "        team_stats_away = {team: {'gf': 0, 'ga': 0, 'matches': 0} for team in teams}\n",
    "        \n",
    "        # Goal patterns: overall, home-only, away-only\n",
    "        patterns_overall = {team: {f'{p}_{t}': 0 for p in ['sc', 'co', 'to'] for t in [2, 3]} for team in teams}\n",
    "        patterns_home = {team: {f'{p}_{t}': 0 for p in ['sc', 'co', 'to'] for t in [2, 3]} for team in teams}\n",
    "        patterns_away = {team: {f'{p}_{t}': 0 for p in ['sc', 'co', 'to'] for t in [2, 3]} for team in teams}\n",
    "        \n",
    "        current_round = None\n",
    "\n",
    "        for _, match in group.iterrows():\n",
    "            ht, at = match['HomeTeam'], match['AwayTeam']\n",
    "            hg, ag = match['FTHG'], match['FTAG']\n",
    "            match_key = (season, div, match['Date'], ht, at)\n",
    "            \n",
    "            # Store BEFORE match\n",
    "            if current_round is not None:\n",
    "                match_to_round[match_key] = current_round.copy()\n",
    "            \n",
    "            # Update overall stats\n",
    "            team_stats_overall[ht]['gf'] += hg\n",
    "            team_stats_overall[ht]['ga'] += ag\n",
    "            team_stats_overall[ht]['matches'] += 1\n",
    "            team_stats_overall[at]['gf'] += ag\n",
    "            team_stats_overall[at]['ga'] += hg\n",
    "            team_stats_overall[at]['matches'] += 1\n",
    "            \n",
    "            # Update home/away specific stats\n",
    "            team_stats_home[ht]['gf'] += hg\n",
    "            team_stats_home[ht]['ga'] += ag\n",
    "            team_stats_home[ht]['matches'] += 1\n",
    "            team_stats_away[at]['gf'] += ag\n",
    "            team_stats_away[at]['ga'] += hg\n",
    "            team_stats_away[at]['matches'] += 1\n",
    "            \n",
    "            # Update goal patterns - OVERALL\n",
    "            for t in [2, 3]:\n",
    "                if hg >= t: \n",
    "                    patterns_overall[ht][f'sc_{t}'] += 1\n",
    "                    patterns_home[ht][f'sc_{t}'] += 1\n",
    "                if ag >= t: \n",
    "                    patterns_overall[ht][f'co_{t}'] += 1\n",
    "                    patterns_home[ht][f'co_{t}'] += 1\n",
    "                    patterns_overall[at][f'sc_{t}'] += 1\n",
    "                    patterns_away[at][f'sc_{t}'] += 1\n",
    "                if hg + ag >= t: \n",
    "                    patterns_overall[ht][f'to_{t}'] += 1\n",
    "                    patterns_overall[at][f'to_{t}'] += 1\n",
    "                    patterns_home[ht][f'to_{t}'] += 1\n",
    "                    patterns_away[at][f'to_{t}'] += 1\n",
    "                if hg >= t:\n",
    "                    patterns_overall[at][f'co_{t}'] += 1\n",
    "                    patterns_away[at][f'co_{t}'] += 1\n",
    "            \n",
    "            # Points\n",
    "            if match['FTR'] == 'H': team_stats_overall[ht]['points'] += 3\n",
    "            elif match['FTR'] == 'A': team_stats_overall[at]['points'] += 3\n",
    "            else: \n",
    "                team_stats_overall[ht]['points'] += 1\n",
    "                team_stats_overall[at]['points'] += 1\n",
    "            \n",
    "            # Check round completion\n",
    "            if len(set(s['matches'] for s in team_stats_overall.values())) == 1:\n",
    "                rows = []\n",
    "                for team in teams:\n",
    "                    s_o = team_stats_overall[team]\n",
    "                    s_h = team_stats_home[team]\n",
    "                    s_a = team_stats_away[team]\n",
    "                    \n",
    "                    row = {\n",
    "                        'Season': season, 'Div': div, 'Team': team,\n",
    "                        'Points': s_o['points'], 'Matches': s_o['matches'],\n",
    "                        'Goals_For': s_o['gf'], 'Goals_Against': s_o['ga']\n",
    "                    }\n",
    "                    \n",
    "                    # Overall goal patterns (count + pct)\n",
    "                    for t in [2, 3]:\n",
    "                        for p, full in [('sc', 'scored'), ('co', 'conceded'), ('to', 'total')]:\n",
    "                            cnt = patterns_overall[team][f'{p}_{t}']\n",
    "                            row[f'{full}_{t}plus_count'] = cnt\n",
    "                            row[f'{full}_{t}plus_pct'] = round(cnt / s_o['matches'] * 100, 1) if s_o['matches'] > 0 else 0\n",
    "                    \n",
    "                    # Home-specific goal patterns\n",
    "                    for t in [2, 3]:\n",
    "                        for p, full in [('sc', 'scored'), ('co', 'conceded'), ('to', 'total')]:\n",
    "                            cnt = patterns_home[team][f'{p}_{t}']\n",
    "                            row[f'home_{full}_{t}plus_count'] = cnt\n",
    "                            row[f'home_{full}_{t}plus_pct'] = round(cnt / s_h['matches'] * 100, 1) if s_h['matches'] > 0 else 0\n",
    "                    \n",
    "                    # Away-specific goal patterns\n",
    "                    for t in [2, 3]:\n",
    "                        for p, full in [('sc', 'scored'), ('co', 'conceded'), ('to', 'total')]:\n",
    "                            cnt = patterns_away[team][f'{p}_{t}']\n",
    "                            row[f'away_{full}_{t}plus_count'] = cnt\n",
    "                            row[f'away_{full}_{t}plus_pct'] = round(cnt / s_a['matches'] * 100, 1) if s_a['matches'] > 0 else 0\n",
    "                    \n",
    "                    rows.append(row)\n",
    "                \n",
    "                current_round = pd.DataFrame(rows).sort_values('Points', ascending=False)\n",
    "                current_round['Position'] = range(1, len(current_round) + 1)\n",
    "        \n",
    "        if current_round is not None:\n",
    "            standings_list.append(current_round)\n",
    "    \n",
    "    final = pd.concat(standings_list, ignore_index=True) if standings_list else pd.DataFrame()\n",
    "    return final, match_to_round\n",
    "\n",
    "# Calculate standings\n",
    "df_season_standings, match_round_map = calculate_standings_with_rounds(df_basic)\n",
    "print(f\"Season standings: {len(df_season_standings)} team-season records\")\n",
    "print(f\"Round-based mappings: {len(match_round_map)} matches with historical standings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create historical position features with lookback logic\n",
    "season_list = sorted(df_basic['Season'].unique())\n",
    "season_to_order = {season: idx for idx, season in enumerate(season_list)}\n",
    "df_basic['season_order'] = df_basic['Season'].map(season_to_order)\n",
    "\n",
    "# Extract season years and create position lookup\n",
    "def extract_season_year(season_str):\n",
    "    return int(season_str.split('/')[0])\n",
    "\n",
    "df_season_standings['season_year'] = df_season_standings['Season'].apply(extract_season_year)\n",
    "unique_season_years = sorted(df_season_standings['season_year'].unique())\n",
    "\n",
    "# Add percentile rankings for cross-league comparability\n",
    "df_season_standings['league_size'] = df_season_standings.groupby(['Season', 'Div'])['Position'].transform('max')\n",
    "df_season_standings['Position_Percentile'] = (\n",
    "    (df_season_standings['league_size'] - df_season_standings['Position'] + 1) /\n",
    "    df_season_standings['league_size'] * 100\n",
    ").round(2)\n",
    "percentile_lookup = df_season_standings.set_index(['Season', 'Div', 'Team'])['Position_Percentile'].to_dict()\n",
    "\n",
    "# Create columns for percentile positions only (not raw positions)\n",
    "for year in unique_season_years:\n",
    "    df_basic[f'home_position_pct_{year}'] = np.nan\n",
    "    df_basic[f'away_position_pct_{year}'] = np.nan\n",
    "\n",
    "# Populate historical features\n",
    "for idx, row in df_basic.iterrows():\n",
    "    current_season_order = row['season_order']\n",
    "    current_season = row['Season']\n",
    "    match_key = (row['Season'], row['Div'], row['Date'], row['HomeTeam'], row['AwayTeam'])\n",
    "\n",
    "    for year in unique_season_years:\n",
    "        target_season = next((s for s in season_list if extract_season_year(s) == year), None)\n",
    "        if not target_season:\n",
    "            continue\n",
    "        \n",
    "        target_season_order = season_to_order[target_season]\n",
    "\n",
    "        # For past seasons: use final standings\n",
    "        if target_season_order < current_season_order:\n",
    "            for team_type, team in [('home', row['HomeTeam']), ('away', row['AwayTeam'])]:\n",
    "                key = next(\n",
    "                    ((target_season, div, team) for div in df_season_standings['Div'].unique()\n",
    "                     if (target_season, div, team) in percentile_lookup),\n",
    "                    None\n",
    "                )\n",
    "                if key:\n",
    "                    df_basic.loc[idx, f'{team_type}_position_pct_{year}'] = percentile_lookup[key]\n",
    "\n",
    "        # For current season (same season as match): use round-based standings\n",
    "        elif target_season == current_season and match_key in match_round_map:\n",
    "            round_standings = match_round_map[match_key]\n",
    "            for team_type, team in [('home', row['HomeTeam']), ('away', row['AwayTeam'])]:\n",
    "                team_row = round_standings[round_standings['Team'] == team]\n",
    "                if not team_row.empty:\n",
    "                    position = team_row['Position'].iloc[0]\n",
    "                    league_size = len(round_standings)\n",
    "                    percentile = ((league_size - position + 1) / league_size * 100)\n",
    "                    df_basic.loc[idx, f'{team_type}_position_pct_{year}'] = round(percentile, 2)\n",
    "\n",
    "position_cols = [col for col in df_basic.columns if 'position_pct_' in col]\n",
    "print(f\"Historical position features: {len(position_cols)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add historical goal-scoring pattern features with home/away context\n",
    "# Create lookups from final standings\n",
    "goal_stat_lookups = {}\n",
    "for threshold in [2, 3]:\n",
    "    for prefix in ['scored', 'conceded', 'total']:\n",
    "        # Overall stats\n",
    "        for suffix in ['count', 'pct']:\n",
    "            col = f'{prefix}_{threshold}plus_{suffix}'\n",
    "            goal_stat_lookups[col] = df_season_standings.set_index(['Season', 'Div', 'Team'])[col].to_dict()\n",
    "        # Home-specific stats\n",
    "        for suffix in ['count', 'pct']:\n",
    "            col = f'home_{prefix}_{threshold}plus_{suffix}'\n",
    "            goal_stat_lookups[col] = df_season_standings.set_index(['Season', 'Div', 'Team'])[col].to_dict()\n",
    "        # Away-specific stats\n",
    "        for suffix in ['count', 'pct']:\n",
    "            col = f'away_{prefix}_{threshold}plus_{suffix}'\n",
    "            goal_stat_lookups[col] = df_season_standings.set_index(['Season', 'Div', 'Team'])[col].to_dict()\n",
    "\n",
    "# Create columns: home team gets overall + home-specific, away team gets overall + away-specific\n",
    "for year in unique_season_years:\n",
    "    for threshold in [2, 3]:\n",
    "        for prefix in ['scored', 'conceded', 'total']:\n",
    "            # Home team: overall + home-specific\n",
    "            df_basic[f'home_{prefix}_{threshold}plus_count_{year}'] = np.nan\n",
    "            df_basic[f'home_{prefix}_{threshold}plus_pct_{year}'] = np.nan\n",
    "            df_basic[f'home_home_{prefix}_{threshold}plus_count_{year}'] = np.nan\n",
    "            df_basic[f'home_home_{prefix}_{threshold}plus_pct_{year}'] = np.nan\n",
    "            # Away team: overall + away-specific\n",
    "            df_basic[f'away_{prefix}_{threshold}plus_count_{year}'] = np.nan\n",
    "            df_basic[f'away_{prefix}_{threshold}plus_pct_{year}'] = np.nan\n",
    "            df_basic[f'away_away_{prefix}_{threshold}plus_count_{year}'] = np.nan\n",
    "            df_basic[f'away_away_{prefix}_{threshold}plus_pct_{year}'] = np.nan\n",
    "\n",
    "# Populate goal statistics\n",
    "for idx, row in df_basic.iterrows():\n",
    "    current_season_order = row['season_order']\n",
    "    current_season = row['Season']\n",
    "    match_key = (row['Season'], row['Div'], row['Date'], row['HomeTeam'], row['AwayTeam'])\n",
    "\n",
    "    for year in unique_season_years:\n",
    "        target_season = next((s for s in season_list if extract_season_year(s) == year), None)\n",
    "        if not target_season:\n",
    "            continue\n",
    "\n",
    "        # For past seasons: use final standings\n",
    "        if season_to_order[target_season] < current_season_order:\n",
    "            # Home team\n",
    "            key_home = next(((target_season, div, row['HomeTeam']) for div in df_season_standings['Div'].unique()\n",
    "                             if (target_season, div, row['HomeTeam']) in percentile_lookup), None)\n",
    "            if key_home:\n",
    "                for threshold in [2, 3]:\n",
    "                    for prefix in ['scored', 'conceded', 'total']:\n",
    "                        # Overall stats\n",
    "                        for suffix in ['count', 'pct']:\n",
    "                            col = f'{prefix}_{threshold}plus_{suffix}'\n",
    "                            val = goal_stat_lookups[col].get(key_home)\n",
    "                            if val is not None:\n",
    "                                df_basic.loc[idx, f'home_{col}_{year}'] = val\n",
    "                        # Home-specific stats\n",
    "                        for suffix in ['count', 'pct']:\n",
    "                            col = f'home_{prefix}_{threshold}plus_{suffix}'\n",
    "                            val = goal_stat_lookups[col].get(key_home)\n",
    "                            if val is not None:\n",
    "                                df_basic.loc[idx, f'home_{col}_{year}'] = val\n",
    "            \n",
    "            # Away team\n",
    "            key_away = next(((target_season, div, row['AwayTeam']) for div in df_season_standings['Div'].unique()\n",
    "                             if (target_season, div, row['AwayTeam']) in percentile_lookup), None)\n",
    "            if key_away:\n",
    "                for threshold in [2, 3]:\n",
    "                    for prefix in ['scored', 'conceded', 'total']:\n",
    "                        # Overall stats\n",
    "                        for suffix in ['count', 'pct']:\n",
    "                            col = f'{prefix}_{threshold}plus_{suffix}'\n",
    "                            val = goal_stat_lookups[col].get(key_away)\n",
    "                            if val is not None:\n",
    "                                df_basic.loc[idx, f'away_{col}_{year}'] = val\n",
    "                        # Away-specific stats\n",
    "                        for suffix in ['count', 'pct']:\n",
    "                            col = f'away_{prefix}_{threshold}plus_{suffix}'\n",
    "                            val = goal_stat_lookups[col].get(key_away)\n",
    "                            if val is not None:\n",
    "                                df_basic.loc[idx, f'away_{col}_{year}'] = val\n",
    "\n",
    "        # For current season: use round-based standings\n",
    "        elif target_season == current_season and match_key in match_round_map:\n",
    "            round_standings = match_round_map[match_key]\n",
    "            \n",
    "            # Home team\n",
    "            home_row = round_standings[round_standings['Team'] == row['HomeTeam']]\n",
    "            if not home_row.empty:\n",
    "                for threshold in [2, 3]:\n",
    "                    for prefix in ['scored', 'conceded', 'total']:\n",
    "                        # Overall stats\n",
    "                        for suffix in ['count', 'pct']:\n",
    "                            col = f'{prefix}_{threshold}plus_{suffix}'\n",
    "                            if col in home_row.columns:\n",
    "                                df_basic.loc[idx, f'home_{col}_{year}'] = home_row[col].iloc[0]\n",
    "                        # Home-specific stats\n",
    "                        for suffix in ['count', 'pct']:\n",
    "                            col = f'home_{prefix}_{threshold}plus_{suffix}'\n",
    "                            if col in home_row.columns:\n",
    "                                df_basic.loc[idx, f'home_{col}_{year}'] = home_row[col].iloc[0]\n",
    "            \n",
    "            # Away team\n",
    "            away_row = round_standings[round_standings['Team'] == row['AwayTeam']]\n",
    "            if not away_row.empty:\n",
    "                for threshold in [2, 3]:\n",
    "                    for prefix in ['scored', 'conceded', 'total']:\n",
    "                        # Overall stats\n",
    "                        for suffix in ['count', 'pct']:\n",
    "                            col = f'{prefix}_{threshold}plus_{suffix}'\n",
    "                            if col in away_row.columns:\n",
    "                                df_basic.loc[idx, f'away_{col}_{year}'] = away_row[col].iloc[0]\n",
    "                        # Away-specific stats\n",
    "                        for suffix in ['count', 'pct']:\n",
    "                            col = f'away_{prefix}_{threshold}plus_{suffix}'\n",
    "                            if col in away_row.columns:\n",
    "                                df_basic.loc[idx, f'away_{col}_{year}'] = away_row[col].iloc[0]\n",
    "\n",
    "goal_stat_cols = [col for col in df_basic.columns if any(f'{p}_{t}plus' in col for p in ['scored', 'conceded', 'total'] for t in [2, 3])]\n",
    "print(f\"Historical goal statistics: {len(goal_stat_cols)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of Section 4.3.4:**\n",
    "\n",
    "We've created comprehensive historical performance features with **round-based tracking and home/away context**:\n",
    "\n",
    "1. **Season Standings** (`df_season_standings`): Team-season records with home/away splits\n",
    "   - **Past seasons:** Final standings (complete data)\n",
    "   - **Current season:** Round-by-round standings (last completed round before each match)\n",
    "   - Metrics: Points, Position, Goals, Goal patterns with home/away context\n",
    "\n",
    "2. **Historical Position Features** (13 columns added to `df_basic`):\n",
    "   - **Overall position only** (no home/away split - league position is always overall)\n",
    "   - **Percentile rankings** (0-100): Normalized across league sizes\n",
    "   - Format: `home_position_pct_YYYY`, `away_position_pct_YYYY` for 6 seasons\n",
    "   - Higher percentile = better position (95th = top of table)\n",
    "   - Uses round-based data for current season to maximize data usage\n",
    "\n",
    "3. **Historical Goal Pattern Features** (288 columns added to `df_basic`):\n",
    "   - **Context-aware:** Home team gets overall + home-specific, away team gets overall + away-specific\n",
    "   - For each context: 6 statistics × 2 metrics (count & percentage)\n",
    "     - `scored_2plus/3plus`: Matches where team scored 2+/3+ goals\n",
    "     - `conceded_2plus/3plus`: Matches where team conceded 2+/3+ goals\n",
    "     - `total_2plus/3plus`: Matches with 2+/3+ total goals\n",
    "   - **Both counts AND percentages** included:\n",
    "     - Counts show sample size (reliability indicator)\n",
    "     - Percentages show the pattern/trend\n",
    "     - Example: 80% from 4 matches vs 50% from 1 match - model learns reliability\n",
    "   - Format: `home_home_scored_3plus_pct_2024` = home team's home-only scoring %\n",
    "   - Format: `away_away_conceded_2plus_count_2024` = away team's away-only conceding count\n",
    "\n",
    "**Key Design Choices:**\n",
    "- **Home/Away Context:** Teams perform differently at home vs away - model gets both perspectives\n",
    "- **Counts + Percentages:** Dual metrics allow model to weight by sample size\n",
    "- **Lookback logic:** Prevents data leakage (features only reference past seasons)\n",
    "- **Percentiles:** Enable fair comparison between different league sizes\n",
    "- **2+ and 3+ thresholds:** Most relevant for Over/Under 2.5 target\n",
    "- **Current season handling:** Uses round-based data to maximize coverage (~95%)\n",
    "\n",
    "**Total features added: 301** (13 position percentiles + 288 goal patterns)\n",
    "\n",
    "**Expected Impact:** Context-aware goal patterns (home team's home performance vs away team's away performance) should provide stronger predictive signals than overall statistics alone. Including both counts and percentages helps model assess reliability of patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.5 Additional Derived Features\n",
    "\n",
    "**Create additional features from existing data that might improve Over/Under 2.5 prediction:**\n",
    "\n",
    "**Team strength differential features:**\n",
    "- Position percentile difference between home and away team (quality gap, normalized across leagues)\n",
    "- Combined attacking/defensive strength indicators (blend recent form + historical patterns)\n",
    "\n",
    "**Form-based features:**\n",
    "- Recent form trend (improving vs declining over last 10 matches)\n",
    "\n",
    "**Contextual features:**\n",
    "- Season progress (early, mid, late season effects)\n",
    "- Days rest advantage (difference in rest days between teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Position difference features (most recent season available for both teams)\n",
    "df_basic['position_pct_diff'] = np.nan\n",
    "\n",
    "for idx, row in df_basic.iterrows():\n",
    "    # Find most recent season where both teams have position data\n",
    "    for year in reversed(unique_season_years):\n",
    "        home_pct_col = f'home_position_pct_{year}'\n",
    "        away_pct_col = f'away_position_pct_{year}'\n",
    "\n",
    "        if (pd.notna(df_basic.loc[idx, home_pct_col]) and\n",
    "            pd.notna(df_basic.loc[idx, away_pct_col])):\n",
    "            # Positive = home team has better position (higher percentile)\n",
    "            df_basic.loc[idx, 'position_pct_diff'] = df_basic.loc[idx, home_pct_col] - df_basic.loc[idx, away_pct_col]\n",
    "            break\n",
    "\n",
    "print(f\" Position percentile difference feature created\")\n",
    "print(f\"Coverage: {(df_basic['position_pct_diff'].notna().sum() / len(df_basic) * 100):.1f}%\")\n",
    "\n",
    "# 2. Combined attacking strength using context-aware stats\n",
    "# Home team uses home-specific stats, away team uses away-specific stats\n",
    "df_basic['combined_attack_strength'] = np.nan\n",
    "df_basic['combined_defense_weakness'] = np.nan\n",
    "\n",
    "for idx, row in df_basic.iterrows():\n",
    "    # Get most recent goal statistics (context-aware: home-specific for home team, away-specific for away)\n",
    "    for year in reversed(unique_season_years):\n",
    "        # Home team: use home-specific scoring\n",
    "        home_scored = df_basic.loc[idx, f'home_home_scored_3plus_pct_{year}']\n",
    "        home_conceded = df_basic.loc[idx, f'home_home_conceded_3plus_pct_{year}']\n",
    "        # Away team: use away-specific scoring\n",
    "        away_scored = df_basic.loc[idx, f'away_away_scored_3plus_pct_{year}']\n",
    "        away_conceded = df_basic.loc[idx, f'away_away_conceded_3plus_pct_{year}']\n",
    "\n",
    "        if pd.notna(home_scored) and pd.notna(away_scored):\n",
    "            # Combine recent MA with historical patterns\n",
    "            home_recent = df_basic.loc[idx, 'home_goals_ma5']\n",
    "            away_recent = df_basic.loc[idx, 'away_goals_ma5']\n",
    "\n",
    "            # Weighted average: 60% recent form, 40% historical context-specific pattern\n",
    "            df_basic.loc[idx, 'combined_attack_strength'] = (\n",
    "                0.6 * (home_recent + away_recent) +\n",
    "                0.4 * ((home_scored + away_scored) / 20)  # Normalize percentage to goals scale\n",
    "            )\n",
    "            df_basic.loc[idx, 'combined_defense_weakness'] = (\n",
    "                0.6 * (df_basic.loc[idx, 'home_conceded_ma5'] + df_basic.loc[idx, 'away_conceded_ma5']) +\n",
    "                0.4 * ((home_conceded + away_conceded) / 20)\n",
    "            )\n",
    "            break\n",
    "\n",
    "print(f\" Combined strength features created (using home/away context)\")\n",
    "print(f\"Coverage: {(df_basic['combined_attack_strength'].notna().sum() / len(df_basic) * 100):.1f}%\")\n",
    "\n",
    "# 3. Form trend (improving vs declining)\n",
    "df_basic['home_form_trend'] = np.nan\n",
    "df_basic['away_form_trend'] = np.nan\n",
    "\n",
    "for team in df_basic['HomeTeam'].unique():\n",
    "    # Home matches\n",
    "    home_mask = df_basic['HomeTeam'] == team\n",
    "    home_dates = df_basic[home_mask].sort_values('Date').index\n",
    "    for i, idx in enumerate(home_dates):\n",
    "        if i >= 10:  # Need at least 10 matches\n",
    "            last_5 = df_basic.loc[home_dates[i-5:i], 'FTHG'].mean()\n",
    "            prev_5 = df_basic.loc[home_dates[i-10:i-5], 'FTHG'].mean()\n",
    "            df_basic.loc[idx, 'home_form_trend'] = last_5 - prev_5  # Positive = improving\n",
    "\n",
    "    # Away matches\n",
    "    away_mask = df_basic['AwayTeam'] == team\n",
    "    away_dates = df_basic[away_mask].sort_values('Date').index\n",
    "    for i, idx in enumerate(away_dates):\n",
    "        if i >= 10:\n",
    "            last_5 = df_basic.loc[away_dates[i-5:i], 'FTAG'].mean()\n",
    "            prev_5 = df_basic.loc[away_dates[i-10:i-5], 'FTAG'].mean()\n",
    "            df_basic.loc[idx, 'away_form_trend'] = last_5 - prev_5\n",
    "\n",
    "# Fill NaN with 0 (no trend info = assume stable)\n",
    "df_basic['home_form_trend'].fillna(0, inplace=True)\n",
    "df_basic['away_form_trend'].fillna(0, inplace=True)\n",
    "\n",
    "print(f\" Form trend features created\")\n",
    "\n",
    "# 4. Rest days advantage\n",
    "df_basic['rest_days_advantage'] = df_basic['home_days_since_last'] - df_basic['away_days_since_last']\n",
    "\n",
    "print(f\" Rest days advantage created\")\n",
    "\n",
    "# 5. Season progress (match number / expected total matches based on league structure)\n",
    "# NO DATA LEAKAGE: Uses team count (known from first match) and round-robin format\n",
    "# Expected matches = N × (N-1) where N = number of teams in the league\n",
    "df_basic['season_progress'] = np.nan\n",
    "for season_div, group in df_basic.groupby(['Season', 'Div']):\n",
    "    # Count unique teams in this season/division (structural information)\n",
    "    teams_in_league = len(group['HomeTeam'].unique())\n",
    "    # Calculate expected total matches for round-robin (home & away)\n",
    "    expected_total_matches = teams_in_league * (teams_in_league - 1)\n",
    "    \n",
    "    # Sort by date to get chronological order\n",
    "    sorted_indices = group.sort_values('Date').index\n",
    "    \n",
    "    for i, idx in enumerate(sorted_indices):\n",
    "        match_number = i + 1\n",
    "        df_basic.loc[idx, 'season_progress'] = match_number / expected_total_matches\n",
    "\n",
    "print(f\" Season progress feature created (using league structure)\")\n",
    "\n",
    "# Summary\n",
    "new_derived_features = ['position_pct_diff', 'combined_attack_strength',\n",
    "                        'combined_defense_weakness', 'home_form_trend', 'away_form_trend',\n",
    "                        'rest_days_advantage', 'season_progress']\n",
    "print(f\"\nTotal new derived features: {len(new_derived_features)}\")\n",
    "print(f\"Features: {new_derived_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify season_progress fix - check values are reasonable\n",
    "print(\"Season Progress Verification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check range\n",
    "print(f\"\\nSeason progress range: {df_basic['season_progress'].min():.4f} to {df_basic['season_progress'].max():.4f}\")\n",
    "\n",
    "# Check a few examples from different league sizes\n",
    "for season_div in [('2024/2025', 'E0'), ('2024/2025', 'SP1'), ('2024/2025', 'SC0')]:\n",
    "    season, div = season_div\n",
    "    mask = (df_basic['Season'] == season) & (df_basic['Div'] == div)\n",
    "    group = df_basic[mask].copy()\n",
    "    \n",
    "    if len(group) > 0:\n",
    "        teams = len(group['HomeTeam'].unique())\n",
    "        expected = teams * (teams - 1)\n",
    "        progress_values = group.sort_values('Date')['season_progress'].values\n",
    "        \n",
    "        print(f\"\\n{season} {div}:\")\n",
    "        print(f\"Teams: {teams}, Expected matches: {expected}\")\n",
    "        print(f\"Matches so far: {len(group)}\")\n",
    "        print(f\"Progress range: {progress_values.min():.4f} to {progress_values.max():.4f}\")\n",
    "        print(f\"First 3 matches progress: {progress_values[:3]}\")\n",
    "\n",
    "print(\"\nSeason progress values verified - using league structure (no data leakage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Leakage Prevention Summary\n",
    "\n",
    "**Season Progress Feature Fix:**\n",
    "- **Problem**: Previously used `len(group)` which included ALL matches in the season (including future unplayed matches)\n",
    "- **Solution**: Calculate expected matches using league structure: `teams × (teams - 1)`\n",
    "- **Why this is NOT leakage**: \n",
    "  - Number of teams is known from the first match\n",
    "  - League format (round-robin home & away) is predetermined\n",
    "  - Similar to knowing NBA has 82 games or Premier League has 38 rounds\n",
    "  - We're using structural information, not future match results\n",
    "\n",
    "**All Features Verified for Data Leakage:**\n",
    " MA5 features: Use `Date < date` or index slicing `[i-5:i]` to exclude current match  \n",
    " Combined strength: Uses pre-calculated MA5 (which is safe)  \n",
    " Form trend: Uses historical windows `[i-5:i]` and `[i-10:i-5]`  \n",
    " Season progress: Now uses league structure (team count)  \n",
    " Extended stats MA5: Uses `Date < date` filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.6 Feature Importance Testing\n",
    "\n",
    "**Test which features matter for predicting Over/Under 2.5 goals**\n",
    "\n",
    "**Correlation Analysis:**\n",
    "- Quick initial screening of linear relationships\n",
    "- Identifies features with direct linear impact on target\n",
    "- Visualize top correlated features by group (time-based, position, goal patterns)\n",
    "- **Limitation:** Only captures linear relationships, misses non-linear patterns and interactions\n",
    "\n",
    "**Note:** While correlations are weak, features may still be valuable in tree-based models that capture non-linear patterns, thresholds, and feature interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Correlation Analysis\n",
    "\n",
    "**Quick screening of linear relationships with Over/Under 2.5 target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features to test\n",
    "num_features = ['home_days_since_last', 'away_days_since_last',\n",
    "                'home_goals_ma5', 'home_conceded_ma5', 'away_goals_ma5', 'away_conceded_ma5',\n",
    "                'league_tier', 'month']\n",
    "\n",
    "# Add historical position features (from section 4.3.4) - percentiles only\n",
    "position_features = [col for col in df_basic.columns if 'position_pct_' in col]\n",
    "\n",
    "# Add historical goal statistics features (from sections 4.3.4 and 4.3.5)\n",
    "# Includes: overall, home-specific, away-specific stats × counts and percentages\n",
    "goal_stat_features = [col for col in df_basic.columns if any(f'{p}_{t}plus' in col for p in ['scored', 'conceded', 'total'] for t in [2, 3])]\n",
    "\n",
    "all_num_features = num_features + position_features + goal_stat_features\n",
    "\n",
    "# Calculate correlations\n",
    "print(\"=\" * 60)\n",
    "print(\"CORRELATIONS WITH over_2_5 TARGET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. TIME-BASED FEATURES:\")\n",
    "time_corrs = []\n",
    "for feat in num_features:\n",
    "    corr = df_basic[feat].corr(df_basic['over_2_5'])\n",
    "    time_corrs.append((feat, corr))\n",
    "    print(f\"{feat:30s}: {corr:7.4f}\")\n",
    "\n",
    "print(f\"\\n2. HISTORICAL POSITION FEATURES ({len(position_features)} total):\")\n",
    "# Group by season year for cleaner display\n",
    "position_corrs = []\n",
    "for feat in position_features:\n",
    "    corr = df_basic[feat].corr(df_basic['over_2_5'])\n",
    "    position_corrs.append((feat, corr))\n",
    "    print(f\"{feat:30s}: {corr:7.4f}\")\n",
    "\n",
    "# Summary statistics for position features\n",
    "position_corr_values = [abs(c[1]) for c in position_corrs if not pd.isna(c[1])]\n",
    "if position_corr_values:\n",
    "    print(f\"\nPosition features summary:\")\n",
    "    print(f\"Max |correlation|: {max(position_corr_values):.4f}\")\n",
    "    print(f\"Mean |correlation|: {np.mean(position_corr_values):.4f}\")\n",
    "    print(f\"Median |correlation|: {np.median(position_corr_values):.4f}\")\n",
    "\n",
    "print(f\"\\n3. HISTORICAL GOAL PATTERN FEATURES ({len(goal_stat_features)} total):\")\n",
    "goal_stat_corrs = []\n",
    "for feat in goal_stat_features:\n",
    "    corr = df_basic[feat].corr(df_basic['over_2_5'])\n",
    "    goal_stat_corrs.append((feat, corr))\n",
    "    print(f\"{feat:40s}: {corr:7.4f}\")\n",
    "\n",
    "# Summary statistics for goal stat features\n",
    "goal_stat_corr_values = [abs(c[1]) for c in goal_stat_corrs if not pd.isna(c[1])]\n",
    "if goal_stat_corr_values:\n",
    "    print(f\"\nGoal stat features summary:\")\n",
    "    print(f\"Max |correlation|: {max(goal_stat_corr_values):.4f}\")\n",
    "    print(f\"Mean |correlation|: {np.mean(goal_stat_corr_values):.4f}\")\n",
    "    print(f\"Median |correlation|: {np.median(goal_stat_corr_values):.4f}\")\n",
    "    \n",
    "# Show top features by absolute correlation\n",
    "all_corrs = time_corrs + position_corrs + goal_stat_corrs\n",
    "all_corrs_sorted = sorted(all_corrs, key=lambda x: abs(x[1]) if not pd.isna(x[1]) else 0, reverse=True)\n",
    "\n",
    "print(f\"\\n4. TOP 15 FEATURES BY ABSOLUTE CORRELATION:\")\n",
    "for feat, corr in all_corrs_sorted[:15]:\n",
    "    print(f\"{feat:40s}: {corr:7.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 1. Time-based features\n",
    "time_corrs = [df_basic[f].corr(df_basic['over_2_5']) for f in num_features]\n",
    "axes[0].barh(num_features, time_corrs, color='steelblue')\n",
    "axes[0].axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "axes[0].set_xlabel('Correlation with Over 2.5', fontsize=11)\n",
    "axes[0].set_title('Time-Based Features', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Position features - show top 10 by absolute correlation\n",
    "position_corr_df = pd.DataFrame(position_corrs, columns=['feature', 'correlation'])\n",
    "position_corr_df['abs_corr'] = position_corr_df['correlation'].abs()\n",
    "top_position = position_corr_df.nlargest(10, 'abs_corr')\n",
    "axes[1].barh(range(len(top_position)), top_position['correlation'].values, color='coral')\n",
    "axes[1].set_yticks(range(len(top_position)))\n",
    "axes[1].set_yticklabels([f.replace('home_', 'H_').replace('away_', 'A_').replace('position_', 'pos_').replace('_pct', '%') for f in top_position['feature']], fontsize=8)\n",
    "axes[1].axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "axes[1].set_xlabel('Correlation with Over 2.5', fontsize=11)\n",
    "axes[1].set_title(f'Top 10 Position Features (of {len(position_features)})', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Goal stats - show top 10 by absolute correlation\n",
    "goal_corr_df = pd.DataFrame(goal_stat_corrs, columns=['feature', 'correlation'])\n",
    "goal_corr_df['abs_corr'] = goal_corr_df['correlation'].abs()\n",
    "top_goals = goal_corr_df.nlargest(10, 'abs_corr')\n",
    "axes[2].barh(range(len(top_goals)), top_goals['correlation'].values, color='mediumseagreen')\n",
    "axes[2].set_yticks(range(len(top_goals)))\n",
    "axes[2].set_yticklabels([f.replace('home_', 'H_').replace('away_', 'A_').replace('_pct_season_', '_S') for f in top_goals['feature']], fontsize=8)\n",
    "axes[2].axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "axes[2].set_xlabel('Correlation with Over 2.5', fontsize=11)\n",
    "axes[2].set_title(f'Top 10 Goal Pattern Features (of {len(goal_stat_features)})', fontsize=12, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CORRELATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Overall weak correlations suggest non-linear relationships are important.\")\n",
    "print(f\"Tree-based models will likely perform better than linear models.\")\n",
    "print(f\"\\nStrongest absolute correlations:\")\n",
    "all_corrs = pd.concat([\n",
    "    pd.DataFrame({'feature': num_features, 'correlation': time_corrs}),\n",
    "    position_corr_df[['feature', 'correlation']],\n",
    "    goal_corr_df[['feature', 'correlation']]\n",
    "])\n",
    "all_corrs['abs_corr'] = all_corrs['correlation'].abs()\n",
    "top_overall = all_corrs.nlargest(5, 'abs_corr')\n",
    "for _, row in top_overall.iterrows():\n",
    "    print(f\"{row['feature']:50s}: {row['correlation']:7.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of Correlation Analysis**\n",
    "\n",
    "**Overall Finding:** All features show **very weak correlations** with the Over/Under 2.5 target (|r| < 0.10), indicating minimal linear relationships.\n",
    "\n",
    "**1. Time-Based Features:**\n",
    "- **Days since last match** (~-0.01): Virtually no effect — rest periods don't linearly predict goal totals\n",
    "- **Home goals MA5** (+0.074): Strongest time-based correlation — teams scoring recently tend toward slightly higher Over 2.5 rates\n",
    "- **Away goals MA5** (+0.048): Weak positive signal for away team scoring form\n",
    "- **Home/Away conceded MA5** (~-0.04 to -0.05): Slight negative — defensive stability correlates with fewer goals\n",
    "- **League tier** (-0.042): Lower divisions marginally less likely to see 3+ goals\n",
    "- **Month** (+0.019): Negligible seasonal effect\n",
    "\n",
    "**2. Historical Position Features (12 total - percentiles only):**\n",
    "- **Summary statistics:**\n",
    "  - Max |correlation|: ~0.04-0.06 (very weak)\n",
    "  - Mean |correlation|: ~0.02-0.03\n",
    "  - Median |correlation|: ~0.02\n",
    "- **Top correlations:** Home/away percentile positions from recent seasons (S-1, S-2) show slightly stronger effects\n",
    "- **Insight:** Position matters, but not in a simple linear way — likely interacts with other factors (e.g., promoted teams behave differently than established teams at same position)\n",
    "- **Scale:** 0-100 percentile where higher = better position (normalized across different league sizes)\n",
    "\n",
    "**3. Historical Goal Pattern Features (72 total):**\n",
    "- **Summary statistics:**\n",
    "  - Max |correlation|: ~0.05-0.07\n",
    "  - Mean |correlation|: ~0.02-0.03\n",
    "  - Top features: Typically `total_2plus_pct` or `scored_2plus_pct` from recent seasons (S-1, S-2)\n",
    "- **Insight:** Teams with history of high-scoring matches continue that pattern, but effect is weak linearly\n",
    "- **Strongest patterns:** Home team total goals 2+ percentages from recent seasons show positive correlations (0.04-0.07)\n",
    "\n",
    "**4. Key Takeaways:**\n",
    "-  **Weak linear relationships** suggest soccer is governed by **non-linear patterns and interactions**\n",
    "-  **Recent form** (MA5 goals) shows slightly stronger signal than historical season data in linear terms\n",
    "-  **Goal patterns persist** — teams with high-scoring history continue that tendency, but modestly\n",
    "-  **Percentile positions** provide normalized strength indicator across different league sizes\n",
    "-  **Modeling implication:** Linear models (e.g., logistic regression) will struggle. Tree-based models (Random Forest, XGBoost) better suited to capture:\n",
    "  - Non-linear thresholds (e.g., optimal rest days 4-6, not linear)\n",
    "  - Interactions (e.g., promoted + low position != stable low position)\n",
    "  - Context-dependent effects (e.g., position matters more in competitive leagues)\n",
    "\n",
    "**Conclusion:** While individual correlations are weak, these features will likely contribute to model performance through interactions and non-linear patterns when combined in ensemble models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Summary: What We Added**\n",
    "\n",
    "**New Features (Section 4.3.5):**\n",
    "1. **Position percentile difference** - Quality gap between teams using normalized percentiles (higher = home team stronger)\n",
    "2. **Combined attack/defense strength** - Blend recent form (60%) + historical patterns (40%)\n",
    "3. **Form trend** - Improving vs declining (last 5 vs previous 5 matches)\n",
    "4. **Rest days advantage** - Difference in recovery time\n",
    "5. **Season progress** - Early/mid/late season effects (0.0 to 1.0)\n",
    "\n",
    "**Total: 7 derived features** (reduced from 8 by using only percentile difference)\n",
    "\n",
    "**Feature Testing (Section 4.3.X):**\n",
    "- **Correlation Analysis** - Fast initial screening, shows linear relationships only\n",
    "- **Correlation Visualization** - Bar charts showing top correlated features by group\n",
    "\n",
    "**Key Insight:** Weak correlations suggest non-linear patterns dominate in soccer:\n",
    "- Goal-scoring has complex dynamics (e.g., very weak teams score less, but so do very defensive teams)\n",
    "- Features likely interact (promoted teams + low position != stable low position)\n",
    "- Thresholds and context matter (rest days, form trends, league tier effects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of Numerical Features**\n",
    "\n",
    "All numerical features show **very weak correlations** (|r| < 0.08) with the Over/Under 2.5 goals target, suggesting minimal linear dependence.  \n",
    "**Feature-wise summary:**\n",
    "- **Days since last match** (-0.010, -0.009):  \n",
    "  Essentially no relationship — rest periods have no meaningful effect on goal totals.\n",
    "- **Home goals MA5** (0.074):  \n",
    "  Slight positive signal — teams scoring more in recent matches tend to have marginally higher Over 2.5 rates.\n",
    "- **Away goals MA5** (0.048):  \n",
    "  Small negative correlation — possibly due to opponents adapting defensively to strong away attacks.\n",
    "- **League tier** (-0.042):  \n",
    "  Strongest (yet still weak) correlation — lower divisions show a slightly lower frequency of high-scoring games.\n",
    "- **Month** (0.019):  \n",
    "  Negligible seasonal influence on goal totals.\n",
    "**Modeling note:**  \n",
    "\n",
    "Although weak on their own, these variables may still provide value to **non-linear or ensemble models** by capturing interaction effects and subtle contextual patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test boolean/categorical features against the target.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean features - automatically include all that exist in df_basic\n",
    "# Define expected boolean features (add new ones here as you create them)\n",
    "expected_bool_features = ['is_weekend', 'home_promoted', 'home_demoted',\n",
    "                          'away_promoted', 'away_demoted']\n",
    "bool_features = [feat for feat in expected_bool_features if feat in df_basic.columns]\n",
    "\n",
    "print(f\"Analyzing {len(bool_features)} boolean/categorical features:\")\n",
    "print(f\"Features: {bool_features}\\n\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Over 2.5 rate by categorical feature:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for feat in bool_features:\n",
    "    grouped = df_basic.groupby(feat)['over_2_5'].agg(['mean', 'count'])\n",
    "    print(f\"\\n{feat}:\")\n",
    "    print(grouped)\n",
    "\n",
    "    # Chi-squared test\n",
    "    contingency = pd.crosstab(df_basic[feat], df_basic['over_2_5'])\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "    print(f\"Chi-squared p-value: {p:.4f}\")\n",
    "\n",
    "# Visualize - dynamically create subplots based on number of features\n",
    "n_features = min(len(bool_features), 5)  # Limit to 5 for readability\n",
    "fig, axes = plt.subplots(1, n_features, figsize=(5 * n_features, 4))\n",
    "if n_features == 1:\n",
    "    axes = [axes]  # Make it iterable\n",
    "\n",
    "# Configuration for feature visualization\n",
    "feature_configs = {\n",
    "    'is_weekend': {'labels': ['Weekday', 'Weekend'], 'title': 'Over 2.5 Rate: Weekend vs Weekday'},\n",
    "    'home_promoted': {'labels': ['Regular', 'Promoted'], 'title': 'Over 2.5 Rate: Home Team Promoted'},\n",
    "    'home_demoted': {'labels': ['Regular', 'Demoted'], 'title': 'Over 2.5 Rate: Home Team Demoted'},\n",
    "    'away_promoted': {'labels': ['Regular', 'Promoted'], 'title': 'Over 2.5 Rate: Away Team Promoted'},\n",
    "    'away_demoted': {'labels': ['Regular', 'Demoted'], 'title': 'Over 2.5 Rate: Away Team Demoted'}\n",
    "}\n",
    "\n",
    "for idx, feat in enumerate(bool_features[:n_features]):\n",
    "    rates = df_basic.groupby(feat)['over_2_5'].mean()\n",
    "    config = feature_configs.get(feat, {'labels': ['False', 'True'], 'title': f'Over 2.5 Rate: {feat}'})\n",
    "\n",
    "    axes[idx].bar(config['labels'], rates.values, color=['steelblue', 'coral'])\n",
    "    axes[idx].set_title(config['title'], fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Over 2.5 Rate')\n",
    "    axes[idx].set_ylim([0.45, 0.55])  # Zoom in on the relevant range\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(rates.values):\n",
    "        axes[idx].text(i, v + 0.002, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of Boolean/Categorical Features:**\n",
    "\n",
    "Chi-squared tests reveal the following significance levels for `{len(bool_features)}` boolean features analyzed:\n",
    "\n",
    "**Statistically Significant (α=0.05):**\n",
    "- **Weekend vs Weekday** (p=0.0012): Weekend matches show notably higher Over 2.5 rate (50.5% vs 48.7% weekday). This is the strongest categorical predictor, suggesting weekend scheduling may influence match dynamics - possibly due to fan attendance, player rest, or tactical approaches.\n",
    "\n",
    "- **Away Team Demoted** (p=0.0121): Away teams that were demoted show lower Over 2.5 rate (47.6% vs 50.1% regular teams). The 2.5 percentage point difference suggests defensive strategies or morale issues affecting demoted teams playing away.\n",
    "\n",
    "- **Home Team Demoted** (p=0.0277): Home teams that were demoted show lower Over 2.5 rate (47.8% vs 50.1% regular teams). The effect is similar to away demoted, indicating demotion status has defensive implications regardless of venue.\n",
    "\n",
    "**Borderline/Not Significant:**\n",
    "- **Away Team Promoted** (p=0.0712): Shows higher Over 2.5 rate (51.8% vs 49.9%) but just misses significance at α=0.05. May reflect attacking ambition of newly promoted teams.\n",
    "\n",
    "- **Home Team Promoted** (p=0.4339): No meaningful difference (50.8% vs 49.9%). Home promoted teams do not show distinct scoring patterns.\n",
    "\n",
    "**Conclusion:**  \n",
    "- **Weekend effect** is the most robust categorical predictor with clear practical significance.\n",
    "- **Demotion features** (both home and away) show consistent negative effects on Over 2.5 rate, suggesting these teams adopt more conservative tactics.\n",
    "- **Promotion features** show weaker/inconsistent effects - away promoted is borderline while home promoted shows no effect.\n",
    "- All `{len(bool_features)}` features detected and analyzed: `{bool_features}`\n",
    "- Individual effects remain modest (all within ±3 percentage points of baseline 50%), but may prove valuable in ensemble models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Extended feature df engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create extended dataframe with all available match data including detailed statistics and betting odds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTENDED DATASET: All available match data including detailed statistics\n",
    "extended_core_features = [col for col in [\n",
    "    # Core match info\n",
    "    'Div', 'Season', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG', 'HTR',\n",
    "    # Match statistics\n",
    "    'Attendance', 'Referee', 'HS', 'AS', 'HST', 'AST', 'HHW', 'AHW', 'HC', 'AC',\n",
    "    'HF', 'AF', 'HFKC', 'AFKC', 'HO', 'AO', 'HY', 'AY', 'HR', 'AR', 'HBP', 'ABP'\n",
    "] if col in all_matches.columns]\n",
    "\n",
    "# All engineered features (using rich match statistics)\n",
    "# Note: Additional features (time-based + historical) will be merged from df_basic after creation\n",
    "extended_engineered_features = [col for col in [\n",
    "    'total_goals', 'ht_total_goals', 'second_half_goals',  # Goal-based\n",
    "    'home_shot_accuracy', 'away_shot_accuracy', 'total_shots', 'total_shots_on_target',  # Shot-based\n",
    "    'shot_dominance', 'corner_dominance', 'total_fouls', 'total_cards', 'card_intensity',  # Game dynamics\n",
    "    'league_tier', 'month', 'is_weekend',  # Date/league features\n",
    "    'over_2_5'  # Target variable\n",
    "] if col in all_matches.columns]\n",
    "\n",
    "# Extended features (betting odds - only high-quality columns after imputation)\n",
    "betting_features = []\n",
    "for col in all_matches.columns:\n",
    "    # Check if it's a betting column and has good data coverage (>10%)\n",
    "    if any(bookmaker in col for bookmaker in ['B365', 'BW', 'PS', 'IW', 'LB', 'WH', 'SJ', 'VC', 'BF', '1XB']):\n",
    "        data_coverage = (all_matches[col].notna().sum() / len(all_matches)) * 100\n",
    "        if data_coverage >= 10:  # Only include columns with at least 10% data coverage\n",
    "            betting_features.append(col)\n",
    "\n",
    "# Create extended dataframe\n",
    "all_extended_features = extended_core_features + extended_engineered_features + betting_features\n",
    "# Remove duplicates while preserving order\n",
    "all_extended_features = list(dict.fromkeys(all_extended_features))\n",
    "\n",
    "df_extended = all_matches[all_extended_features].copy()\n",
    "df_extended = df_extended.sort_values(['Div', 'Date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Extended dataframe created\")\n",
    "print(f\"Shape: {df_extended.shape}\")\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"Core features: {len(extended_core_features)}\")\n",
    "print(f\"Engineered features: {len(extended_engineered_features)}\")\n",
    "print(f\"Betting features (>10% coverage): {len(betting_features)}\")\n",
    "print(f\"Total features: {len(all_extended_features)}\")\n",
    "print(f\"\\nColumns: {df_extended.columns.tolist()[:20]}...\")  # Show first 20\n",
    "df_extended.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Merging engineered features from df_basic\n",
    "\n",
    "Add all engineered features from df_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_based_features = [\n",
    "    'home_days_since_last', 'away_days_since_last',\n",
    "    'home_goals_ma5', 'away_goals_ma5', 'home_conceded_ma5', 'away_conceded_ma5',\n",
    "    'home_promoted', 'away_promoted', 'home_demoted', 'away_demoted'\n",
    "]\n",
    "\n",
    "# Historical position features (overall only)\n",
    "historical_position_cols = [col for col in df_basic.columns if 'position_pct_' in col]\n",
    "\n",
    "# Historical goal pattern features (overall + home-specific + away-specific, counts + percentages)\n",
    "historical_goal_cols = [col for col in df_basic.columns if any(\n",
    "    f'{p}_{t}plus' in col for p in ['scored', 'conceded', 'total'] for t in [2, 3]\n",
    ")]\n",
    "\n",
    "# Derived features combining historical patterns\n",
    "derived_features = [col for col in df_basic.columns if any(\n",
    "    pattern in col for pattern in ['_strength_', '_combined_']\n",
    ")]\n",
    "\n",
    "features_to_merge = time_based_features + historical_position_cols + historical_goal_cols + derived_features\n",
    "\n",
    "print(f\"Features to merge from df_basic: {len(features_to_merge)}\")\n",
    "print(f\"Time-based features: {len(time_based_features)}\")\n",
    "print(f\"Position percentiles: {len(historical_position_cols)}\")\n",
    "print(f\"Goal patterns (overall + home/away, counts + pct): {len(historical_goal_cols)}\")\n",
    "print(f\"Derived features: {len(derived_features)}\")\n",
    "\n",
    "df_basic_sorted = df_basic.sort_values(['Div', 'Date']).reset_index(drop=True)\n",
    "df_extended_sorted = df_extended.sort_values(['Div', 'Date']).reset_index(drop=True)\n",
    "\n",
    "match_check = (\n",
    "    (df_basic_sorted['Div'] == df_extended_sorted['Div']) &\n",
    "    (df_basic_sorted['Date'] == df_extended_sorted['Date']) &\n",
    "    (df_basic_sorted['HomeTeam'] == df_extended_sorted['HomeTeam']) &\n",
    "    (df_basic_sorted['AwayTeam'] == df_extended_sorted['AwayTeam'])\n",
    ").all()\n",
    "\n",
    "if match_check:\n",
    "    print(f\"\\nRow alignment verified - safe to merge features\")\n",
    "\n",
    "    for col in features_to_merge:\n",
    "        df_extended[col] = df_basic_sorted[col].values\n",
    "\n",
    "    print(f\"\n{len(features_to_merge)} features added to df_extended\")\n",
    "    print(f\"- {len(time_based_features)} time-based features\")\n",
    "    print(f\"- {len(historical_position_cols)} position percentiles\")\n",
    "    print(f\"- {len(historical_goal_cols)} goal pattern features\")\n",
    "    print(f\"- {len(derived_features)} derived features\")\n",
    "    print(f\"\\nFinal df_extended shape: {df_extended.shape}\")\n",
    "else:\n",
    "    print(\"⚠ Row mismatch detected - cannot safely merge features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2. More features for df_extended\n",
    "\n",
    "**Extended Statistics - Moving Averages & Seasonal Patterns**\n",
    "\n",
    "Calculate MA5 and historical patterns for match statistics (shots, corners, fouls, cards, etc.) following the same approach as goal-based features.\n",
    "\n",
    " **Data Leakage Prevention:**\n",
    "- MA5 calculations use `Date < date` to exclude current match\n",
    "- NaN values preserved for early-season matches (no fillna with overall means)\n",
    "- Same fix applied in Section 4.3.2 (basic df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate moving averages and seasonal history for extended match statistics.\n",
    "Following the same pattern as goal-based features in df_basic.\n",
    "\n",
    "MA5 Features: High + Medium Priority (Shots, Shots on Target, Corners, Fouls, Yellow Cards)\n",
    "Seasonal Patterns: High Priority Only (Shots, Shots on Target, Corners)\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SECTION 4.4.2: EXTENDED STATS MOVING AVERAGES & SEASONAL PATTERNS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: 5-Match Moving Averages (High + Medium Priority)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n1. CALCULATING 5-MATCH MOVING AVERAGES\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Define statistics to calculate MA5 for - High and Medium priority\n",
    "ma5_stats_config = {\n",
    "    # High Priority - Shooting\n",
    "    ('HS', 'shots'): 'home',\n",
    "    ('AS', 'shots'): 'away',\n",
    "    ('HST', 'shots_target'): 'home',\n",
    "    ('AST', 'shots_target'): 'away',\n",
    "    # High Priority - Attacking Pressure  \n",
    "    ('HC', 'corners'): 'home',\n",
    "    ('AC', 'corners'): 'away',\n",
    "    # Medium Priority - Discipline\n",
    "    ('HF', 'fouls'): 'home',\n",
    "    ('AF', 'fouls'): 'away',\n",
    "    ('HY', 'yellows'): 'home',\n",
    "    ('AY', 'yellows'): 'away',\n",
    "}\n",
    "\n",
    "# Initialize MA5 columns\n",
    "print(f\"Initializing {len(ma5_stats_config)} MA5 columns...\")\n",
    "for (raw_col, stat_name), team_type in ma5_stats_config.items():\n",
    "    ma5_col = f'{team_type}_{stat_name}_ma5'\n",
    "    df_extended[ma5_col] = np.nan\n",
    "\n",
    "# Calculate moving averages - optimized by pre-sorting\n",
    "print(\"Calculating moving averages (optimized groupby approach)...\")\n",
    "df_extended_sorted = df_extended.sort_values(['Div', 'Date']).reset_index(drop=True)\n",
    "\n",
    "for (raw_col, stat_name), team_type in ma5_stats_config.items():\n",
    "    if raw_col not in df_extended.columns:\n",
    "        continue\n",
    "    \n",
    "    ma5_col = f'{team_type}_{stat_name}_ma5'\n",
    "    team_col = 'HomeTeam' if team_type == 'home' else 'AwayTeam'\n",
    "    \n",
    "    print(f\"Processing {ma5_col}...\")\n",
    "    \n",
    "    # Group by division and team, then calculate MA5\n",
    "    for (div, team), group in df_extended_sorted.groupby(['Div', team_col]):\n",
    "        indices = group.index.tolist()\n",
    "        values = group[raw_col].values\n",
    "        \n",
    "        # Calculate MA5 for each match in this group\n",
    "        for i, idx in enumerate(indices):\n",
    "            if i >= 5:  # Need at least 5 previous matches\n",
    "                ma5_value = np.nanmean(values[i-5:i])  # Exclude current match\n",
    "                if not np.isnan(ma5_value):\n",
    "                    df_extended.loc[idx, ma5_col] = ma5_value\n",
    "\n",
    "# NO DATA LEAKAGE: NaN values preserved for early-season matches\n",
    "print(\"\\nMA5 Summary:\")\n",
    "for (raw_col, stat_name), team_type in ma5_stats_config.items():\n",
    "    ma5_col = f'{team_type}_{stat_name}_ma5'\n",
    "    if ma5_col in df_extended.columns:\n",
    "        missing = df_extended[ma5_col].isna().sum()\n",
    "        missing_pct = (missing / len(df_extended)) * 100\n",
    "        mean_val = df_extended[ma5_col].mean()\n",
    "        print(f\"{ma5_col:25s}: missing={missing:5d} ({missing_pct:4.1f}%), mean={mean_val:6.2f}\")\n",
    "\n",
    "print(f\"\nAdded {len(ma5_stats_config)} moving average features\")\n",
    "print(f\"(NaN values preserved for early-season matches - no data leakage)\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: Seasonal Historical Patterns (High Priority Only)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n2. CALCULATING SEASONAL HISTORICAL PATTERNS\")\n",
    "print(\"-\" * 70)\n",
    "print(\"High priority features only: Shots, Shots on Target, Corners\")\n",
    "\n",
    "# Define thresholds for HIGH PRIORITY stats only\n",
    "stat_thresholds = {\n",
    "    'shots': [10, 15],           # 10+ shots, 15+ shots\n",
    "    'shots_target': [5, 8],      # 5+ on target, 8+ on target\n",
    "    'corners': [6, 10],          # 6+ corners, 10+ corners\n",
    "}\n",
    "\n",
    "# Season info\n",
    "unique_seasons = sorted(df_extended['Season'].unique())\n",
    "season_to_order = {s: i for i, s in enumerate(unique_seasons)}\n",
    "past_seasons = ['2019/2020', '2020/2021', '2021/2022', '2022/2023', '2023/2024', '2024/2025']\n",
    "season_years = [2019, 2020, 2021, 2022, 2023, 2024]\n",
    "\n",
    "# Map stat names to columns\n",
    "stat_to_home_col = {'shots': 'HS', 'shots_target': 'HST', 'corners': 'HC'}\n",
    "stat_to_away_col = {'shots': 'AS', 'shots_target': 'AST', 'corners': 'AC'}\n",
    "\n",
    "# Pre-calculate seasonal statistics for all teams (OPTIMIZATION)\n",
    "print(\"Pre-calculating team-season statistics...\")\n",
    "team_season_stats = {}\n",
    "\n",
    "for season in past_seasons:\n",
    "    for div in df_extended['Div'].unique():\n",
    "        season_div_mask = (df_extended['Season'] == season) & (df_extended['Div'] == div)\n",
    "        season_div_data = df_extended[season_div_mask]\n",
    "        \n",
    "        if len(season_div_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get all teams in this season/division\n",
    "        all_teams = set(season_div_data['HomeTeam'].unique()) | set(season_div_data['AwayTeam'].unique())\n",
    "        \n",
    "        for team in all_teams:\n",
    "            # Overall stats (all matches)\n",
    "            overall_mask = (season_div_data['HomeTeam'] == team) | (season_div_data['AwayTeam'] == team)\n",
    "            overall_matches = season_div_data[overall_mask]\n",
    "            \n",
    "            # Home-only stats\n",
    "            home_mask = season_div_data['HomeTeam'] == team\n",
    "            home_matches = season_div_data[home_mask]\n",
    "            \n",
    "            # Away-only stats\n",
    "            away_mask = season_div_data['AwayTeam'] == team\n",
    "            away_matches = season_div_data[away_mask]\n",
    "            \n",
    "            # Calculate stats for each threshold\n",
    "            stats = {}\n",
    "            for stat_name, thresholds in stat_thresholds.items():\n",
    "                h_col = stat_to_home_col[stat_name]\n",
    "                a_col = stat_to_away_col[stat_name]\n",
    "                \n",
    "                for threshold in thresholds:\n",
    "                    # Overall stats\n",
    "                    count = 0\n",
    "                    for _, match in overall_matches.iterrows():\n",
    "                        col = h_col if match['HomeTeam'] == team else a_col\n",
    "                        if not pd.isna(match.get(col)) and match.get(col, 0) >= threshold:\n",
    "                            count += 1\n",
    "                    total = len(overall_matches)\n",
    "                    stats[f'{stat_name}_{threshold}plus_count'] = count\n",
    "                    stats[f'{stat_name}_{threshold}plus_pct'] = count / total if total > 0 else 0\n",
    "                    \n",
    "                    # Home-only stats\n",
    "                    if h_col in home_matches.columns:\n",
    "                        home_count = (home_matches[h_col] >= threshold).sum()\n",
    "                        home_total = len(home_matches)\n",
    "                        stats[f'home_{stat_name}_{threshold}plus_count'] = home_count\n",
    "                        stats[f'home_{stat_name}_{threshold}plus_pct'] = home_count / home_total if home_total > 0 else 0\n",
    "                    \n",
    "                    # Away-only stats\n",
    "                    if a_col in away_matches.columns:\n",
    "                        away_count = (away_matches[a_col] >= threshold).sum()\n",
    "                        away_total = len(away_matches)\n",
    "                        stats[f'away_{stat_name}_{threshold}plus_count'] = away_count\n",
    "                        stats[f'away_{stat_name}_{threshold}plus_pct'] = away_count / away_total if away_total > 0 else 0\n",
    "            \n",
    "            team_season_stats[(season, div, team)] = stats\n",
    "\n",
    "print(f\"Pre-calculated stats for {len(team_season_stats)} team-season combinations\")\n",
    "\n",
    "# Initialize seasonal pattern columns\n",
    "print(\"Initializing seasonal pattern columns...\")\n",
    "total_columns = 0\n",
    "for stat_name in stat_thresholds.keys():\n",
    "    for threshold in stat_thresholds[stat_name]:\n",
    "        for year in season_years:\n",
    "            # Overall patterns\n",
    "            for prefix in ['home', 'away']:\n",
    "                df_extended[f'{prefix}_{stat_name}_{threshold}plus_count_{year}'] = 0\n",
    "                df_extended[f'{prefix}_{stat_name}_{threshold}plus_pct_{year}'] = np.nan\n",
    "                total_columns += 2\n",
    "            # Context-aware patterns\n",
    "            df_extended[f'home_home_{stat_name}_{threshold}plus_count_{year}'] = 0\n",
    "            df_extended[f'home_home_{stat_name}_{threshold}plus_pct_{year}'] = np.nan\n",
    "            df_extended[f'away_away_{stat_name}_{threshold}plus_count_{year}'] = 0\n",
    "            df_extended[f'away_away_{stat_name}_{threshold}plus_pct_{year}'] = np.nan\n",
    "            total_columns += 4\n",
    "\n",
    "print(f\"Created {total_columns} seasonal pattern columns\")\n",
    "\n",
    "# Populate features using pre-calculated statistics (FAST!)\n",
    "print(\"\\nPopulating seasonal pattern features (using lookups)...\")\n",
    "for idx, row in df_extended.iterrows():\n",
    "    current_season = row['Season']\n",
    "    current_season_order = season_to_order.get(current_season, 999)\n",
    "    \n",
    "    for year_idx, year in enumerate(season_years):\n",
    "        if year_idx >= current_season_order:\n",
    "            continue\n",
    "        \n",
    "        past_season = past_seasons[year_idx]\n",
    "        \n",
    "        # Lookup home team stats\n",
    "        home_key = (past_season, row['Div'], row['HomeTeam'])\n",
    "        if home_key in team_season_stats:\n",
    "            home_stats = team_season_stats[home_key]\n",
    "            for stat_name in stat_thresholds.keys():\n",
    "                for threshold in stat_thresholds[stat_name]:\n",
    "                    # Overall\n",
    "                    df_extended.loc[idx, f'home_{stat_name}_{threshold}plus_count_{year}'] = home_stats.get(f'{stat_name}_{threshold}plus_count', 0)\n",
    "                    df_extended.loc[idx, f'home_{stat_name}_{threshold}plus_pct_{year}'] = home_stats.get(f'{stat_name}_{threshold}plus_pct', np.nan)\n",
    "                    # Home-specific\n",
    "                    df_extended.loc[idx, f'home_home_{stat_name}_{threshold}plus_count_{year}'] = home_stats.get(f'home_{stat_name}_{threshold}plus_count', 0)\n",
    "                    df_extended.loc[idx, f'home_home_{stat_name}_{threshold}plus_pct_{year}'] = home_stats.get(f'home_{stat_name}_{threshold}plus_pct', np.nan)\n",
    "        \n",
    "        # Lookup away team stats\n",
    "        away_key = (past_season, row['Div'], row['AwayTeam'])\n",
    "        if away_key in team_season_stats:\n",
    "            away_stats = team_season_stats[away_key]\n",
    "            for stat_name in stat_thresholds.keys():\n",
    "                for threshold in stat_thresholds[stat_name]:\n",
    "                    # Overall\n",
    "                    df_extended.loc[idx, f'away_{stat_name}_{threshold}plus_count_{year}'] = away_stats.get(f'{stat_name}_{threshold}plus_count', 0)\n",
    "                    df_extended.loc[idx, f'away_{stat_name}_{threshold}plus_pct_{year}'] = away_stats.get(f'{stat_name}_{threshold}plus_pct', np.nan)\n",
    "                    # Away-specific\n",
    "                    df_extended.loc[idx, f'away_away_{stat_name}_{threshold}plus_count_{year}'] = away_stats.get(f'away_{stat_name}_{threshold}plus_count', 0)\n",
    "                    df_extended.loc[idx, f'away_away_{stat_name}_{threshold}plus_pct_{year}'] = away_stats.get(f'away_{stat_name}_{threshold}plus_pct', np.nan)\n",
    "    \n",
    "    # Progress indicator\n",
    "    if (idx + 1) % 10000 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(df_extended)} matches...\")\n",
    "\n",
    "print(f\"\nPopulated {total_columns} seasonal pattern features\")\n",
    "\n",
    "# ============================================================\n",
    "# Summary\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY - SECTION 4.4.2 FEATURE ENGINEERING\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Moving Average Features (MA5): {len(ma5_stats_config)}\")\n",
    "print(f\"- High Priority: Shots, Shots on Target, Corners\")\n",
    "print(f\"- Medium Priority: Fouls, Yellow Cards\")\n",
    "print(f\"\\nSeasonal Pattern Features: {total_columns}\")\n",
    "print(f\"- High Priority Only: Shots, Shots on Target, Corners\")\n",
    "print(f\"- Overall + Home/Away context for each stat\")\n",
    "print(f\"- {len(stat_thresholds)} stat types × 2 thresholds × 6 seasons\")\n",
    "print(f\"\\nTotal New Features: {len(ma5_stats_config) + total_columns}\")\n",
    "print(f\"Final df_extended shape: {df_extended.shape}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3. Betting odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Betting Odds Feature Engineering - Section 4.4.3\n",
    "\n",
    "Goals:\n",
    "1. Merge BetVictor (BV) and VC Bet odds (same company, rebranded)\n",
    "2. Calculate minimum odds for each betting category\n",
    "3. Count number of bookmakers per betting category\n",
    "4. Remove individual bookmaker columns, keep only aggregates (Max/Avg/Min/Count)\n",
    "\n",
    "Betting Categories:\n",
    "- Match Odds (H/D/A): 1X2 pre-match\n",
    "- Match Odds Closing (CH/CD/CA): 1X2 closing\n",
    "- Over/Under (>2.5/<2.5): Total goals pre-match\n",
    "- Over/Under Closing (C>2.5/C<2.5): Total goals closing\n",
    "- Asian Handicap (AHH/AHA): Pre-match handicap\n",
    "- Asian Handicap Closing (CAHH/CAHA): Closing handicap\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SECTION 4.4.3: BETTING ODDS FEATURE ENGINEERING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# STEP 1: Add Max/Avg columns from all_matches if missing\n",
    "\n",
    "print(\"\\n1. ADDING MAX/AVG AGGREGATE COLUMNS FROM SOURCE DATA\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Add Max/Avg columns from all_matches if they're not already in df_extended\n",
    "max_avg_cols_to_add = [col for col in all_matches.columns \n",
    "                       if col.startswith(('Max', 'Avg')) \n",
    "                       and col not in df_extended.columns]\n",
    "\n",
    "if max_avg_cols_to_add:\n",
    "    print(f\"Adding {len(max_avg_cols_to_add)} Max/Avg columns:\")\n",
    "    for col in sorted(max_avg_cols_to_add):\n",
    "        df_extended[col] = all_matches[col]\n",
    "        print(f\"+ {col}\")\n",
    "else:\n",
    "    print(\"All Max/Avg columns already present\")\n",
    "\n",
    "# STEP 2: Merge BetVictor and VC Bet (Same Company)\n",
    "\n",
    "\n",
    "print(\"\\n2. MERGING BETVICTOR (BV) AND VC BET ODDS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# BetVictor columns for different betting categories\n",
    "bv_vc_pairs = {\n",
    "    'BVH': 'VCH', 'BVD': 'VCD', 'BVA': 'VCA',\n",
    "    'BVCH': 'VCCH', 'BVCD': 'VCCD', 'BVCA': 'VCCA',\n",
    "}\n",
    "\n",
    "merged_count = 0\n",
    "for bv_col, vc_col in bv_vc_pairs.items():\n",
    "    if bv_col in df_extended.columns and vc_col in df_extended.columns:\n",
    "        before_na = df_extended[bv_col].isna().sum()\n",
    "        df_extended[bv_col] = df_extended[bv_col].fillna(df_extended[vc_col])\n",
    "        after_na = df_extended[bv_col].isna().sum()\n",
    "        filled = before_na - after_na\n",
    "        if filled > 0:\n",
    "            print(f\"{bv_col}: filled {filled} values from {vc_col}\")\n",
    "            merged_count += 1\n",
    "\n",
    "if merged_count == 0:\n",
    "    print(f\"No VC columns found or already merged\")\n",
    "\n",
    "print(f\" BetVictor and VC Bet odds merged\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: Define Bookmaker Columns by Category\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n3. IDENTIFYING BOOKMAKER COLUMNS BY CATEGORY\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Define bookmaker prefixes (excluding aggregates like Max, Avg, Bb)\n",
    "bookmakers = ['1XB', 'B365', 'BF', 'BFD', 'BMGM', 'BV', 'BS', 'BW', 'CL', \n",
    "              'GB', 'IW', 'LB', 'PS', 'PSH', 'PSD', 'PSA', 'PH', 'PD', 'PA',\n",
    "              'SO', 'SB', 'SJ', 'SY', 'WH']\n",
    "\n",
    "# Define betting categories with their suffixes and column naming\n",
    "betting_categories = {\n",
    "    'match_odds_open': {\n",
    "        'suffixes': ['H', 'D', 'A'],\n",
    "        'count_col': 'NumBookmakers_MatchOdds',\n",
    "        'min_cols': ['MinH', 'MinD', 'MinA']\n",
    "    },\n",
    "    'match_odds_closing': {\n",
    "        'suffixes': ['CH', 'CD', 'CA'],\n",
    "        'count_col': 'NumBookmakers_MatchOddsClosing',\n",
    "        'min_cols': ['MinCH', 'MinCD', 'MinCA']\n",
    "    },\n",
    "    'over_under_open': {\n",
    "        'suffixes': ['>2.5', '<2.5'],\n",
    "        'count_col': 'NumBookmakers_OverUnder',\n",
    "        'min_cols': ['Min>2.5', 'Min<2.5']\n",
    "    },\n",
    "    'over_under_closing': {\n",
    "        'suffixes': ['C>2.5', 'C<2.5'],\n",
    "        'count_col': 'NumBookmakers_OverUnderClosing',\n",
    "        'min_cols': ['MinC>2.5', 'MinC<2.5']\n",
    "    },\n",
    "    'asian_handicap_open': {\n",
    "        'suffixes': ['AHH', 'AHA'],\n",
    "        'count_col': 'NumBookmakers_AsianHandicap',\n",
    "        'min_cols': ['MinAHH', 'MinAHA']\n",
    "    },\n",
    "    'asian_handicap_closing': {\n",
    "        'suffixes': ['CAHH', 'CAHA'],\n",
    "        'count_col': 'NumBookmakers_AsianHandicapClosing',\n",
    "        'min_cols': ['MinCAHH', 'MinCAHA']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Find bookmaker columns for each category\n",
    "bookmaker_cols_by_category = {}\n",
    "\n",
    "for category_name, category_info in betting_categories.items():\n",
    "    suffixes = category_info['suffixes']\n",
    "    bookmaker_cols_by_category[category_name] = {}\n",
    "    \n",
    "    for suffix in suffixes:\n",
    "        cols = []\n",
    "        for bookmaker in bookmakers:\n",
    "            # Check for exact column match\n",
    "            col_name = f\"{bookmaker}{suffix}\"\n",
    "            if col_name in df_extended.columns:\n",
    "                cols.append(col_name)\n",
    "        \n",
    "        bookmaker_cols_by_category[category_name][suffix] = cols\n",
    "        if cols:\n",
    "            print(f\"{category_name:30s} [{suffix:6s}]: {len(cols):2d} bookmakers\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4: Calculate Min Odds and Bookmaker Counts per Category\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n4. CALCULATING MIN ODDS AND BOOKMAKER COUNTS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for category_name, category_info in betting_categories.items():\n",
    "    suffixes = category_info['suffixes']\n",
    "    count_col = category_info['count_col']\n",
    "    min_cols = category_info['min_cols']\n",
    "    \n",
    "    # Track unique bookmakers for this category\n",
    "    unique_bookmakers_per_row = []\n",
    "    \n",
    "    for i, suffix in enumerate(suffixes):\n",
    "        bookmaker_cols = bookmaker_cols_by_category[category_name][suffix]\n",
    "        \n",
    "        if not bookmaker_cols:\n",
    "            continue\n",
    "        \n",
    "        # Calculate minimum odds for this suffix\n",
    "        min_col = min_cols[i]\n",
    "        df_extended[min_col] = df_extended[bookmaker_cols].min(axis=1)\n",
    "        \n",
    "        print(f\"Created {min_col:15s} from {len(bookmaker_cols)} bookmakers\")\n",
    "    \n",
    "    # Calculate bookmaker count for this category\n",
    "    # Count how many bookmakers provided at least one odds type for this match\n",
    "    # For match odds: if a bookmaker has H, D, or A, count it once\n",
    "    all_bookmaker_cols_in_category = []\n",
    "    for suffix in suffixes:\n",
    "        all_bookmaker_cols_in_category.extend(bookmaker_cols_by_category[category_name][suffix])\n",
    "    \n",
    "    if all_bookmaker_cols_in_category:\n",
    "        # Extract unique bookmaker prefixes from column names\n",
    "        def extract_bookmaker_prefix(col_name):\n",
    "            \"\"\"Extract bookmaker prefix from column name\"\"\"\n",
    "            for bm in bookmakers:\n",
    "                if col_name.startswith(bm):\n",
    "                    return bm\n",
    "            return None\n",
    "        \n",
    "        # OPTIMIZED: Use vectorized pandas operations instead of nested loop\n",
    "        # For each row, count unique bookmakers that have at least one non-NaN value\n",
    "        def count_unique_bookmakers(row):\n",
    "            \"\"\"Count unique bookmakers with at least one non-NaN value for this row\"\"\"\n",
    "            unique_bm = set()\n",
    "            for col in all_bookmaker_cols_in_category:\n",
    "                if pd.notna(row[col]):\n",
    "                    bm_prefix = extract_bookmaker_prefix(col)\n",
    "                    if bm_prefix:\n",
    "                        unique_bm.add(bm_prefix)\n",
    "            return len(unique_bm)\n",
    "        \n",
    "        # Apply function across rows - much faster than explicit loop\n",
    "        df_extended[count_col] = df_extended[all_bookmaker_cols_in_category].apply(count_unique_bookmakers, axis=1)\n",
    "        avg_count = df_extended[count_col].mean()\n",
    "        print(f\"Created {count_col:40s} (avg: {avg_count:.1f} bookmakers/match)\")\n",
    "\n",
    "print(f\"\nCreated minimum odds and bookmaker counts for all categories\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 5: Calculate Disagreement (Max - Min)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n5. CALCULATING BOOKMAKER DISAGREEMENT (MAX - MIN)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Define disagreement calculations based on existing Max columns\n",
    "disagreement_mapping = {\n",
    "    'DisagreementH': ('MaxH', 'MinH'),\n",
    "    'DisagreementD': ('MaxD', 'MinD'),\n",
    "    'DisagreementA': ('MaxA', 'MinA'),\n",
    "    'DisagreementCH': ('MaxCH', 'MinCH'),\n",
    "    'DisagreementCD': ('MaxCD', 'MinCD'),\n",
    "    'DisagreementCA': ('MaxCA', 'MinCA'),\n",
    "    'Disagreement>2.5': ('Max>2.5', 'Min>2.5'),\n",
    "    'Disagreement<2.5': ('Max<2.5', 'Min<2.5'),\n",
    "    'DisagreementC>2.5': ('MaxC>2.5', 'MinC>2.5'),\n",
    "    'DisagreementC<2.5': ('MaxC<2.5', 'MinC<2.5'),\n",
    "    'DisagreementAHH': ('MaxAHH', 'MinAHH'),\n",
    "    'DisagreementAHA': ('MaxAHA', 'MinAHA'),\n",
    "    'DisagreementCAHH': ('MaxCAHH', 'MinCAHH'),\n",
    "    'DisagreementCAHA': ('MaxCAHA', 'MinCAHA'),\n",
    "}\n",
    "\n",
    "for disagreement_col, (max_col, min_col) in disagreement_mapping.items():\n",
    "    if max_col in df_extended.columns and min_col in df_extended.columns:\n",
    "        df_extended[disagreement_col] = df_extended[max_col] - df_extended[min_col]\n",
    "        avg_disagreement = df_extended[disagreement_col].mean()\n",
    "        print(f\"{disagreement_col:25s} = {max_col:12s} - {min_col:12s} (avg: {avg_disagreement:.3f})\")\n",
    "\n",
    "print(f\"\nCreated disagreement features for available Max/Min pairs\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 6: Remove Individual Bookmaker Columns\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n6. REMOVING INDIVIDUAL BOOKMAKER COLUMNS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Identify all individual bookmaker columns to remove\n",
    "individual_bookmaker_cols = []\n",
    "for col in df_extended.columns:\n",
    "    # Check if column starts with a bookmaker prefix\n",
    "    if any(col.startswith(bm) for bm in bookmakers):\n",
    "        # Exclude if it's an aggregate or new feature we created\n",
    "        if not col.startswith(('Num', 'Min', 'Disagreement')):\n",
    "            individual_bookmaker_cols.append(col)\n",
    "\n",
    "print(f\"Removing {len(individual_bookmaker_cols)} individual bookmaker columns...\")\n",
    "df_extended = df_extended.drop(columns=individual_bookmaker_cols)\n",
    "\n",
    "print(f\" Removed individual bookmaker columns\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 7: Summary\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BETTING ODDS FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Count final betting features\n",
    "betting_features_remaining = [col for col in df_extended.columns if any(\n",
    "    pattern in col for pattern in ['Max', 'Min', 'Avg', 'Disagreement', 'NumBookmakers', 'Bb']\n",
    ")]\n",
    "\n",
    "print(f\"\\nFinal betting features: {len(betting_features_remaining)}\")\n",
    "print(f\"- Aggregate odds (Max/Avg/Min): {sum(1 for c in betting_features_remaining if c.startswith(('Max', 'Avg', 'Min')) and 'Disagreement' not in c and 'NumBookmakers' not in c)}\")\n",
    "print(f\"- Disagreement features: {sum(1 for c in betting_features_remaining if 'Disagreement' in c)}\")\n",
    "print(f\"- Bookmaker count features: {sum(1 for c in betting_features_remaining if 'NumBookmakers' in c)}\")\n",
    "print(f\"- BetBrain features: {sum(1 for c in betting_features_remaining if c.startswith('Bb'))}\")\n",
    "\n",
    "print(f\"\\nFinal df_extended shape: {df_extended.shape}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Final feature dataframe\n",
    "\n",
    "Combine all features into the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of engineered features (BEFORE removing post-match columns)\n",
    "# Note: Post-match features like FTHG, FTAG, FTR, total_goals will be removed in section 4.6\n",
    "print(\"Final engineered dataframe (with all features):\")\n",
    "print(f\"Shape: {df_basic.shape}\")\n",
    "print(f\"\\nColumns:\")\n",
    "print(df_basic.columns.tolist())\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df_basic.isnull().sum().sum())\n",
    "print(f\"\\nSample:\")\n",
    "# Display columns that exist (some may not exist depending on data processing)\n",
    "display_cols = [col for col in ['Date', 'HomeTeam', 'AwayTeam', 'total_goals', 'over_2_5',\n",
    "    'home_days_since_last', 'home_goals_ma5', 'home_promoted', 'is_weekend'] if col in df_basic.columns]\n",
    "df_basic[display_cols].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Added Features**\n",
    "\n",
    "**Time-based metrics:**  \n",
    "- `home_days_since_last`, `away_days_since_last`: Days since each team’s previous match (average ≈ 9.5 days)\n",
    "\n",
    "**Recent performance (5-match moving averages):**  \n",
    "- `home_goals_ma5`, `away_goals_ma5`: Average goals scored in the last 5 matches  \n",
    "- `home_conceded_ma5`, `away_conceded_ma5`: Average goals conceded in the last 5 matches  \n",
    "\n",
    "**League transitions:**  \n",
    "- `home_promoted`, `away_promoted`, `home_demoted`, `away_demoted`: Indicators of team movement between divisions  \n",
    "\n",
    "#### **Main Observations** \n",
    "   - All features show very weak correlations (|r| < 0.02) with the Over/Under 2.5 goals target.  \n",
    "\n",
    "   - Chi-square tests are insignificant (p > 0.05), suggesting no strong individual relationships. \n",
    "\n",
    "   - League level shows the strongest (though still small) correlation at −0.012, indicating that lower leagues may have slightly fewer high-scoring games.\n",
    "\n",
    "   - The home team’s recent attacking form has a weak positive correlation (0.010), but it’s negligible in isolation.\n",
    "\n",
    "   - Newly promoted or relegated teams do not exhibit consistent differences in total goals per match.\n",
    "\n",
    "   - Rest days and weekend scheduling have no measurable effect on goal totals.\n",
    "\n",
    "#### **Implications for Modeling**\n",
    "\n",
    "While each feature provides limited predictive power on its own, they may still add value when used together in non-linear models such as tree-based or ensemble methods.  \n",
    "\n",
    "Weak linear relationships are expected, as the Over/Under 2.5 target is a roughly balanced binary outcome, making individual predictors inherently limited in isolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Remove Post-Match Features to Prevent Data Leakage\n",
    "\n",
    "Now that all historical features have been calculated from past match data, we must remove all columns that contain post-match information or direct derivatives of the target variable. This ensures that our feature matrices contain only genuine pre-match inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-match columns to remove from BASELINE dataset (df_basic)\n",
    "# These columns contain information only available AFTER the match is played\n",
    "post_match_basic = [\n",
    "    'FTHG', 'FTAG', 'FTR',  # Final match results\n",
    "    'total_goals',  # Direct derivative of the target (FTHG + FTAG)\n",
    "    'HTHG', 'HTAG', 'HTR',  # Half-time results (if present)\n",
    "]\n",
    "\n",
    "# Remove columns that exist in df_basic\n",
    "cols_to_drop_basic = [col for col in post_match_basic if col in df_basic.columns]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BASELINE DATASET (df_basic) - Removing Post-Match Features\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Shape before: {df_basic.shape}\")\n",
    "print(f\"Columns to remove: {cols_to_drop_basic}\")\n",
    "\n",
    "df_basic = df_basic.drop(columns=cols_to_drop_basic)\n",
    "\n",
    "print(f\"Shape after: {df_basic.shape}\")\n",
    "print(f\"\\nRemaining columns:\")\n",
    "print(df_basic.columns.tolist())\n",
    "print(f\"\\nBaseline dataset cleaned - only pre-match features remain\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-match columns to remove from EXTENDED dataset (df_extended)\n",
    "# These include all match statistics and results that are only available after the match\n",
    "post_match_extended = [\n",
    "    # Final match results\n",
    "    'FTHG', 'FTAG', 'FTR',\n",
    "    # Half-time results\n",
    "    'HTHG', 'HTAG', 'HTR',\n",
    "    # Shot statistics\n",
    "    'HS', 'AS', 'HST', 'AST',\n",
    "    # Corners\n",
    "    'HC', 'AC',\n",
    "    # Fouls\n",
    "    'HF', 'AF',\n",
    "    # Cards\n",
    "    'HY', 'AY', 'HR', 'AR',\n",
    "    # Other in-game statistics\n",
    "    'HHW', 'AHW', 'HFKC', 'AFKC', 'HO', 'AO', 'HBP', 'ABP',\n",
    "    # Derived features from post-match data\n",
    "    'total_goals', 'ht_total_goals', 'second_half_goals',\n",
    "    'home_shot_accuracy', 'away_shot_accuracy', 'total_shots', 'total_shots_on_target',\n",
    "    'shot_dominance', 'corner_dominance', 'total_fouls', 'total_cards', 'card_intensity',\n",
    "    # MA5 features calculated from in-game stats (if any exist in df_extended)\n",
    "    'home_shots_ma5', 'away_shots_ma5', 'home_shots_target_ma5', 'away_shots_target_ma5',\n",
    "    'home_corners_ma5', 'away_corners_ma5', 'home_fouls_ma5', 'away_fouls_ma5',\n",
    "    'home_yellows_ma5', 'away_yellows_ma5',\n",
    "]\n",
    "\n",
    "# Remove columns that exist in df_extended\n",
    "cols_to_drop_extended = [col for col in post_match_extended if col in df_extended.columns]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXTENDED DATASET (df_extended) - Removing Post-Match Features\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Shape before: {df_extended.shape}\")\n",
    "print(f\"Columns to remove ({len(cols_to_drop_extended)}): {cols_to_drop_extended}\")\n",
    "\n",
    "df_extended = df_extended.drop(columns=cols_to_drop_extended)\n",
    "\n",
    "print(f\"Shape after: {df_extended.shape}\")\n",
    "print(f\"\\nRemaining columns:\")\n",
    "print(df_extended.columns.tolist())\n",
    "print(f\"\\nExtended dataset cleaned - only pre-match features remain\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Data Leakage Prevention\n",
    "\n",
    "**What was removed:**\n",
    "- **Match results:** FTHG, FTAG, FTR, HTHG, HTAG, HTR\n",
    "- **In-game statistics:** All shots, corners, fouls, cards\n",
    "- **Derived features:** total_goals, shot_accuracy, dominance metrics, card_intensity, etc.\n",
    "\n",
    "**What remains:**\n",
    "- **Pre-match identifiers:** Div, Season, Date, Time, HomeTeam, AwayTeam\n",
    "- **Pre-match context:** league_tier, month, is_weekend, Attendance, Referee\n",
    "- **Historical features:** All MA5 features, position percentiles, goal patterns from PAST seasons\n",
    "- **Time-based features:** days_since_last, promoted/demoted indicators\n",
    "- **Betting odds:** Pre-match odds from bookmakers\n",
    "- **Target variable:** over_2_5 (kept for modeling)\n",
    "\n",
    "**Key principle:** All features in df_basic and df_extended are now calculated exclusively from:\n",
    "1. Information available BEFORE the match starts (dates, teams, league info)\n",
    "2. Historical data from PREVIOUS matches only (MA5, seasonal patterns, positions)\n",
    "3. Pre-match betting odds\n",
    "\n",
    "This ensures no information from the current match leaks into the predictive features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Train/Validation/Test Split by Season\n",
    "\n",
    "Create temporal splits to prevent data leakage. We'll use:\n",
    "- **Training:** 3 oldest seasons (2019/2020, 2020/2021, 2021/2022)\n",
    "- **Validation:** 1 middle season (2022/2023)\n",
    "- **Testing:** 2 most recent seasons (2023/2024, 2024/2025)\n",
    "\n",
    "This ensures the model is trained on past data and evaluated on future data, mimicking real-world deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define season splits\n",
    "train_seasons = ['2019/2020', '2020/2021', '2021/2022']\n",
    "val_seasons = ['2022/2023']\n",
    "test_seasons = ['2023/2024', '2024/2025']\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAIN/VALIDATION/TEST SPLIT BY SEASON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Function to perform split for any dataframe\n",
    "def split_by_season(df, train_seasons, val_seasons, test_seasons, dataset_name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Split dataframe by season with validation and ensure chronological sorting.\n",
    "    \n",
    "    IMPORTANT: Data is sorted by Date to maintain temporal ordering, which is\n",
    "    critical for target encoding with shuffle=False in KFold cross-validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check which seasons exist in the data\n",
    "    available_seasons = df['Season'].unique()\n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    print(f\"Available seasons: {sorted(available_seasons)}\")\n",
    "    \n",
    "    # Create splits\n",
    "    train_mask = df['Season'].isin(train_seasons)\n",
    "    val_mask = df['Season'].isin(val_seasons)\n",
    "    test_mask = df['Season'].isin(test_seasons)\n",
    "    \n",
    "    # Sort by Date to ensure chronological order for target encoding\n",
    "    # Reset index to ensure sequential integer indexing (0, 1, 2, ...)\n",
    "    df_train = df[train_mask].copy().sort_values('Date').reset_index(drop=True)\n",
    "    df_val = df[val_mask].copy().sort_values('Date').reset_index(drop=True)\n",
    "    df_test = df[test_mask].copy().sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # Report split sizes\n",
    "    print(f\"Training seasons: {train_seasons}\")\n",
    "    print(f\"Shape: {df_train.shape}\")\n",
    "    print(f\"Date range: {df_train['Date'].min()} to {df_train['Date'].max()}\")\n",
    "    \n",
    "    print(f\"Validation seasons: {val_seasons}\")\n",
    "    print(f\"Shape: {df_val.shape}\")\n",
    "    print(f\"Date range: {df_val['Date'].min()} to {df_val['Date'].max()}\")\n",
    "    \n",
    "    print(f\"Test seasons: {test_seasons}\")\n",
    "    print(f\"Shape: {df_test.shape}\")\n",
    "    print(f\"Date range: {df_test['Date'].min()} to {df_test['Date'].max()}\")\n",
    "    \n",
    "    # Check for target distribution\n",
    "    if 'over_2_5' in df.columns:\n",
    "        train_rate = df_train['over_2_5'].mean()\n",
    "        val_rate = df_val['over_2_5'].mean()\n",
    "        test_rate = df_test['over_2_5'].mean()\n",
    "        print(f\"Over 2.5 rates: Train={train_rate:.3f}, Val={val_rate:.3f}, Test={test_rate:.3f}\")\n",
    "    \n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "# Split baseline dataset\n",
    "df_basic_train, df_basic_val, df_basic_test = split_by_season(\n",
    "    df_basic, train_seasons, val_seasons, test_seasons, \"BASELINE (df_basic)\"\n",
    ")\n",
    "\n",
    "# Split extended dataset\n",
    "df_extended_train, df_extended_val, df_extended_test = split_by_season(\n",
    "    df_extended, train_seasons, val_seasons, test_seasons, \"EXTENDED (df_extended)\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Temporal split completed successfully\")\n",
    "print(\"Data sorted chronologically and ready for target encoding\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Categorical Encoding\n",
    "\n",
    "## Overview\n",
    "\n",
    "We need to encode categorical variables before they can be used in machine learning models. We use different encoding strategies based on cardinality:\n",
    "\n",
    "### 1. Target Encoding (K-Fold) - For High-Cardinality Variables\n",
    "**Variables:** HomeTeam, AwayTeam, Referee\n",
    "\n",
    "**Why Target Encoding?**\n",
    "- **High cardinality**: These variables have many unique values (hundreds of teams/referees)\n",
    "- **One-hot encoding** would create too many features (curse of dimensionality)\n",
    "- **Label encoding** would impose arbitrary ordering\n",
    "- **Target encoding** captures the relationship with the target variable\n",
    "\n",
    "**K-Fold Cross-Validation Approach:**\n",
    "1. Split training data into K folds (K=5)\n",
    "2. For each fold:\n",
    "   - Calculate target mean for each category using OTHER folds\n",
    "   - Apply smoothing: `(count × mean + smoothing × global_mean) / (count + smoothing)`\n",
    "   - Encode the current fold using these statistics\n",
    "3. For validation/test sets: use full training set statistics\n",
    "\n",
    "**Smoothing Parameter:**\n",
    "- Prevents overfitting to rare categories\n",
    "- Higher smoothing = more regularization (closer to global mean)\n",
    "- We use `smoothing=10.0` as a reasonable default\n",
    "\n",
    "### 2. One-Hot Encoding - For Low-Cardinality Variable\n",
    "**Variable:** Div (League)\n",
    "\n",
    "**Why One-Hot Encoding?**\n",
    "- Low cardinality (~10 unique leagues)\n",
    "- Creates binary indicator columns for each league\n",
    "- No ordinality assumption\n",
    "- Standard approach for categorical variables with few categories\n",
    "\n",
    "### Variables NOT Encoded (Excluded from Features)\n",
    "- **Season**: Used for temporal train/test split - must exclude to prevent leakage\n",
    "- **Date/Time**: Temporal metadata (could extract cyclical features if needed)\n",
    "- **Original categorical columns**: HomeTeam, AwayTeam, Referee, Div (replaced by encoded versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def target_encode_with_kfold(train_data, val_data, test_data, cat_column, target_column='over_2_5', n_folds=5, smoothing=1.0):\n",
    "    \"\"\"\n",
    "    Perform target encoding with K-Fold cross-validation to prevent overfitting.\n",
    "    \n",
    "    IMPORTANT: Uses shuffle=False to maintain temporal ordering for time-series data.\n",
    "    This ensures that encoding statistics are calculated from past data only,\n",
    "    preventing future match information from leaking into past match encodings.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data : DataFrame\n",
    "        Training data (must include both the categorical column and target)\n",
    "        Should be sorted chronologically before passing to this function\n",
    "    val_data : DataFrame  \n",
    "        Validation data (must include the categorical column)\n",
    "    test_data : DataFrame\n",
    "        Test data (must include the categorical column)\n",
    "    cat_column : str\n",
    "        Name of the categorical column to encode\n",
    "    target_column : str\n",
    "        Name of the target column\n",
    "    n_folds : int\n",
    "        Number of folds for cross-validation\n",
    "    smoothing : float\n",
    "        Smoothing parameter to handle rare categories (higher = more regularization)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    train_encoded, val_encoded, test_encoded : Series\n",
    "        Encoded values for train, validation, and test sets\n",
    "    \"\"\"\n",
    "    # Global mean of target (fallback for unseen categories)\n",
    "    global_mean = train_data[target_column].mean()\n",
    "    \n",
    "    # Initialize encoded column for train set\n",
    "    train_encoded = np.zeros(len(train_data))\n",
    "    \n",
    "    # K-Fold cross-validation for training set\n",
    "    # shuffle=False to maintain temporal ordering and prevent data leakage\n",
    "    kf = KFold(n_splits=n_folds, shuffle=False, random_state=42)\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(train_data):\n",
    "        # Split data\n",
    "        train_fold = train_data.iloc[train_idx]\n",
    "        val_fold = train_data.iloc[val_idx]\n",
    "        \n",
    "        # Calculate target mean for each category in train fold\n",
    "        target_means = train_fold.groupby(cat_column)[target_column].agg(['mean', 'count'])\n",
    "        \n",
    "        # Apply smoothing: (count * mean + smoothing * global_mean) / (count + smoothing)\n",
    "        target_means['smoothed_mean'] = (\n",
    "            (target_means['count'] * target_means['mean'] + smoothing * global_mean) /\n",
    "            (target_means['count'] + smoothing)\n",
    "        )\n",
    "        \n",
    "        # Map to validation fold\n",
    "        train_encoded[val_idx] = val_fold[cat_column].map(target_means['smoothed_mean']).fillna(global_mean).values\n",
    "    \n",
    "    # For validation and test sets, use full training set statistics\n",
    "    full_target_means = train_data.groupby(cat_column)[target_column].agg(['mean', 'count'])\n",
    "    full_target_means['smoothed_mean'] = (\n",
    "        (full_target_means['count'] * full_target_means['mean'] + smoothing * global_mean) /\n",
    "        (full_target_means['count'] + smoothing)\n",
    "    )\n",
    "    \n",
    "    # Encode validation and test sets\n",
    "    val_encoded = val_data[cat_column].map(full_target_means['smoothed_mean']).fillna(global_mean)\n",
    "    test_encoded = test_data[cat_column].map(full_target_means['smoothed_mean']).fillna(global_mean)\n",
    "    \n",
    "    return train_encoded, val_encoded, test_encoded\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CATEGORICAL ENCODING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create copies to avoid modifying original data\n",
    "df_basic_train_encoded = df_basic_train.copy()\n",
    "df_basic_val_encoded = df_basic_val.copy()\n",
    "df_basic_test_encoded = df_basic_test.copy()\n",
    "\n",
    "df_extended_train_encoded = df_extended_train.copy()\n",
    "df_extended_val_encoded = df_extended_val.copy()\n",
    "df_extended_test_encoded = df_extended_test.copy()\n",
    "\n",
    "# Part 1: TARGET ENCODING for high-cardinality variables\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 1: TARGET ENCODING (K-FOLD) FOR HIGH-CARDINALITY VARIABLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "categorical_vars = ['HomeTeam', 'AwayTeam', 'Referee']\n",
    "\n",
    "# Encode each categorical variable for both datasets\n",
    "for cat_var in categorical_vars:\n",
    "    print(f\"\\n{cat_var}:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Check if variable exists in datasets\n",
    "    if cat_var not in df_basic_train.columns:\n",
    "        print(f\"{cat_var} not found in dataset, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # BASELINE dataset encoding\n",
    "    print(f\"BASELINE:\")\n",
    "    print(f\"Unique values in train: {df_basic_train[cat_var].nunique()}\")\n",
    "    print(f\"Unique values in val: {df_basic_val[cat_var].nunique()}\")\n",
    "    print(f\"Unique values in test: {df_basic_test[cat_var].nunique()}\")\n",
    "    \n",
    "    train_enc, val_enc, test_enc = target_encode_with_kfold(\n",
    "        df_basic_train_encoded, \n",
    "        df_basic_val_encoded, \n",
    "        df_basic_test_encoded,\n",
    "        cat_var,\n",
    "        n_folds=5,\n",
    "        smoothing=10.0\n",
    "    )\n",
    "    \n",
    "    # Add encoded columns\n",
    "    df_basic_train_encoded[f'{cat_var}_encoded'] = train_enc\n",
    "    df_basic_val_encoded[f'{cat_var}_encoded'] = val_enc\n",
    "    df_basic_test_encoded[f'{cat_var}_encoded'] = test_enc\n",
    "    \n",
    "    print(f\"Encoded range: [{train_enc.min():.4f}, {train_enc.max():.4f}]\")\n",
    "    print(f\"Encoded mean: {train_enc.mean():.4f}\")\n",
    "    \n",
    "    # EXTENDED dataset encoding\n",
    "    print(f\"EXTENDED:\")\n",
    "    print(f\"Unique values in train: {df_extended_train[cat_var].nunique()}\")\n",
    "    print(f\"Unique values in val: {df_extended_val[cat_var].nunique()}\")\n",
    "    print(f\"Unique values in test: {df_extended_test[cat_var].nunique()}\")\n",
    "    \n",
    "    train_enc, val_enc, test_enc = target_encode_with_kfold(\n",
    "        df_extended_train_encoded,\n",
    "        df_extended_val_encoded,\n",
    "        df_extended_test_encoded,\n",
    "        cat_var,\n",
    "        n_folds=5,\n",
    "        smoothing=10.0\n",
    "    )\n",
    "    \n",
    "    # Add encoded columns\n",
    "    df_extended_train_encoded[f'{cat_var}_encoded'] = train_enc\n",
    "    df_extended_val_encoded[f'{cat_var}_encoded'] = val_enc\n",
    "    df_extended_test_encoded[f'{cat_var}_encoded'] = test_enc\n",
    "    \n",
    "    print(f\"Encoded range: [{train_enc.min():.4f}, {train_enc.max():.4f}]\")\n",
    "    print(f\"Encoded mean: {train_enc.mean():.4f}\")\n",
    "\n",
    "print(\"\nTarget encoding completed\")\n",
    "\n",
    "\n",
    "# Part 2: ONE-HOT ENCODING for low-cardinality variable (Div)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 2: ONE-HOT ENCODING FOR DIV (LEAGUE)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check Div cardinality\n",
    "print(f\"\\nDiv (League) Analysis:\")\n",
    "print(f\"BASELINE - Unique leagues in train: {df_basic_train['Div'].nunique()}\")\n",
    "print(f\"BASELINE - Leagues: {sorted(df_basic_train['Div'].unique())}\")\n",
    "print(f\"\\nEXTENDED - Unique leagues in train: {df_extended_train['Div'].nunique()}\")\n",
    "print(f\"EXTENDED - Leagues: {sorted(df_extended_train['Div'].unique())}\")\n",
    "\n",
    "# One-hot encode Div for BASELINE dataset\n",
    "print(f\"\\nOne-hot encoding Div for BASELINE:\")\n",
    "div_train_baseline = pd.get_dummies(df_basic_train_encoded['Div'], prefix='Div', drop_first=False)\n",
    "div_val_baseline = pd.get_dummies(df_basic_val_encoded['Div'], prefix='Div', drop_first=False)\n",
    "div_test_baseline = pd.get_dummies(df_basic_test_encoded['Div'], prefix='Div', drop_first=False)\n",
    "\n",
    "# Align columns (in case val/test have missing categories)\n",
    "all_div_cols = sorted(set(div_train_baseline.columns) | set(div_val_baseline.columns) | set(div_test_baseline.columns))\n",
    "for col in all_div_cols:\n",
    "    if col not in div_train_baseline.columns:\n",
    "        div_train_baseline[col] = 0\n",
    "    if col not in div_val_baseline.columns:\n",
    "        div_val_baseline[col] = 0\n",
    "    if col not in div_test_baseline.columns:\n",
    "        div_test_baseline[col] = 0\n",
    "\n",
    "div_train_baseline = div_train_baseline[all_div_cols]\n",
    "div_val_baseline = div_val_baseline[all_div_cols]\n",
    "div_test_baseline = div_test_baseline[all_div_cols]\n",
    "\n",
    "# Add to dataframes\n",
    "df_basic_train_encoded = pd.concat([df_basic_train_encoded, div_train_baseline], axis=1)\n",
    "df_basic_val_encoded = pd.concat([df_basic_val_encoded, div_val_baseline], axis=1)\n",
    "df_basic_test_encoded = pd.concat([df_basic_test_encoded, div_test_baseline], axis=1)\n",
    "\n",
    "print(f\"Created {len(all_div_cols)} dummy columns: {all_div_cols}\")\n",
    "\n",
    "# One-hot encode Div for EXTENDED dataset\n",
    "print(f\"\\nOne-hot encoding Div for EXTENDED:\")\n",
    "div_train_extended = pd.get_dummies(df_extended_train_encoded['Div'], prefix='Div', drop_first=False)\n",
    "div_val_extended = pd.get_dummies(df_extended_val_encoded['Div'], prefix='Div', drop_first=False)\n",
    "div_test_extended = pd.get_dummies(df_extended_test_encoded['Div'], prefix='Div', drop_first=False)\n",
    "\n",
    "# Align columns\n",
    "all_div_cols_ext = sorted(set(div_train_extended.columns) | set(div_val_extended.columns) | set(div_test_extended.columns))\n",
    "for col in all_div_cols_ext:\n",
    "    if col not in div_train_extended.columns:\n",
    "        div_train_extended[col] = 0\n",
    "    if col not in div_val_extended.columns:\n",
    "        div_val_extended[col] = 0\n",
    "    if col not in div_test_extended.columns:\n",
    "        div_test_extended[col] = 0\n",
    "\n",
    "div_train_extended = div_train_extended[all_div_cols_ext]\n",
    "div_val_extended = div_val_extended[all_div_cols_ext]\n",
    "div_test_extended = div_test_extended[all_div_cols_ext]\n",
    "\n",
    "# Add to dataframes\n",
    "df_extended_train_encoded = pd.concat([df_extended_train_encoded, div_train_extended], axis=1)\n",
    "df_extended_val_encoded = pd.concat([df_extended_val_encoded, div_val_extended], axis=1)\n",
    "df_extended_test_encoded = pd.concat([df_extended_test_encoded, div_test_extended], axis=1)\n",
    "\n",
    "print(f\"Created {len(all_div_cols_ext)} dummy columns: {all_div_cols_ext}\")\n",
    "\n",
    "print(\"\nOne-hot encoding completed\")\n",
    "\n",
    "\n",
    "# Part 3: DROP ORIGINAL CATEGORICAL COLUMNS\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 3: REMOVING ORIGINAL CATEGORICAL COLUMNS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Drop original categorical columns since we now have encoded versions\n",
    "cols_to_drop = ['HomeTeam', 'AwayTeam', 'Referee', 'Div']\n",
    "\n",
    "# BASELINE datasets\n",
    "for col in cols_to_drop:\n",
    "    if col in df_basic_train_encoded.columns:\n",
    "        df_basic_train_encoded = df_basic_train_encoded.drop(columns=[col])\n",
    "        df_basic_val_encoded = df_basic_val_encoded.drop(columns=[col])\n",
    "        df_basic_test_encoded = df_basic_test_encoded.drop(columns=[col])\n",
    "        print(f\"Dropped {col} from BASELINE datasets\")\n",
    "\n",
    "# EXTENDED datasets\n",
    "for col in cols_to_drop:\n",
    "    if col in df_extended_train_encoded.columns:\n",
    "        df_extended_train_encoded = df_extended_train_encoded.drop(columns=[col])\n",
    "        df_extended_val_encoded = df_extended_val_encoded.drop(columns=[col])\n",
    "        df_extended_test_encoded = df_extended_test_encoded.drop(columns=[col])\n",
    "        print(f\"Dropped {col} from EXTENDED datasets\")\n",
    "\n",
    "print(\"\nOriginal categorical columns removed\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CATEGORICAL ENCODING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"BASELINE datasets:\")\n",
    "print(f\"Train shape: {df_basic_train_encoded.shape}\")\n",
    "print(f\"Val shape: {df_basic_val_encoded.shape}\")\n",
    "print(f\"Test shape: {df_basic_test_encoded.shape}\")\n",
    "print(f\"\\nEXTENDED datasets:\")\n",
    "print(f\"Train shape: {df_extended_train_encoded.shape}\")\n",
    "print(f\"Val shape: {df_extended_val_encoded.shape}\")\n",
    "print(f\"Test shape: {df_extended_test_encoded.shape}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Create Feature Matrices with Encoded Variables\n",
    "\n",
    "Now prepare the final feature matrices including the encoded categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now update the feature preparation to include encoded variables\n",
    "print(\"=\" * 70)\n",
    "print(\"PREPARING FINAL FEATURE MATRICES WITH ENCODED VARIABLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Update the exclude columns list (original columns, not encoded versions)\n",
    "exclude_cols = ['Div', 'Season', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'Referee', 'over_2_5']\n",
    "\n",
    "# BASELINE dataset\n",
    "feature_cols_basic = [col for col in df_basic_train_encoded.columns if col not in exclude_cols]\n",
    "print(f\"\\nBASELINE:\")\n",
    "print(f\"Total features: {len(feature_cols_basic)}\")\n",
    "print(f\"Encoded features: {[col for col in feature_cols_basic if '_encoded' in col]}\")\n",
    "\n",
    "X_basic_train = df_basic_train_encoded[feature_cols_basic].copy()\n",
    "X_basic_val = df_basic_val_encoded[feature_cols_basic].copy()\n",
    "X_basic_test = df_basic_test_encoded[feature_cols_basic].copy()\n",
    "\n",
    "y_basic_train = df_basic_train_encoded['over_2_5'].copy()\n",
    "y_basic_val = df_basic_val_encoded['over_2_5'].copy()\n",
    "y_basic_test = df_basic_test_encoded['over_2_5'].copy()\n",
    "\n",
    "print(f\"X_train shape: {X_basic_train.shape}\")\n",
    "print(f\"X_val shape: {X_basic_val.shape}\")\n",
    "print(f\"X_test shape: {X_basic_test.shape}\")\n",
    "\n",
    "# EXTENDED dataset\n",
    "feature_cols_extended = [col for col in df_extended_train_encoded.columns if col not in exclude_cols]\n",
    "print(f\"\\nEXTENDED:\")\n",
    "print(f\"Total features: {len(feature_cols_extended)}\")\n",
    "print(f\"Encoded features: {[col for col in feature_cols_extended if '_encoded' in col]}\")\n",
    "\n",
    "X_extended_train = df_extended_train_encoded[feature_cols_extended].copy()\n",
    "X_extended_val = df_extended_val_encoded[feature_cols_extended].copy()\n",
    "X_extended_test = df_extended_test_encoded[feature_cols_extended].copy()\n",
    "\n",
    "y_extended_train = df_extended_train_encoded['over_2_5'].copy()\n",
    "y_extended_val = df_extended_val_encoded['over_2_5'].copy()\n",
    "y_extended_test = df_extended_test_encoded['over_2_5'].copy()\n",
    "\n",
    "print(f\"X_train shape: {X_extended_train.shape}\")\n",
    "print(f\"X_val shape: {X_extended_val.shape}\")\n",
    "print(f\"X_test shape: {X_extended_test.shape}\")\n",
    "\n",
    "# Verify no categorical columns remain\n",
    "print(f\"\\nVerification:\")\n",
    "categorical_basic = X_basic_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "categorical_extended = X_extended_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "if categorical_basic:\n",
    "    print(f\"WARNING: Categorical features remain in BASELINE: {categorical_basic}\")\n",
    "else:\n",
    "    print(f\"BASELINE: All features are numeric\")\n",
    "\n",
    "if categorical_extended:\n",
    "    print(f\"WARNING: Categorical features remain in EXTENDED: {categorical_extended}\")\n",
    "else:\n",
    "    print(f\"EXTENDED: All features are numeric\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Final feature matrices ready for modeling\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 Save Preprocessed Data to Pickle Files\n",
    "\n",
    "Save the final preprocessed datasets (both baseline and extended) as pickle files for easy loading in the modeling notebook.\n",
    "\n",
    "**What's being saved:**\n",
    "- Feature matrices: `X_train`, `X_val`, `X_test`\n",
    "- Target vectors: `y_train`, `y_val`, `y_test`\n",
    "- Feature names: List of all feature column names\n",
    "- Metadata: Information about the preprocessing (dates, sizes, encoded features, etc.)\n",
    "\n",
    "**Output files:**\n",
    "- `processed/baseline_preprocessed.pkl` - Baseline feature set\n",
    "- `processed/extended_preprocessed.pkl` - Extended feature set with rolling statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create processed directory if it doesn't exist\n",
    "output_dir = 'processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# Save BASELINE dataset\n",
    "\n",
    "print(\"\\nSaving BASELINE dataset...\")\n",
    "\n",
    "baseline_data = {\n",
    "    'X_train': X_basic_train,\n",
    "    'X_val': X_basic_val,\n",
    "    'X_test': X_basic_test,\n",
    "    'y_train': y_basic_train,\n",
    "    'y_val': y_basic_val,\n",
    "    'y_test': y_basic_test,\n",
    "    'feature_names': feature_cols_basic,\n",
    "}\n",
    "\n",
    "baseline_filename = f'{output_dir}/baseline_preprocessed.pkl'\n",
    "with open(baseline_filename, 'wb') as f:\n",
    "    pickle.dump(baseline_data, f)\n",
    "\n",
    "print(f\"Saved to: {baseline_filename}\")\n",
    "print(f\"Train shape: {X_basic_train.shape}\")\n",
    "print(f\"Val shape: {X_basic_val.shape}\")\n",
    "print(f\"Test shape: {X_basic_test.shape}\")\n",
    "print(f\"Features: {len(feature_cols_basic)}\")\n",
    "\n",
    "\n",
    "# Save EXTENDED dataset\n",
    "\n",
    "print(\"\\nSaving EXTENDED dataset...\")\n",
    "\n",
    "extended_data = {\n",
    "    'X_train': X_extended_train,\n",
    "    'X_val': X_extended_val,\n",
    "    'X_test': X_extended_test,\n",
    "    'y_train': y_extended_train,\n",
    "    'y_val': y_extended_val,\n",
    "    'y_test': y_extended_test,\n",
    "    'feature_names': feature_cols_extended,\n",
    "}\n",
    "\n",
    "extended_filename = f'{output_dir}/extended_preprocessed.pkl'\n",
    "with open(extended_filename, 'wb') as f:\n",
    "    pickle.dump(extended_data, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NBksARdgdMkP",
    "Qx3gpH8TdToi",
    "O5SNQWIMhxZA",
    "NNyVnO2Bk4zo",
    "XMiRfCrVzDVd",
    "fwwBVM5ex2SA"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}