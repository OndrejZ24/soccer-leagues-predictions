{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML2 Semestral Project - Football O/U 2.5\n",
    "**Authors:** Phuong Nhi Tranová, Vít Maruniak, Šimon Slánský, Radim Škoukal, Ondřej Zetek, Martin Kareš, Jan Korčák, Jakub Maličkay, Jáchym Janouch  \n",
    "**Course:** FIS 4IT344 Machine Learning 2 (2025/2026)  \n",
    "**Goal:** Compare baseline (current features) vs extended (richer features) models for O/U 2.5 goals across markets; translate accuracy gains into optimal profit and **maximum data subscription price per country** *.  \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "***maximum data subscription price per country**\n",
    "- the most money our company should be willing to pay for that country's additional data\n",
    "- that's how much extra profit the improved model generates\n",
    "- baseline model → accuracy = A₀\n",
    "    - Generates profit Π*(A₀)\n",
    "- extended model → accuracy = A₁\n",
    "    - Generates profit Π*(A₁)\n",
    "- profit improvement = ΔΠ = Π(A₁) − Π(A₀)*\n",
    "    - basically how much more money the comany earns each year by using the better data\n",
    "- the maximum data subscription price per country = ΔΠ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports and paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore, chi2_contingency\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Library parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "OUTPUT_DIR = f\"./processed\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_matches(data_dir: str) -> pd.DataFrame:\n",
    "    csv_files = glob.glob(os.path.join(data_dir, \"**\", \"*.csv\"), recursive=True)\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSV files found under {data_dir}\")\n",
    "\n",
    "    frames = []\n",
    "    for fp in csv_files:\n",
    "        # extract path info\n",
    "        rel = os.path.relpath(fp, data_dir)\n",
    "        parts = Path(rel).parts\n",
    "        country = parts[0] if len(parts) >= 1 else None\n",
    "        league  = parts[1] if len(parts) >= 2 else None\n",
    "        season_file = parts[2] if len(parts) >= 3 else None\n",
    "        season_code = os.path.splitext(season_file)[0] if season_file else None\n",
    "\n",
    "        # read and rename\n",
    "        try:\n",
    "            df = pd.read_csv(fp, low_memory=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {fp}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Format season as YYYY/YYYY format\n",
    "        if season_code and len(season_code) == 4 and season_code.isdigit():\n",
    "            # Handle formats like \"1920\" or \"2021\"\n",
    "            year1 = int(season_code[:2])\n",
    "            year2 = int(season_code[2:])\n",
    "\n",
    "            # Determine century based on year range\n",
    "            if year1 >= 19 and year1 <= 24:  # 19-24 maps to 2019-2024\n",
    "                year1_full = 2000 + year1\n",
    "            else:\n",
    "                year1_full = 1900 + year1\n",
    "\n",
    "            if year2 >= 19 and year2 <= 99:\n",
    "                if year2 < year1:  # Next year (e.g., 19->20, 23->24)\n",
    "                    year2_full = 2000 + year2\n",
    "                else:\n",
    "                    year2_full = 2000 + year2\n",
    "            else:\n",
    "                year2_full = 1900 + year2\n",
    "\n",
    "            season_formatted = f\"{year1_full}/{year2_full}\"\n",
    "        else:\n",
    "            season_formatted = season_code  # Fallback to original if format is unexpected\n",
    "\n",
    "        # Add Season column right after Div (if Div exists)\n",
    "        if 'Div' in df.columns:\n",
    "            div_idx = df.columns.get_loc('Div')\n",
    "            df.insert(div_idx + 1, 'Season', season_formatted)\n",
    "        else:\n",
    "            df['Season'] = season_formatted\n",
    "\n",
    "        frames.append(df)\n",
    "\n",
    "    all_df = pd.concat(frames, ignore_index=True, sort=False)\n",
    "    return all_df\n",
    "\n",
    "# run the loader\n",
    "all_matches = pd.DataFrame(load_all_matches(DATA_DIR))\n",
    "print(all_matches.columns.tolist())\n",
    "print(all_matches.shape)\n",
    "display(all_matches.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis\n",
    "\n",
    "Before proceeding with data cleaning, let's understand our data better through comprehensive exploratory data analysis. This will help us make informed decisions about preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Shape and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset shape: {all_matches.shape}\")\n",
    "print(f\"Number of seasons/countries covered:\")\n",
    "print(f\"Countries: {all_matches['Div'].str[:-1].nunique()}\")\n",
    "print(f\"Leagues: {all_matches['Div'].nunique()}\")\n",
    "print(f\"Date range: {all_matches['Date'].min()} to {all_matches['Date'].max()}\")\n",
    "\n",
    "# Check basic statistics\n",
    "print(f\"\\nBasic goal statistics:\")\n",
    "print(f\"Total goals per match stats:\")\n",
    "total_goals = all_matches['FTHG'] + all_matches['FTAG']\n",
    "print(total_goals.describe())\n",
    "\n",
    "print(f\"\\nOver/Under 2.5 goals distribution:\")\n",
    "over_2_5 = (total_goals > 2.5).astype(int)\n",
    "print(f\"Over 2.5: {over_2_5.sum()} ({over_2_5.mean():.2%})\")\n",
    "print(f\"Under 2.5: {(~over_2_5.astype(bool)).sum()} ({(1-over_2_5.mean()):.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our target variable (Over/Under 2.5 goals) is perfectly balanced with almost exactly 50/50 split, which is ideal for classification. Mainly because the model won't be biased toward either class and we can use standard accuracy but also because we won't have to do any kind of resampling or rebalancing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed missing values analysis\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'column': all_matches.columns,\n",
    "    'missing_count': all_matches.isnull().sum(),\n",
    "    'missing_percentage': (all_matches.isnull().sum() / len(all_matches)) * 100,\n",
    "    'dtype': all_matches.dtypes\n",
    "})\n",
    "\n",
    "# Filter to show only columns with missing values\n",
    "missing_analysis = missing_analysis[missing_analysis['missing_count'] > 0].sort_values('missing_percentage', ascending=False)\n",
    "\n",
    "print(f\"Columns with missing values: {len(missing_analysis)}\")\n",
    "print(f\"Total columns: {len(all_matches.columns)}\")\n",
    "print(f\"\\nTop 20 columns with highest missing percentage:\")\n",
    "display(missing_analysis.head(20))\n",
    "\n",
    "# Check missing patterns in key variables\n",
    "key_stats = ['HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "print(f\"\\nMissing data in key match statistics:\")\n",
    "for stat in key_stats:\n",
    "    if stat in all_matches.columns:\n",
    "        missing_pct = (all_matches[stat].isnull().sum() / len(all_matches)) * 100\n",
    "        print(f\"{stat}: {missing_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing data analysis reveals that:\n",
    "1. **Betting odds** have the highest missing percentages (80%+) - this is expected as not all bookmakers operate in all leagues/seasons\n",
    "2. **Key match statistics** (shots, corners, fouls, cards) have very low missing rates (<0.1%), which is excellent for our modeling\n",
    "3. Most missing data is in betting-related columns, which we can handle appropriately\n",
    "\n",
    "also we have found 4 unnamed columns that are 100% missing. they're most likely artifacts from csv exports so they're definitely safe to drop outright"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets do a bit more of a in depth analysis, shall we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = all_matches.copy()\n",
    "\n",
    "# missingness flag\n",
    "stats_cols = ['HS','AS','HST','AST','HF','AF','HC','AC','HY','AY','HR','AR']\n",
    "for c in stats_cols:\n",
    "    if c in raw.columns:\n",
    "        raw[f'isna_{c}'] = raw[c].isna().astype(int)\n",
    "\n",
    "# Row-level summary: how many of the 12 stats are missing in the same row?\n",
    "flag_cols = [f'isna_{c}' for c in stats_cols if f'isna_{c}' in raw.columns]\n",
    "raw['missing_count_stats'] = raw[flag_cols].sum(axis=1)\n",
    "\n",
    "# Quick overview\n",
    "print(raw['missing_count_stats'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the rows seem to have no missigness/ However, there are 41 rows that have are missing all 12 variables, which seems pretty clustered. Suggesting that the missing data likely stem from a specific data source or a batch issue rather than random omission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single-stat missing % (already computed as flags)\n",
    "single_rates = (raw[flag_cols].mean() * 100)\n",
    "single_rates.index = [c.replace('isna_', '') for c in single_rates.index]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "ax.bar(single_rates.index, single_rates.values)\n",
    "ax.set_title('Missingness by variables (%)')\n",
    "ax.set_ylabel('% missing')\n",
    "ax.set_xlabel('stat')\n",
    "ax.set_xticklabels(single_rates.index, rotation=45, ha='right')\n",
    "for i, v in enumerate(single_rates.values):\n",
    "    ax.text(i, v, f'{v:.3f}%', ha='center', va='bottom', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "misigness seems uniformly low across all variables, there seems to be no issue with a variable specific collection issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "cmap = 'plasma'\n",
    "\n",
    "# Extract country from Div column (e.g., 'E1' -> 'E', 'SP2' -> 'SP')\n",
    "raw['country_code'] = raw['Div'].str[:-1]\n",
    "\n",
    "# 1️⃣ Country × Stat\n",
    "if 'country_code' in raw.columns:\n",
    "    M1 = raw.groupby('country_code')[flag_cols].mean().mul(100)\n",
    "    order = M1.mean(axis=1).sort_values(ascending=False).index\n",
    "    M1 = M1.loc[order]\n",
    "    M1.columns = [c.replace('isna_', '') for c in M1.columns]\n",
    "\n",
    "    im1 = axes[0].imshow(M1.values, aspect='auto', cmap=cmap)\n",
    "    axes[0].set_xticks(np.arange(M1.shape[1]))\n",
    "    axes[0].set_xticklabels(M1.columns, rotation=45, ha='right')\n",
    "    axes[0].set_yticks(np.arange(M1.shape[0]))\n",
    "    axes[0].set_yticklabels(M1.index)\n",
    "    axes[0].set_title('Country × Stat')\n",
    "    fig.colorbar(im1, ax=axes[0], label='% missing')\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, \"Missing 'Div' column\", ha='center', va='center')\n",
    "    axes[0].set_axis_off()\n",
    "\n",
    "# 2️⃣ Year × Stat (using Date column)\n",
    "if 'Date' in raw.columns:\n",
    "    raw['year'] = pd.to_datetime(raw['Date']).dt.year\n",
    "    M2 = raw.groupby('year')[flag_cols].mean().mul(100)\n",
    "    order = M2.mean(axis=1).sort_values(ascending=False).index\n",
    "    M2 = M2.loc[order]\n",
    "    M2.columns = [c.replace('isna_', '') for c in M2.columns]\n",
    "\n",
    "    im2 = axes[1].imshow(M2.values, aspect='auto', cmap=cmap)\n",
    "    axes[1].set_xticks(np.arange(M2.shape[1]))\n",
    "    axes[1].set_xticklabels(M2.columns, rotation=45, ha='right')\n",
    "    axes[1].set_yticks(np.arange(M2.shape[0]))\n",
    "    axes[1].set_yticklabels(M2.index.astype(int))\n",
    "    axes[1].set_title('Year × Stat')\n",
    "    fig.colorbar(im2, ax=axes[1], label='% missing')\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, \"Missing 'Date' column\", ha='center', va='center')\n",
    "    axes[1].set_axis_off()\n",
    "\n",
    "# 3️⃣ Year × Country\n",
    "needed = {'year', 'country_code'}\n",
    "if needed.issubset(raw.columns):\n",
    "    G = raw.groupby(['year','country_code'])[flag_cols].mean().mul(100)\n",
    "    G['avg_missing'] = G.mean(axis=1)\n",
    "    year_order  = G['avg_missing'].groupby(level=0).mean().sort_values(ascending=False).index\n",
    "    country_order = G['avg_missing'].groupby(level=1).mean().sort_values(ascending=False).index\n",
    "    P3 = (G['avg_missing'].unstack('country_code')\n",
    "          .reindex(index=year_order, columns=country_order)\n",
    "          .fillna(0))\n",
    "\n",
    "    im3 = axes[2].imshow(P3.values, aspect='auto', cmap=cmap)\n",
    "    axes[2].set_xticks(np.arange(P3.shape[1]))\n",
    "    axes[2].set_xticklabels(P3.columns, rotation=45, ha='right')\n",
    "    axes[2].set_yticks(np.arange(P3.shape[0]))\n",
    "    axes[2].set_yticklabels(P3.index.astype(int))\n",
    "    axes[2].set_title('Year × Country')\n",
    "    fig.colorbar(im3, ax=axes[2], label='% missing')\n",
    "else:\n",
    "    axes[2].text(0.5, 0.5, \"Missing required columns\", ha='center', va='center')\n",
    "    axes[2].set_axis_off()\n",
    "\n",
    "# 4️⃣ Country × League\n",
    "needed = {'country_code', 'Div'}\n",
    "if needed.issubset(raw.columns):\n",
    "    G = raw.groupby(['country_code', 'Div'])[flag_cols].mean().mul(100)\n",
    "    G['avg_missing'] = G.mean(axis=1)\n",
    "    P4 = (G['avg_missing'].unstack('Div').fillna(0))\n",
    "    country_order = P4.mean(axis=1).sort_values(ascending=False).index\n",
    "    league_order  = P4.mean(axis=0).sort_values(ascending=False).index\n",
    "    P4 = P4.loc[country_order, league_order]\n",
    "\n",
    "    im4 = axes[3].imshow(P4.values, aspect='auto', cmap=cmap)\n",
    "    axes[3].set_xticks(np.arange(P4.shape[1]))\n",
    "    axes[3].set_xticklabels(P4.columns, rotation=45, ha='right')\n",
    "    axes[3].set_yticks(np.arange(P4.shape[0]))\n",
    "    axes[3].set_yticklabels(P4.index)\n",
    "    axes[3].set_title('Country × League')\n",
    "    fig.colorbar(im4, ax=axes[3], label='% missing')\n",
    "else:\n",
    "    axes[3].text(0.5, 0.5, \"Missing required columns\", ha='center', va='center')\n",
    "    axes[3].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first heatmap shows missing data by country. Turkey has the most missing data by far, with over 1.4 percent missing on average. All other countries have very little missing data, less than 0.5 percent each.\n",
    "\n",
    "The second heatmap shows missing data by year. The years 2023 has slightly more missing data than the other years.\n",
    "\n",
    "The third heatmap combines year and country together. It shows that Turkey has most missing values in 2023. In other years, the missingness is not so bad.\n",
    "\n",
    "The fourth heatmap shows missing data by country and league division. Again, Turkey stands out with the highest missing data. Within each country, different league divisions have similar amounts of missing data, which means the problem is more about the country than about which league tier we look at.\n",
    "\n",
    "Overall, the missing data is not random. It is concentrated mainly in Turkey and in the year 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_cols  = [f'isna_{c}' for c in stats_cols if f'isna_{c}' in raw.columns]\n",
    "top_n      = 15\n",
    "min_matches_ref = 50   # ignore refs with tiny sample sizes\n",
    "\n",
    "def group_missing_rate(df, key):\n",
    "    \"\"\"Return DataFrame with avg % missing across 12 stats, plus counts.\"\"\"\n",
    "    grp = df.groupby(key)[flag_cols]\n",
    "    rate = grp.mean().mul(100).mean(axis=1)\n",
    "    cnt  = df.groupby(key).size()\n",
    "    out  = pd.DataFrame({'rate': rate, 'n': cnt}).sort_values('rate', ascending=False)\n",
    "    return out\n",
    "\n",
    "# 1️⃣ Home, Away, Referee\n",
    "home_df = group_missing_rate(raw, 'HomeTeam') if 'HomeTeam' in raw.columns else pd.DataFrame()\n",
    "away_df = group_missing_rate(raw, 'AwayTeam') if 'AwayTeam' in raw.columns else pd.DataFrame()\n",
    "ref_df  = group_missing_rate(raw, 'Referee')  if 'Referee'  in raw.columns else pd.DataFrame()\n",
    "if not ref_df.empty:\n",
    "    ref_df = ref_df[ref_df['n'] >= min_matches_ref].sort_values('rate', ascending=False)\n",
    "\n",
    "# 2️⃣ Merge for Home vs Away comparison (teams present in both)\n",
    "both = pd.DataFrame()\n",
    "if not home_df.empty and not away_df.empty:\n",
    "    both = (home_df[['rate']].rename(columns={'rate': 'home_rate'})\n",
    "            .merge(away_df[['rate']], left_index=True, right_index=True, how='inner')\n",
    "            .rename(columns={'rate': 'away_rate'}))\n",
    "    both['diff'] = both['home_rate'] - both['away_rate']\n",
    "    both = both.sort_values('home_rate', ascending=False).head(top_n)\n",
    "\n",
    "# =======================\n",
    "# FIGURE 1 — Home & Away\n",
    "# =======================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# A) Top Home teams\n",
    "if not home_df.empty:\n",
    "    htop = home_df.head(top_n)[::-1]\n",
    "    axes[0].barh(htop.index.astype(str), htop['rate'].values, color='#8c564b')\n",
    "    axes[0].set_title('Missingness by HomeTeam (avg % across stats)')\n",
    "    axes[0].set_xlabel('% missing')\n",
    "    for y, (r, n) in enumerate(zip(htop['rate'].values, htop['n'].values)):\n",
    "        axes[0].text(r, y, f'  {r:.2f}% (n={n})', va='center', ha='left', fontsize=9)\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, \"HomeTeam column not found\", ha='center', va='center')\n",
    "    axes[0].set_axis_off()\n",
    "\n",
    "# B) Top Away teams\n",
    "if not away_df.empty:\n",
    "    atop = away_df.head(top_n)[::-1]\n",
    "    axes[1].barh(atop.index.astype(str), atop['rate'].values, color='#1f77b4')\n",
    "    axes[1].set_title('Missingness by AwayTeam (avg % across stats)')\n",
    "    axes[1].set_xlabel('% missing')\n",
    "    for y, (r, n) in enumerate(zip(atop['rate'].values, atop['n'].values)):\n",
    "        axes[1].text(r, y, f'  {r:.2f}% (n={n})', va='center', ha='left', fontsize=9)\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, \"AwayTeam column not found\", ha='center', va='center')\n",
    "    axes[1].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization compares the average percentage of missing match statistics for each team when playing at home (brown dots) versus away (blue dots). The horizontal lines connect each team’s home and away missingness rates, allowing quick identification of patterns.\n",
    "\n",
    "Most teams show very little difference between home and away games, suggesting that data gaps are not related to the venue. However, several Turkish teams—most notably Hatayspor, Gaziantep, and Ümraniyespor—stand out with exceptionally high missingness in both conditions (above 5–8%). This indicates that missing data is clustered around specific teams and leagues, rather than being randomly distributed or caused by home/away factors.\n",
    "\n",
    "Overall, the visualization reinforces that the missingness originates from systematic collection or feed issues affecting particular teams or competitions, rather than isolated recording errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missigness for referees\n",
    "if not ref_df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    rtop = ref_df.head(top_n)[::-1]\n",
    "    ax.barh(rtop.index.astype(str), rtop['rate'].values, color='#9467bd')\n",
    "    ax.set_title(f'Missingness by Referee (avg % across stats, n≥{min_matches_ref})')\n",
    "    ax.set_xlabel('% missing')\n",
    "    for y, (r, n) in enumerate(zip(rtop['rate'].values, rtop['n'].values)):\n",
    "        ax.text(r, y, f'  {r:.2f}% (n={n})', va='center', ha='left', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No referees pass the sample-size filter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper: % missing by group (avg across rows)\n",
    "def pct_missing_by(group_cols, cols):\n",
    "    G = raw.groupby(group_cols)[cols].mean().mul(100)   # % per stat\n",
    "    return G\n",
    "\n",
    "# =========================\n",
    "# Figure A — Year × Stat\n",
    "# =========================\n",
    "if 'year' in raw.columns:\n",
    "    YS = pct_missing_by(['year'], flag_cols)\n",
    "    # order years by overall missingness (desc)\n",
    "    order = YS.mean(axis=1).sort_values(ascending=False).index\n",
    "    YS = YS.loc[order]\n",
    "    YS.columns = [c.replace('isna_', '') for c in YS.columns]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    im = ax.imshow(YS.values, aspect='auto')\n",
    "    ax.set_xticks(np.arange(YS.shape[1]))\n",
    "    ax.set_xticklabels(YS.columns, rotation=45, ha='right')\n",
    "    ax.set_yticks(np.arange(YS.shape[0]))\n",
    "    ax.set_yticklabels(YS.index.astype(int))\n",
    "    ax.set_title('Missingness heatmap (%) — Year × Stat')\n",
    "    fig.colorbar(im, ax=ax, label='% missing')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================\n",
    "# Figure B — Year trend (avg across all stats)\n",
    "# =============================================\n",
    "if 'year' in raw.columns:\n",
    "    Y_avg = (raw.groupby('year')[flag_cols].mean().mul(100).mean(axis=1)\n",
    "             .sort_index())\n",
    "    fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "    ax.plot(Y_avg.index.astype(int), Y_avg.values, marker='o')\n",
    "    for x, y in zip(Y_avg.index, Y_avg.values):\n",
    "        ax.text(x, y, f'{y:.2f}%', va='bottom', ha='center', fontsize=9)\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('% missing (avg across stats)')\n",
    "    ax.set_title('Missingness over time (yearly average)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =========================\n",
    "# Figure C — Hour × Stat\n",
    "# =========================\n",
    "if 'hour' in raw.columns:\n",
    "    # drop hours that are NaN (unparseable)\n",
    "    HH = raw.dropna(subset=['hour']).copy()\n",
    "    HH['hour'] = HH['hour'].astype(int)\n",
    "    HS = (HH.groupby('hour')[flag_cols].mean().mul(100))\n",
    "    # ensure 0–23 present (fill with zeros if absent)\n",
    "    HS = HS.reindex(range(0, 24), fill_value=0)\n",
    "    HS.columns = [c.replace('isna_', '') for c in HS.columns]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    im = ax.imshow(HS.values, aspect='auto')\n",
    "    ax.set_xticks(np.arange(HS.shape[1]))\n",
    "    ax.set_xticklabels(HS.columns, rotation=45, ha='right')\n",
    "    ax.set_yticks(np.arange(HS.shape[0]))\n",
    "    ax.set_yticklabels(HS.index.astype(int))\n",
    "    ax.set_title('Missingness heatmap (%) — Hour × Stat')\n",
    "    fig.colorbar(im, ax=ax, label='% missing')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Sanity Checks\n",
    "\n",
    "Before moving forward, we need to verify that our data makes logical sense. We will check if the relationships between different columns are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_issues = []\n",
    "\n",
    "# Check 1: Full time goals should be >= half time goals\n",
    "print(\"\\nFull Time Goals >= Half Time Goals\")\n",
    "ht_ft_home_check = all_matches['FTHG'] >= all_matches['HTHG']\n",
    "ht_ft_away_check = all_matches['FTAG'] >= all_matches['HTAG']\n",
    "home_violations = (~ht_ft_home_check).sum()\n",
    "away_violations = (~ht_ft_away_check).sum()\n",
    "print(f\"   Home goals violations: {home_violations}\")\n",
    "print(f\"   Away goals violations: {away_violations}\")\n",
    "if home_violations > 0 or away_violations > 0:\n",
    "    sanity_issues.append(f\"FT goals < HT goals: {home_violations + away_violations} cases\")\n",
    "\n",
    "# Check 2: Full time result should match actual goals\n",
    "print(\"\\nFull Time Result matches actual goals\")\n",
    "ftr_check = pd.Series(index=all_matches.index, dtype=bool)\n",
    "ftr_check = (\n",
    "    ((all_matches['FTR'] == 'H') & (all_matches['FTHG'] > all_matches['FTAG'])) |\n",
    "    ((all_matches['FTR'] == 'A') & (all_matches['FTAG'] > all_matches['FTHG'])) |\n",
    "    ((all_matches['FTR'] == 'D') & (all_matches['FTHG'] == all_matches['FTAG']))\n",
    ")\n",
    "ftr_violations = (~ftr_check).sum()\n",
    "print(f\"   FTR mismatches: {ftr_violations}\")\n",
    "if ftr_violations > 0:\n",
    "    sanity_issues.append(f\"FTR doesn't match goals: {ftr_violations} cases\")\n",
    "\n",
    "# Check 3: Half time result should match half time goals\n",
    "print(\"\\nChecking: Half Time Result matches half time goals\")\n",
    "htr_check = pd.Series(index=all_matches.index, dtype=bool)\n",
    "htr_check = (\n",
    "    ((all_matches['HTR'] == 'H') & (all_matches['HTHG'] > all_matches['HTAG'])) |\n",
    "    ((all_matches['HTR'] == 'A') & (all_matches['HTAG'] > all_matches['HTHG'])) |\n",
    "    ((all_matches['HTR'] == 'D') & (all_matches['HTHG'] == all_matches['HTAG']))\n",
    ")\n",
    "htr_violations = (~htr_check).sum()\n",
    "print(f\"   HTR mismatches: {htr_violations}\")\n",
    "if htr_violations > 0:\n",
    "    sanity_issues.append(f\"HTR doesn't match HT goals: {htr_violations} cases\")\n",
    "\n",
    "# Check 4: Shots on target should be <= total shots\n",
    "print(\"\\nShots on Target <= Total Shots\")\n",
    "home_shot_check = all_matches['HST'] <= all_matches['HS']\n",
    "away_shot_check = all_matches['AST'] <= all_matches['AS']\n",
    "home_shot_violations = (~home_shot_check).sum()\n",
    "away_shot_violations = (~away_shot_check).sum()\n",
    "print(f\"   Home shots violations: {home_shot_violations}\")\n",
    "print(f\"   Away shots violations: {away_shot_violations}\")\n",
    "if home_shot_violations > 0 or away_shot_violations > 0:\n",
    "    sanity_issues.append(f\"Shots on target > total shots: {home_shot_violations + away_shot_violations} cases\")\n",
    "\n",
    "# Check 5: Goals should be <= shots on target (generally, but not always)\n",
    "print(\"\\nGoals <= Shots on Target (usually)\")\n",
    "home_goals_shots_check = all_matches['FTHG'] <= all_matches['HST']\n",
    "away_goals_shots_check = all_matches['FTAG'] <= all_matches['AST']\n",
    "home_goals_violations = (~home_goals_shots_check).sum()\n",
    "away_goals_violations = (~away_goals_shots_check).sum()\n",
    "print(f\"   Home goals > shots on target: {home_goals_violations}\")\n",
    "print(f\"   Away goals > shots on target: {away_goals_violations}\")\n",
    "print(f\"   Note: Some violations are possible due to own goals or deflections\")\n",
    "if home_goals_violations > 10 or away_goals_violations > 10:\n",
    "    sanity_issues.append(f\"Goals > shots on target: {home_goals_violations + away_goals_violations} cases (check if excessive)\")\n",
    "\n",
    "# Check 6: Red cards should be <= yellow cards + red cards\n",
    "print(\"\\nCard counts are reasonable\")\n",
    "home_red_check = all_matches['HR'] <= (all_matches['HY'] + all_matches['HR'])\n",
    "away_red_check = all_matches['AR'] <= (all_matches['AY'] + all_matches['AR'])\n",
    "print(f\"   Home card logic violations: {(~home_red_check).sum()}\")\n",
    "print(f\"   Away card logic violations: {(~away_red_check).sum()}\")\n",
    "\n",
    "# Check 7: Negative values check\n",
    "print(\"\\nNo negative values in count columns\")\n",
    "count_columns = ['FTHG', 'FTAG', 'HTHG', 'HTAG', 'HS', 'AS', 'HST', 'AST',\n",
    "                'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "negative_found = False\n",
    "for col in count_columns:\n",
    "    if col in all_matches.columns:\n",
    "        negative_count = (all_matches[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            print(f\"   {col}: {negative_count} negative values\")\n",
    "            negative_found = True\n",
    "            sanity_issues.append(f\"{col} has {negative_count} negative values\")\n",
    "if not negative_found:\n",
    "    print(f\"   No negative values found\")\n",
    "\n",
    "# Check 8: Extreme values check\n",
    "print(\"\\nExtreme values that might be data errors\")\n",
    "extreme_checks = {\n",
    "    'FTHG': 15,\n",
    "    'FTAG': 15,\n",
    "    'HS': 50,\n",
    "    'AS': 50,\n",
    "    'HC': 30,\n",
    "    'AC': 30,\n",
    "    'HY': 10,\n",
    "    'AY': 10,\n",
    "    'HR': 5,\n",
    "    'AR': 5\n",
    "}\n",
    "for col, threshold in extreme_checks.items():\n",
    "    if col in all_matches.columns:\n",
    "        extreme_count = (all_matches[col] > threshold).sum()\n",
    "        if extreme_count > 0:\n",
    "            max_value = all_matches[col].max()\n",
    "            print(f\"   {col} > {threshold}: {extreme_count} cases (max: {max_value})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sanity checks help us verify that the data is internally consistent. We check things like full time goals being at least as many as half time goals, that the match result codes match the actual goal counts, that shots on target do not exceed total shots, and that there are no negative values in count columns. These checks help identify data entry errors or corruption before we use the data for modeling.\n",
    "\n",
    "Our data passed most checks well. Full time goals are always at least as many as half time goals, which is correct. The full time result codes match the actual scores perfectly.\n",
    "\n",
    "We found 41 matches where the half time result code does not match the half time goals. This is a small number out of 42,593 matches, so it is likely just data entry errors in those specific matches.\n",
    "\n",
    "We found 6 matches where shots on target are higher than total shots. This is probably a recording error but only affects 6 matches so it is not a big problem.\n",
    "\n",
    "We found 234 matches where a team scored more goals than they had shots on target. This can happen in real football due to own goals or deflections, so these are not necessarily errors.\n",
    "\n",
    "We found one match where a team got 9 red cards. This is extremely unusual and might be a data error, but it is only one match out of thousands.\n",
    "\n",
    "Overall, the data quality is very good. The few issues we found affect less than 1 percent of matches and will not significantly impact our model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 League and Country Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# League distribution\n",
    "league_counts = all_matches['Div'].value_counts()\n",
    "print(\"League distribution:\")\n",
    "display(league_counts)\n",
    "\n",
    "# Country mapping for better understanding\n",
    "country_mapping = {\n",
    "    'E': 'England', 'SC': 'Scotland', 'SP': 'Spain', 'I': 'Italy',\n",
    "    'D': 'Germany', 'F': 'France', 'N': 'Netherlands', 'B': 'Belgium',\n",
    "    'P': 'Portugal', 'T': 'Turkey', 'G': 'Greece'\n",
    "}\n",
    "\n",
    "all_matches['Country'] = all_matches['Div'].str[:-1].map(country_mapping)\n",
    "country_counts = all_matches['Country'].value_counts()\n",
    "print(f\"\\nMatches per country:\")\n",
    "display(country_counts)\n",
    "\n",
    "# Visualize the distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Country distribution\n",
    "country_counts_sorted = country_counts.dropna().sort_values(ascending=False)\n",
    "bars = ax1.bar(country_counts_sorted.index, country_counts_sorted.values, color='skyblue')\n",
    "ax1.set_title('Matches per Country')\n",
    "ax1.set_xlabel('Country')\n",
    "ax1.set_ylabel('Number of Matches')\n",
    "ax1.tick_params(axis='x', rotation=30)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# data labels\n",
    "for rect in bars:\n",
    "    height = rect.get_height()\n",
    "    ax1.text(rect.get_x() + rect.get_width()/2, height, f\"{int(height):,}\",\n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Goals distribution\n",
    "total_goals = all_matches['FTHG'] + all_matches['FTAG']\n",
    "max_g = int(np.nanmax(total_goals))\n",
    "bins = np.arange(-0.5, max(10, max_g) + 1.5, 1)\n",
    "\n",
    "ax2.hist(total_goals, bins=bins, color='lightcoral', alpha=0.7)\n",
    "ax2.axvline(x=2.5, linestyle='--', linewidth=2, label='2.5 goals threshold')\n",
    "ax2.set_title('Distribution of Total Goals per Match')\n",
    "ax2.set_xlabel('Total Goals')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_xticks(range(0, max(10, max_g) + 1))\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "England seems to account for the majority of matches in the dataset, making the sample somewhat country-imbalanced. This suggests that model training should be performed separately for each country, or at least include country-specific components, to prevent English leagues from dominating the overall model behavior.  \n",
    "\n",
    "When building time-aware models, it would also be beneficial to use chronological splits within each country and consider assigning higher weights to more recent matches, since they better reflect current team dynamics and scoring trends.  \n",
    "\n",
    "Alse, the distribution of total goals per match is right-skewed, with mode around 2–3 goals. The red dashed line at 2.5 goals marks the classification threshold for our target variable. Visually, the mass on either side of this threshold is roughly equal, which confirms the balanced 50/50 split observed in the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Handling csv issues\n",
    "It seems like the renaming and loading went smoothly! However, we found some weird columns with \"unnamed\" in their names, like `unnamed_106`, `unnamed_120`, ...  \n",
    "That sometimes happens when excel files have extra blank columns. We'll take a quick look to see if they have any data, and if they're totally empty (full of NaNs), we'll just get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnamed_cols = [c for c in all_matches.columns if c.lower().startswith(\"unnamed\")]\n",
    "all_matches[unnamed_cols].isna().mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're 100% full of NaNs so we can now safely drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = all_matches.drop(columns=unnamed_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Normalizing league codes\n",
    "Let's normalize the leagues, as English and Scottish leagues have the best leagues interpreted as E0, SC0, respectively. All other countries mark the best league as CountryCode1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = all_matches['Div'].str.startswith(('E', 'SC'))\n",
    "all_matches.loc[mask, 'Div'] = all_matches.loc[mask, 'Div'].apply(\n",
    "    lambda x: f\"{x[:-1]}{int(x[-1]) + 1}\"\n",
    ")\n",
    "\n",
    "print(all_matches['Div'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Handling English and Scottish yellow cards\n",
    "We need to take care of the first note in notex.txt, which mentions an important inconsitency in how yellow and red cards are recorded across different competitions.  \n",
    "\n",
    "In English and Scottish leagues, when a player receives a second yellow card that leads to a red card, the initial yellow card is not counted in the match statistics, only the red card is recorded. However, European and international competitions record both: the second yellow is counted as an additional yellow card plus a red card \n",
    "\n",
    "As a result, yellow card totals in English and Scottish matches can underestimate the true number of yellow cards compared to other leagues. To correct for this and ensure consistency across competitions, we applied a simple adjustment:\n",
    "- whenever a team has exactly one red card and one yellow card, we add one additional yellow card.\n",
    "- and if a team has 0 reds, 2 or more reds, or 1 red but no yellows, we make no adjustment.\n",
    "\n",
    "We acknowledge that this rule is an approximation, our adjustment may not always be the case and it may introduce some bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = all_matches['Div'].str.startswith(('E', 'SC'))\n",
    "red_mask = mask & ((all_matches['HR'] == 1) | (all_matches['AR'] == 1))\n",
    "\n",
    "print(\"Before adjustment (sample):\")\n",
    "print(all_matches.loc[red_mask, ['Div', 'HY', 'HR', 'AY', 'AR']].head())\n",
    "\n",
    "all_matches.loc[mask & (all_matches['HR'] == 1) & (all_matches['HY'] == 0), 'HY'] += 1\n",
    "all_matches.loc[mask & (all_matches['AR'] == 1) & (all_matches['AY'] == 0), 'AY'] += 1\n",
    "\n",
    "print(\"\\nAfter adjustment (sample):\")\n",
    "print(all_matches.loc[red_mask, ['Div', 'HY', 'HR', 'AY', 'AR']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Correcting data types\n",
    "Now, let's inspect the data types of our columns. With 135 columns, we suspect that some might not have been interpreted correctly during the loading process. Checking the data types is an important step before proceeding with any further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, dtype in all_matches.dtypes.items():\n",
    "    print(f\"{col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_columns = ['Date', 'Time']\n",
    "\n",
    "category_columns = ['Div', 'HomeTeam', 'AwayTeam', 'FTR', 'HTR', 'Referee', 'Country']\n",
    "\n",
    "int_columns = ['FTHG', 'FTAG', 'HTHG', 'HTAG', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "\n",
    "float_columns = ['B365CH', 'BWCA', '1XBH']\n",
    "\n",
    "for col in time_columns:\n",
    "    if col == 'Date':\n",
    "        all_matches[col] = pd.to_datetime(all_matches[col])\n",
    "    else:\n",
    "        all_matches[col] = pd.to_datetime(all_matches[col], format='%H:%M').dt.time\n",
    "\n",
    "for col in category_columns:\n",
    "    all_matches[col] = all_matches[col].astype('category')\n",
    "\n",
    "for col in int_columns:\n",
    "    all_matches[col] = pd.to_numeric(all_matches[col], errors='coerce').astype('Int64')\n",
    "\n",
    "for col in float_columns:\n",
    "    all_matches[col] = pd.to_numeric(all_matches[col], errors='coerce').astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, dtype in all_matches.dtypes.items():\n",
    "    print(f\"{col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Missing value imputation with domain-specific strategies\n",
    "\n",
    "Based on our EDA, we'll handle missing values with different strategies based on data characteristics:\n",
    "\n",
    "1. **Key match statistics**: Very few missing values (~0.1%) - use SimpleImputer with median strategy\n",
    "2. **Betting odds**: High missingness (80%+) but match-specific - use cross-bookmaker median imputation per match, then overall median fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns with low missingness that need imputation\n",
    "low_missingness_cols = ['HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "\n",
    "# Check current missing values before imputation\n",
    "print(\"Missing values before imputation:\")\n",
    "for col in low_missingness_cols:\n",
    "    if col in all_matches.columns:\n",
    "        missing_count = all_matches[col].isnull().sum()\n",
    "        missing_pct = (missing_count / len(all_matches)) * 100\n",
    "        print(f\"{col}: {missing_count} ({missing_pct:.3f}%)\")\n",
    "\n",
    "# Apply median imputation for numerical match statistics\n",
    "if any(all_matches[col].isnull().sum() > 0 for col in low_missingness_cols if col in all_matches.columns):\n",
    "    match_stats_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "    # Only impute columns that actually exist and have missing values\n",
    "    cols_to_impute = [col for col in low_missingness_cols if col in all_matches.columns and all_matches[col].isnull().sum() > 0]\n",
    "\n",
    "    if cols_to_impute:\n",
    "        print(f\"\\nApplying median imputation to: {cols_to_impute}\")\n",
    "        all_matches[cols_to_impute] = match_stats_imputer.fit_transform(all_matches[cols_to_impute])\n",
    "\n",
    "        print(\"Imputation completed. Median values used:\")\n",
    "        for col in cols_to_impute:\n",
    "            median_val = all_matches[col].median()\n",
    "            print(f\"  {col}: {median_val}\")\n",
    "    else:\n",
    "        print(\"No missing values found in match statistics columns.\")\n",
    "else:\n",
    "    print(\"No missing values found in match statistics columns.\")\n",
    "\n",
    "# Handle categorical columns separately\n",
    "if 'Referee' in all_matches.columns:\n",
    "    referee_missing = all_matches['Referee'].isnull().sum()\n",
    "    if referee_missing > 0:\n",
    "        if 'Unknown' not in all_matches['Referee'].cat.categories:\n",
    "            all_matches['Referee'] = all_matches['Referee'].cat.add_categories(['Unknown'])\n",
    "        all_matches['Referee'] = all_matches['Referee'].fillna('Unknown')\n",
    "        print(f\"Filled {referee_missing} missing referees with 'Unknown'\")\n",
    "\n",
    "if 'Time' in all_matches.columns:\n",
    "    time_missing = all_matches['Time'].isnull().sum()\n",
    "    if time_missing > 0:\n",
    "        all_matches['Time'] = all_matches['Time'].fillna(pd.to_datetime('15:00', format='%H:%M').time())\n",
    "        print(f\"Filled {time_missing} missing times with '15:00'\")\n",
    "\n",
    "# Verify no missing values remain in core match statistics\n",
    "print(f\"\\nVerification - remaining missing values in core columns:\")\n",
    "verification_cols = low_missingness_cols + ['Referee', 'Time']\n",
    "total_missing = 0\n",
    "for col in verification_cols:\n",
    "    if col in all_matches.columns:\n",
    "        missing = all_matches[col].isnull().sum()\n",
    "        if missing > 0:\n",
    "            print(f\"{col}: {missing}\")\n",
    "            total_missing += missing\n",
    "\n",
    "if total_missing == 0:\n",
    "    print(\"All core match statistics successfully imputed - no missing values remain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle the gaps, we used median imputation for the numerical match statistics because they're robust to outliers and preserve the central distribution of the data. For categorical fields, missing Referee entries were replaced with ‘Unknown’, and missing Time values were set to 15:00, which should be the typical match kickoff time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle betting odds imputation with proper market categorization\n",
    "# Based on notes.txt, betting odds are organized by market type (1X2, O/U 2.5, Asian Handicap)\n",
    "\n",
    "# Identify betting odds columns\n",
    "betting_cols = [col for col in all_matches.columns if any(bookmaker in col for bookmaker in ['B365', 'BW', 'PS', 'IW', 'LB', 'WH', 'SJ', 'VC', 'BF', '1XB', 'CL', 'GB', 'SO', 'SB', 'SY', 'Max', 'Avg', 'Bb'])]\n",
    "\n",
    "print(f\"Found {len(betting_cols)} betting odds columns\")\n",
    "\n",
    "# Analyze missing patterns in betting odds\n",
    "betting_missing_analysis = []\n",
    "high_missing_cols = []\n",
    "for col in betting_cols:\n",
    "    missing_count = all_matches[col].isnull().sum()\n",
    "    missing_pct = (missing_count / len(all_matches)) * 100\n",
    "    betting_missing_analysis.append({\n",
    "        'column': col,\n",
    "        'missing_count': missing_count,\n",
    "        'missing_pct': missing_pct\n",
    "    })\n",
    "    if missing_pct > 80:  # Track columns with very high missingness\n",
    "        high_missing_cols.append(col)\n",
    "\n",
    "print(f\"Columns with >80% missing values: {len(high_missing_cols)}\")\n",
    "print(\"Sample of betting odds with lower missingness (<80%):\")\n",
    "for item in sorted(betting_missing_analysis, key=lambda x: x['missing_pct'])[:15]:\n",
    "    if item['missing_pct'] < 80:\n",
    "        print(f\"  {item['column']}: {item['missing_pct']:.1f}%\")\n",
    "\n",
    "# Categorize betting odds by market type based on notes.txt\n",
    "def categorize_betting_market(col_name):\n",
    "    \"\"\"Categorize betting column by market type based on column name patterns\"\"\"\n",
    "    col = col_name.upper()\n",
    "\n",
    "    # 1X2 Market (Home/Draw/Away win)\n",
    "    if col.endswith('H') and not any(x in col for x in ['AH', '>', '<']):\n",
    "        return 'home_win'\n",
    "    elif col.endswith('D') and not any(x in col for x in ['AH', '>', '<']):\n",
    "        return 'draw'\n",
    "    elif col.endswith('A') and not any(x in col for x in ['AH', '>', '<']):\n",
    "        return 'away_win'\n",
    "\n",
    "    # Over/Under 2.5 Goals Market\n",
    "    elif '>2.5' in col or 'O2.5' in col:\n",
    "        return 'over_2_5'\n",
    "    elif '<2.5' in col or 'U2.5' in col:\n",
    "        return 'under_2_5'\n",
    "\n",
    "    # Asian Handicap Market\n",
    "    elif 'AH' in col and col.endswith('H'):\n",
    "        return 'ah_home'\n",
    "    elif 'AH' in col and col.endswith('A'):\n",
    "        return 'ah_away'\n",
    "    elif 'AH' in col and not col.endswith(('H', 'A')):\n",
    "        return 'ah_handicap'\n",
    "\n",
    "    # Other markets\n",
    "    elif 'C>' in col:  # Corner markets\n",
    "        return 'corners'\n",
    "    elif any(x in col for x in ['FKCH', 'FKCA']):  # Free kicks\n",
    "        return 'free_kicks'\n",
    "\n",
    "    return 'other'\n",
    "\n",
    "# Group betting columns by market type\n",
    "market_groups = {}\n",
    "for col in betting_cols:\n",
    "    market_type = categorize_betting_market(col)\n",
    "    market_groups.setdefault(market_type, []).append(col)\n",
    "\n",
    "print(f\"\\nBetting odds grouped by market type:\")\n",
    "for market_type, columns in market_groups.items():\n",
    "    avg_missing = np.mean([item['missing_pct'] for item in betting_missing_analysis if item['column'] in columns])\n",
    "    print(f\"  {market_type}: {len(columns)} columns (avg missing: {avg_missing:.1f}%)\")\n",
    "\n",
    "# Apply cross-bookmaker median imputation within each market for each match\n",
    "total_imputed = 0\n",
    "markets_processed = []\n",
    "\n",
    "for market_type, columns in market_groups.items():\n",
    "    if len(columns) > 1 and market_type != 'other':  # Only process markets with multiple bookmakers\n",
    "        print(f\"\\nProcessing {market_type} market ({len(columns)} columns)...\")\n",
    "        markets_processed.append(market_type)\n",
    "\n",
    "        # Check how much data we have for this market\n",
    "        market_data_availability = []\n",
    "        for col in columns:\n",
    "            non_missing = all_matches[col].notna().sum()\n",
    "            market_data_availability.append(non_missing)\n",
    "\n",
    "        if max(market_data_availability) > 1000:  # Only process if we have reasonable data\n",
    "            match_imputed = 0\n",
    "\n",
    "            # Process each match individually\n",
    "            for idx in all_matches.index:\n",
    "                # Get odds for this match across all bookmakers for this market\n",
    "                match_odds = all_matches.loc[idx, columns]\n",
    "\n",
    "                # If any values are missing but others exist, use median of available bookmakers\n",
    "                if match_odds.isnull().any() and not match_odds.isnull().all():\n",
    "                    match_median = match_odds.median()\n",
    "\n",
    "                    # Fill missing values with the cross-bookmaker median for this match\n",
    "                    for col in columns:\n",
    "                        if pd.isnull(all_matches.loc[idx, col]):\n",
    "                            all_matches.loc[idx, col] = match_median\n",
    "                            total_imputed += 1\n",
    "                            match_imputed += 1\n",
    "\n",
    "            print(f\"  {market_type}: {match_imputed} values imputed using cross-bookmaker median\")\n",
    "\n",
    "print(f\"\\nCross-bookmaker imputation completed: {total_imputed} values imputed across {len(markets_processed)} markets\")\n",
    "\n",
    "# For remaining missing values, apply conservative strategy\n",
    "# Only use overall median fallback for markets with reasonable data coverage\n",
    "remaining_imputed = 0\n",
    "columns_fully_imputed = []\n",
    "\n",
    "for market_type, columns in market_groups.items():\n",
    "    if market_type in ['home_win', 'draw', 'away_win', 'over_2_5', 'under_2_5']:  # Core markets only\n",
    "        for col in columns:\n",
    "            missing_before = all_matches[col].isnull().sum()\n",
    "            data_coverage = (all_matches[col].notna().sum() / len(all_matches)) * 100\n",
    "\n",
    "            # Only apply fallback imputation if we have at least 10% data coverage\n",
    "            if missing_before > 0 and data_coverage >= 10:\n",
    "                overall_median = all_matches[col].median()\n",
    "                all_matches[col] = all_matches[col].fillna(overall_median)\n",
    "                remaining_imputed += missing_before\n",
    "                columns_fully_imputed.append(col)\n",
    "\n",
    "print(f\"Overall median fallback applied to {len(columns_fully_imputed)} columns: {remaining_imputed} values imputed\")\n",
    "\n",
    "# For columns with <10% data coverage, we'll exclude them from modeling rather than impute\n",
    "excluded_cols = []\n",
    "for col in betting_cols:\n",
    "    data_coverage = (all_matches[col].notna().sum() / len(all_matches)) * 100\n",
    "    if data_coverage < 10:\n",
    "        excluded_cols.append(col)\n",
    "\n",
    "print(f\"\\nColumns excluded due to <10% data coverage: {len(excluded_cols)}\")\n",
    "print(\"These will be excluded from the extended dataset to avoid poor imputation quality\")\n",
    "\n",
    "# Verify imputation results for key markets\n",
    "print(f\"\\nVerification - missing values after imputation for key betting markets:\")\n",
    "key_betting_cols = [col for col in betting_cols if col not in excluded_cols][:15]  # Check sample\n",
    "final_missing = 0\n",
    "for col in key_betting_cols:\n",
    "    missing = all_matches[col].isnull().sum()\n",
    "    if missing > 0:\n",
    "        data_coverage = (all_matches[col].notna().sum() / len(all_matches)) * 100\n",
    "        print(f\"  {col}: {missing} missing ({data_coverage:.1f}% coverage)\")\n",
    "        final_missing += missing\n",
    "\n",
    "if final_missing == 0:\n",
    "    print(\"Key betting odds successfully imputed\")\n",
    "else:\n",
    "    print(f\"⚠ {final_missing} missing values remain in key betting columns\")\n",
    "\n",
    "# Update betting features list to exclude low-coverage columns\n",
    "print(f\"\\nUpdating betting features list:\")\n",
    "print(f\"Original betting columns: {len(betting_cols)}\")\n",
    "print(f\"Excluded low-coverage columns: {len(excluded_cols)}\")\n",
    "print(f\"Final betting columns for modeling: {len(betting_cols) - len(excluded_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Outlier detection and handling\n",
    "\n",
    "Following the methodology from Week1 (house pricing), we'll use z-score analysis to detect outliers in match statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical columns for outlier detection\n",
    "match_stats_cols = ['HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "numerical_cols = ['FTHG', 'FTAG', 'HTHG', 'HTAG'] + match_stats_cols\n",
    "\n",
    "# Calculate z-scores for numerical columns\n",
    "print(\"Outlier analysis using z-score > 3:\")\n",
    "outlier_counts = {}\n",
    "\n",
    "for col in numerical_cols:\n",
    "    if col in all_matches.columns:\n",
    "        z_scores = np.abs(zscore(all_matches[col].dropna()))\n",
    "        outliers = (z_scores > 3).sum()\n",
    "        outlier_counts[col] = outliers\n",
    "        if outliers > 0:\n",
    "            print(f\"{col}: {outliers} outliers ({outliers/len(all_matches)*100:.2f}%)\")\n",
    "\n",
    "# Look at extreme cases\n",
    "print(f\"\\nExamples of potential outliers:\")\n",
    "print(f\"Highest total goals: {all_matches['FTHG'].max() + all_matches['FTAG'].max()}\")\n",
    "print(f\"Most shots in a match: {all_matches['HS'].max() + all_matches['AS'].max()}\")\n",
    "print(f\"Most cards in a match: {all_matches['HY'].max() + all_matches['AY'].max()}\")\n",
    "\n",
    "# Visualize outliers for key variables\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "key_vars = ['FTHG', 'FTAG', 'HS', 'AS']\n",
    "\n",
    "for i, var in enumerate(key_vars):\n",
    "    row, col = i // 2, i % 2\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    # Box plot to show outliers\n",
    "    all_matches[var].plot(kind='box', ax=ax)\n",
    "    ax.set_title(f'Box Plot of {var}')\n",
    "    ax.set_ylabel(var)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# For football data, we'll be more conservative with outlier removal\n",
    "# as extreme scores can be legitimate (unlike house prices)\n",
    "print(f\"\\nDecision: Keep outliers for football data as high scores/stats can be legitimate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature engineering\n",
    "\n",
    "Based on soccer domain knowledge and the course materials, we'll create meaningful features that could help predict Over/Under 2.5 goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Target variable creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the main target variable: Over/Under 2.5 goals\n",
    "all_matches['total_goals'] = all_matches['FTHG'] + all_matches['FTAG']\n",
    "all_matches['over_2_5'] = (all_matches['total_goals'] > 2.5).astype(int)\n",
    "\n",
    "print(\"Target variable distribution:\")\n",
    "print(all_matches['over_2_5'].value_counts())\n",
    "print(f\"Over 2.5 rate: {all_matches['over_2_5'].mean():.2%}\")\n",
    "\n",
    "# Also create alternative targets for analysis\n",
    "all_matches['over_1_5'] = (all_matches['total_goals'] > 1.5).astype(int)\n",
    "all_matches['over_3_5'] = (all_matches['total_goals'] > 3.5).astype(int)\n",
    "\n",
    "print(f\"\\nOther thresholds:\")\n",
    "print(f\"Over 1.5 rate: {all_matches['over_1_5'].mean():.2%}\")\n",
    "print(f\"Over 3.5 rate: {all_matches['over_3_5'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Basic feature engineering\n",
    "\n",
    "Creating features that capture match dynamics and team performance patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Shot efficiency features\n",
    "all_matches['home_shot_accuracy'] = all_matches['HST'] / (all_matches['HS'] + 0.001)  # avoid division by zero\n",
    "all_matches['away_shot_accuracy'] = all_matches['AST'] / (all_matches['AS'] + 0.001)\n",
    "all_matches['total_shots'] = all_matches['HS'] + all_matches['AS']\n",
    "all_matches['total_shots_on_target'] = all_matches['HST'] + all_matches['AST']\n",
    "\n",
    "# 2. Attacking vs Defensive balance\n",
    "all_matches['shot_dominance'] = (all_matches['HS'] - all_matches['AS']) / (all_matches['HS'] + all_matches['AS'] + 0.001)\n",
    "all_matches['corner_dominance'] = (all_matches['HC'] - all_matches['AC']) / (all_matches['HC'] + all_matches['AC'] + 0.001)\n",
    "\n",
    "# 3. Game intensity features\n",
    "all_matches['total_fouls'] = all_matches['HF'] + all_matches['AF']\n",
    "all_matches['total_cards'] = all_matches['HY'] + all_matches['AY'] + all_matches['HR'] + all_matches['AR']\n",
    "all_matches['card_intensity'] = all_matches['total_cards'] / (all_matches['total_fouls'] + 0.001)\n",
    "\n",
    "# 4. Half-time patterns\n",
    "all_matches['ht_total_goals'] = all_matches['HTHG'] + all_matches['HTAG']\n",
    "all_matches['second_half_goals'] = all_matches['total_goals'] - all_matches['ht_total_goals']\n",
    "\n",
    "# 5. League tier (lower tiers might have different patterns)\n",
    "all_matches['league_tier'] = all_matches['Div'].str[-1].astype(int)\n",
    "\n",
    "# 6. Season timing features\n",
    "all_matches['month'] = all_matches['Date'].dt.month\n",
    "all_matches['is_weekend'] = all_matches['Date'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "\n",
    "print(\"Created basic engineered features:\")\n",
    "new_features = ['home_shot_accuracy', 'away_shot_accuracy', 'total_shots', 'total_shots_on_target',\n",
    "               'shot_dominance', 'corner_dominance', 'total_fouls', 'total_cards', 'card_intensity',\n",
    "               'ht_total_goals', 'second_half_goals', 'league_tier', 'month', 'is_weekend']\n",
    "\n",
    "for feature in new_features:\n",
    "    print(f\"- {feature}: mean={all_matches[feature].mean():.3f}, std={all_matches[feature].std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check what columns we actually have available\n",
    "available_cols = all_matches.columns.tolist()\n",
    "\n",
    "# Check for core match info columns\n",
    "core_match_info = ['Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG', 'HTR']\n",
    "\n",
    "# Check for match statistics columns from notes.txt\n",
    "match_stats_from_notes = ['Attendance', 'Referee', 'HS', 'AS', 'HST', 'AST', 'HHW', 'AHW', 'HC', 'AC',\n",
    "                         'HF', 'AF', 'HFKC', 'AFKC', 'HO', 'AO', 'HY', 'AY', 'HR', 'AR', 'HBP', 'ABP']\n",
    "\n",
    "# Identify all available core and match statistics columns\n",
    "basic_available = [col for col in core_match_info if col in available_cols]\n",
    "extended_available = [col for col in core_match_info + match_stats_from_notes if col in available_cols]\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Basic dataset core columns available: {len(basic_available)}\")\n",
    "print(f\"Extended dataset core columns available: {len(extended_available)}\")\n",
    "\n",
    "# Identify categorical and numerical columns for modeling\n",
    "categorical_features = ['Div', 'HomeTeam', 'AwayTeam', 'Country', 'FTR', 'HTR', 'Referee']  # League division, teams, and results\n",
    "ordinal_features = ['league_tier', 'month']  # Features with natural ordering\n",
    "\n",
    "# BASIC DATASET: Core match information (what would be available from basic match reports)\n",
    "basic_core_features = [col for col in ['Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR'] if col in all_matches.columns]\n",
    "\n",
    "# Engineered features that can be created from basic core columns only\n",
    "basic_engineered_features = [\n",
    "    'total_goals',   # Goal-based features\n",
    "    'league_tier', 'month', 'is_weekend'  # Date/league features\n",
    "]\n",
    "\n",
    "# EXTENDED DATASET: All available match data including detailed statistics\n",
    "extended_core_features = [col for col in [\n",
    "    # Core match info\n",
    "    'Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG', 'HTR',\n",
    "    # Match statistics\n",
    "    'Attendance', 'Referee', 'HS', 'AS', 'HST', 'AST', 'HHW', 'AHW', 'HC', 'AC',\n",
    "    'HF', 'AF', 'HFKC', 'AFKC', 'HO', 'AO', 'HY', 'AY', 'HR', 'AR', 'HBP', 'ABP'\n",
    "] if col in all_matches.columns]\n",
    "\n",
    "# All engineered features (using rich match statistics)\n",
    "extended_engineered_features = [\n",
    "    'total_goals', 'ht_total_goals', 'second_half_goals',  # Goal-based\n",
    "    'home_shot_accuracy', 'away_shot_accuracy', 'total_shots', 'total_shots_on_target',  # Shot-based\n",
    "    'shot_dominance', 'corner_dominance', 'total_fouls', 'total_cards', 'card_intensity',  # Game dynamics\n",
    "    'league_tier', 'month', 'is_weekend'  # Date/league features\n",
    "]\n",
    "\n",
    "# Extended features (betting odds - only high-quality columns after imputation)\n",
    "betting_features = []\n",
    "for col in all_matches.columns:\n",
    "    # Check if it's a betting column and has good data coverage (>10%)\n",
    "    if any(bookmaker in col for bookmaker in ['B365', 'BW', 'PS', 'IW', 'LB', 'WH', 'SJ', 'VC', 'BF', '1XB']) and col not in categorical_features:\n",
    "        data_coverage = (all_matches[col].notna().sum() / len(all_matches)) * 100\n",
    "        if data_coverage >= 10:  # Only include columns with at least 10% data coverage\n",
    "            betting_features.append(col)\n",
    "\n",
    "print(f\"\\nBASIC DATASET:\")\n",
    "print(f\"  Core features: {len(basic_core_features)} - {basic_core_features}\")\n",
    "print(f\"  Engineered features: {len(basic_engineered_features)} - {basic_engineered_features}\")\n",
    "print(f\"  Total basic features: {len(basic_core_features + basic_engineered_features)}\")\n",
    "\n",
    "print(f\"\\nEXTENDED DATASET:\")\n",
    "print(f\"  Core features: {len(extended_core_features)}\")\n",
    "print(f\"  Engineered features: {len(extended_engineered_features)}\")\n",
    "print(f\"  Betting features with >10% coverage: {len(betting_features)}\")\n",
    "print(f\"  Total extended features: {len(extended_core_features + extended_engineered_features + betting_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Base feature df engineering\n",
    "\n",
    "Start with core + basic engineered features as the foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with core and basic engineered features\n",
    "df_basic = all_matches[['Div', 'Season', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR',\n",
    "                   'total_goals', 'league_tier', 'month', 'is_weekend', \"over_2_5\"]].copy()\n",
    "# by date for time-based features\n",
    "df_basic = df_basic.sort_values(['Div', 'Date']).reset_index(drop=True)\n",
    "print(f\"Columns: {df_basic.columns.tolist()}\")\n",
    "df_basic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Days since last match\n",
    "\n",
    "\n",
    "**Calculate days since last match for both home and away teams.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days since last match for each team\n",
    "df_basic['home_days_since_last'] = np.nan\n",
    "df_basic['away_days_since_last'] = np.nan\n",
    "for team in df_basic['HomeTeam'].unique():\n",
    "    home_mask = df_basic['HomeTeam'] == team\n",
    "    away_mask = df_basic['AwayTeam'] == team\n",
    "    team_matches = df_basic[home_mask | away_mask].sort_values('Date')\n",
    "    #days between matches\n",
    "    team_matches['days_diff'] = team_matches['Date'].diff().dt.days\n",
    "    # to home/away columns\n",
    "    for idx, row in team_matches.iterrows():\n",
    "        if df_basic.loc[idx, 'HomeTeam'] == team:\n",
    "            df_basic.loc[idx, 'home_days_since_last'] = row['days_diff']\n",
    "        else:\n",
    "            df_basic.loc[idx, 'away_days_since_last'] = row['days_diff']\n",
    "# first matches filling with median\n",
    "df_basic['home_days_since_last'].fillna(df_basic['home_days_since_last'].median(), inplace=True)\n",
    "df_basic['away_days_since_last'].fillna(df_basic['away_days_since_last'].median(), inplace=True)\n",
    "\n",
    "print(f\"Home days since last - mean: {df_basic['home_days_since_last'].mean():.1f}, median: {df_basic['home_days_since_last'].median():.1f}\")\n",
    "print(f\"Away days since last - mean: {df_basic['away_days_since_last'].mean():.1f}, median: {df_basic['away_days_since_last'].median():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Everything seems ok, the usual pause is one week (expected), but they are visible pauses between individual seasons.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 5-match Moving Averages\n",
    "\n",
    "**Calculate 5-match moving averages for goals scored and conceded.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basic['home_goals_ma5'] = np.nan\n",
    "df_basic['home_conceded_ma5'] = np.nan\n",
    "df_basic['away_goals_ma5'] = np.nan\n",
    "df_basic['away_conceded_ma5'] = np.nan\n",
    "\n",
    "for team in df_basic['HomeTeam'].unique():\n",
    "\n",
    "    # Home matches\n",
    "    home_mask = df_basic['HomeTeam'] == team\n",
    "    home_dates = df_basic[home_mask].sort_values('Date').index\n",
    "    for i, idx in enumerate(home_dates):\n",
    "        if i >= 5:\n",
    "            last_5_home = df_basic.loc[home_dates[i-5:i]]\n",
    "            df_basic.loc[idx, 'home_goals_ma5'] = last_5_home['FTHG'].mean()\n",
    "            df_basic.loc[idx, 'home_conceded_ma5'] = last_5_home['FTAG'].mean()\n",
    "\n",
    "    # Away matches\n",
    "    away_mask = df_basic['AwayTeam'] == team\n",
    "    away_dates = df_basic[away_mask].sort_values('Date').index\n",
    "\n",
    "    for i, idx in enumerate(away_dates):\n",
    "        if i >= 5:\n",
    "            last_5_away = df_basic.loc[away_dates[i-5:i]]\n",
    "            df_basic.loc[idx, 'away_goals_ma5'] = last_5_away['FTAG'].mean()\n",
    "            df_basic.loc[idx, 'away_conceded_ma5'] = last_5_away['FTHG'].mean()\n",
    "\n",
    "# NO DATA LEAKAGE: Don't fill NaN values with overall means\n",
    "# NaN values remain for teams' first 5 matches - will be handled by model\n",
    "# or imputed during train/test split using only training data\n",
    "print(f\"\\nMA5 Statistics (NaN preserved for early-season matches):\")\n",
    "print(f\"  home_goals_ma5    - mean: {df_basic['home_goals_ma5'].mean():.2f}, missing: {df_basic['home_goals_ma5'].isna().sum()}\")\n",
    "print(f\"  away_goals_ma5    - mean: {df_basic['away_goals_ma5'].mean():.2f}, missing: {df_basic['away_goals_ma5'].isna().sum()}\")\n",
    "print(f\"  home_conceded_ma5 - mean: {df_basic['home_conceded_ma5'].mean():.2f}, missing: {df_basic['home_conceded_ma5'].isna().sum()}\")\n",
    "print(f\"  away_conceded_ma5 - mean: {df_basic['away_conceded_ma5'].mean():.2f}, missing: {df_basic['away_conceded_ma5'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Fixed: Data leakage removed**\n",
    "- NaN values for early-season matches (first 5 games) are now preserved\n",
    "- No longer filling with overall mean (which would use future data)\n",
    "- Tree-based models can handle NaN values natively\n",
    "- Alternative: Impute during train/test split using only training data statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Promoted/Demoted\n",
    "\n",
    "**Detect teams that changed leagues between seasons using the Season column (e.g., 2019/2020 → 2020/2021).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# promotion/demotion flags\n",
    "df_basic['home_promoted'] = 0\n",
    "df_basic['home_demoted'] = 0\n",
    "df_basic['away_promoted'] = 0\n",
    "df_basic['away_demoted'] = 0\n",
    "\n",
    "# For each team, check if they changed tier between seasons\n",
    "for team in df_basic['HomeTeam'].unique():\n",
    "    team_data = df_basic[(df_basic['HomeTeam'] == team) | (df_basic['AwayTeam'] == team)].sort_values('Date')\n",
    "\n",
    "    # Grouping by season and get the league tier for each season\n",
    "    season_tiers = team_data.groupby('Season')['league_tier'].first()\n",
    "\n",
    "    for i in range(1, len(season_tiers)):\n",
    "        season = season_tiers.index[i]\n",
    "        prev_tier = season_tiers.iloc[i-1]\n",
    "        curr_tier = season_tiers.iloc[i]\n",
    "\n",
    "        if curr_tier < prev_tier:  # Lower tier number = higher division\n",
    "            promoted = 1\n",
    "            demoted = 0\n",
    "        elif curr_tier > prev_tier:\n",
    "            promoted = 0\n",
    "            demoted = 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        season_mask = (df_basic['Season'] == season)\n",
    "        home_mask = season_mask & (df_basic['HomeTeam'] == team)\n",
    "        away_mask = season_mask & (df_basic['AwayTeam'] == team)\n",
    "\n",
    "        df_basic.loc[home_mask, 'home_promoted'] = promoted\n",
    "        df_basic.loc[home_mask, 'home_demoted'] = demoted\n",
    "        df_basic.loc[away_mask, 'away_promoted'] = promoted\n",
    "        df_basic.loc[away_mask, 'away_demoted'] = demoted\n",
    "\n",
    "print(f\"Home teams promoted: {df_basic['home_promoted'].sum()}\")\n",
    "print(f\"Home teams demoted: {df_basic['home_demoted'].sum()}\")\n",
    "print(f\"Away teams promoted: {df_basic['away_promoted'].sum()}\")\n",
    "print(f\"Away teams demoted: {df_basic['away_demoted'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Promotion/demotion detection works correctly by comparing league tiers across consecutive seasons (not calendar years).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4 Historical Season Positions & Goal Patterns\n",
    "\n",
    "**Calculate team standings and goal-scoring patterns with round-based tracking and home/away splits.**\n",
    "\n",
    "**Methodology:**\n",
    "- **Past seasons:** Use final end-of-season standings (complete data)\n",
    "- **Current season:** Use standings after last completed round (prevents data leakage)\n",
    "  - Round = all teams played same number of matches\n",
    "  - Match uses standings from previous completed round only\n",
    "  - Example: If Team A has played 5 matches but round 5 not complete → use round 4 standings\n",
    "- **Position features:** Overall league position only (no home/away split)\n",
    "- **Goal pattern features:** Track separately for overall, home-only, and away-only\n",
    "  - Home team gets: Overall stats + Home-specific stats\n",
    "  - Away team gets: Overall stats + Away-specific stats\n",
    "  - Both absolute counts AND percentages (counts show sample size/reliability)\n",
    "- Use percentiles (0-100) for cross-league comparability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate league standings: final for past seasons, round-by-round for current season\n",
    "def calculate_standings_with_rounds(df):\n",
    "    \"\"\"\n",
    "    Calculate standings with home/away splits:\n",
    "    - Position: Overall only\n",
    "    - Goal patterns: Overall + Home-specific + Away-specific (both counts and percentages)\n",
    "    \"\"\"\n",
    "    standings_list = []\n",
    "    match_to_round = {}\n",
    "\n",
    "    for (season, div), group in df.groupby(['Season', 'Div']):\n",
    "        group = group.sort_values('Date').reset_index(drop=True)\n",
    "        \n",
    "        # Initialize tracking for overall, home, away\n",
    "        teams = set(group['HomeTeam'].unique()) | set(group['AwayTeam'].unique())\n",
    "        team_stats_overall = {team: {'points': 0, 'gf': 0, 'ga': 0, 'matches': 0} for team in teams}\n",
    "        team_stats_home = {team: {'gf': 0, 'ga': 0, 'matches': 0} for team in teams}\n",
    "        team_stats_away = {team: {'gf': 0, 'ga': 0, 'matches': 0} for team in teams}\n",
    "        \n",
    "        # Goal patterns: overall, home-only, away-only\n",
    "        patterns_overall = {team: {f'{p}_{t}': 0 for p in ['sc', 'co', 'to'] for t in [2, 3]} for team in teams}\n",
    "        patterns_home = {team: {f'{p}_{t}': 0 for p in ['sc', 'co', 'to'] for t in [2, 3]} for team in teams}\n",
    "        patterns_away = {team: {f'{p}_{t}': 0 for p in ['sc', 'co', 'to'] for t in [2, 3]} for team in teams}\n",
    "        \n",
    "        current_round = None\n",
    "\n",
    "        for _, match in group.iterrows():\n",
    "            ht, at = match['HomeTeam'], match['AwayTeam']\n",
    "            hg, ag = match['FTHG'], match['FTAG']\n",
    "            match_key = (season, div, match['Date'], ht, at)\n",
    "            \n",
    "            # Store BEFORE match\n",
    "            if current_round is not None:\n",
    "                match_to_round[match_key] = current_round.copy()\n",
    "            \n",
    "            # Update overall stats\n",
    "            team_stats_overall[ht]['gf'] += hg\n",
    "            team_stats_overall[ht]['ga'] += ag\n",
    "            team_stats_overall[ht]['matches'] += 1\n",
    "            team_stats_overall[at]['gf'] += ag\n",
    "            team_stats_overall[at]['ga'] += hg\n",
    "            team_stats_overall[at]['matches'] += 1\n",
    "            \n",
    "            # Update home/away specific stats\n",
    "            team_stats_home[ht]['gf'] += hg\n",
    "            team_stats_home[ht]['ga'] += ag\n",
    "            team_stats_home[ht]['matches'] += 1\n",
    "            team_stats_away[at]['gf'] += ag\n",
    "            team_stats_away[at]['ga'] += hg\n",
    "            team_stats_away[at]['matches'] += 1\n",
    "            \n",
    "            # Update goal patterns - OVERALL\n",
    "            for t in [2, 3]:\n",
    "                if hg >= t: \n",
    "                    patterns_overall[ht][f'sc_{t}'] += 1\n",
    "                    patterns_home[ht][f'sc_{t}'] += 1\n",
    "                if ag >= t: \n",
    "                    patterns_overall[ht][f'co_{t}'] += 1\n",
    "                    patterns_home[ht][f'co_{t}'] += 1\n",
    "                    patterns_overall[at][f'sc_{t}'] += 1\n",
    "                    patterns_away[at][f'sc_{t}'] += 1\n",
    "                if hg + ag >= t: \n",
    "                    patterns_overall[ht][f'to_{t}'] += 1\n",
    "                    patterns_overall[at][f'to_{t}'] += 1\n",
    "                    patterns_home[ht][f'to_{t}'] += 1\n",
    "                    patterns_away[at][f'to_{t}'] += 1\n",
    "                if hg >= t:\n",
    "                    patterns_overall[at][f'co_{t}'] += 1\n",
    "                    patterns_away[at][f'co_{t}'] += 1\n",
    "            \n",
    "            # Points\n",
    "            if match['FTR'] == 'H': team_stats_overall[ht]['points'] += 3\n",
    "            elif match['FTR'] == 'A': team_stats_overall[at]['points'] += 3\n",
    "            else: \n",
    "                team_stats_overall[ht]['points'] += 1\n",
    "                team_stats_overall[at]['points'] += 1\n",
    "            \n",
    "            # Check round completion\n",
    "            if len(set(s['matches'] for s in team_stats_overall.values())) == 1:\n",
    "                rows = []\n",
    "                for team in teams:\n",
    "                    s_o = team_stats_overall[team]\n",
    "                    s_h = team_stats_home[team]\n",
    "                    s_a = team_stats_away[team]\n",
    "                    \n",
    "                    row = {\n",
    "                        'Season': season, 'Div': div, 'Team': team,\n",
    "                        'Points': s_o['points'], 'Matches': s_o['matches'],\n",
    "                        'Goals_For': s_o['gf'], 'Goals_Against': s_o['ga']\n",
    "                    }\n",
    "                    \n",
    "                    # Overall goal patterns (count + pct)\n",
    "                    for t in [2, 3]:\n",
    "                        for p, full in [('sc', 'scored'), ('co', 'conceded'), ('to', 'total')]:\n",
    "                            cnt = patterns_overall[team][f'{p}_{t}']\n",
    "                            row[f'{full}_{t}plus_count'] = cnt\n",
    "                            row[f'{full}_{t}plus_pct'] = round(cnt / s_o['matches'] * 100, 1) if s_o['matches'] > 0 else 0\n",
    "                    \n",
    "                    # Home-specific goal patterns\n",
    "                    for t in [2, 3]:\n",
    "                        for p, full in [('sc', 'scored'), ('co', 'conceded'), ('to', 'total')]:\n",
    "                            cnt = patterns_home[team][f'{p}_{t}']\n",
    "                            row[f'home_{full}_{t}plus_count'] = cnt\n",
    "                            row[f'home_{full}_{t}plus_pct'] = round(cnt / s_h['matches'] * 100, 1) if s_h['matches'] > 0 else 0\n",
    "                    \n",
    "                    # Away-specific goal patterns\n",
    "                    for t in [2, 3]:\n",
    "                        for p, full in [('sc', 'scored'), ('co', 'conceded'), ('to', 'total')]:\n",
    "                            cnt = patterns_away[team][f'{p}_{t}']\n",
    "                            row[f'away_{full}_{t}plus_count'] = cnt\n",
    "                            row[f'away_{full}_{t}plus_pct'] = round(cnt / s_a['matches'] * 100, 1) if s_a['matches'] > 0 else 0\n",
    "                    \n",
    "                    rows.append(row)\n",
    "                \n",
    "                current_round = pd.DataFrame(rows).sort_values('Points', ascending=False)\n",
    "                current_round['Position'] = range(1, len(current_round) + 1)\n",
    "        \n",
    "        if current_round is not None:\n",
    "            standings_list.append(current_round)\n",
    "    \n",
    "    final = pd.concat(standings_list, ignore_index=True) if standings_list else pd.DataFrame()\n",
    "    return final, match_to_round\n",
    "\n",
    "# Calculate standings\n",
    "df_season_standings, match_round_map = calculate_standings_with_rounds(df_basic)\n",
    "print(f\"✓ Season standings: {len(df_season_standings)} team-season records\")\n",
    "print(f\"  Round-based mappings: {len(match_round_map)} matches with historical standings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create historical position features with lookback logic\n",
    "season_list = sorted(df_basic['Season'].unique())\n",
    "season_to_order = {season: idx for idx, season in enumerate(season_list)}\n",
    "df_basic['season_order'] = df_basic['Season'].map(season_to_order)\n",
    "\n",
    "# Extract season years and create position lookup\n",
    "def extract_season_year(season_str):\n",
    "    return int(season_str.split('/')[0])\n",
    "\n",
    "df_season_standings['season_year'] = df_season_standings['Season'].apply(extract_season_year)\n",
    "unique_season_years = sorted(df_season_standings['season_year'].unique())\n",
    "\n",
    "# Add percentile rankings for cross-league comparability\n",
    "df_season_standings['league_size'] = df_season_standings.groupby(['Season', 'Div'])['Position'].transform('max')\n",
    "df_season_standings['Position_Percentile'] = (\n",
    "    (df_season_standings['league_size'] - df_season_standings['Position'] + 1) /\n",
    "    df_season_standings['league_size'] * 100\n",
    ").round(2)\n",
    "percentile_lookup = df_season_standings.set_index(['Season', 'Div', 'Team'])['Position_Percentile'].to_dict()\n",
    "\n",
    "# Create columns for percentile positions only (not raw positions)\n",
    "for year in unique_season_years:\n",
    "    df_basic[f'home_position_pct_{year}'] = np.nan\n",
    "    df_basic[f'away_position_pct_{year}'] = np.nan\n",
    "\n",
    "# Populate historical features\n",
    "for idx, row in df_basic.iterrows():\n",
    "    current_season_order = row['season_order']\n",
    "    current_season = row['Season']\n",
    "    match_key = (row['Season'], row['Div'], row['Date'], row['HomeTeam'], row['AwayTeam'])\n",
    "\n",
    "    for year in unique_season_years:\n",
    "        target_season = next((s for s in season_list if extract_season_year(s) == year), None)\n",
    "        if not target_season:\n",
    "            continue\n",
    "        \n",
    "        target_season_order = season_to_order[target_season]\n",
    "\n",
    "        # For past seasons: use final standings\n",
    "        if target_season_order < current_season_order:\n",
    "            for team_type, team in [('home', row['HomeTeam']), ('away', row['AwayTeam'])]:\n",
    "                key = next(\n",
    "                    ((target_season, div, team) for div in df_season_standings['Div'].unique()\n",
    "                     if (target_season, div, team) in percentile_lookup),\n",
    "                    None\n",
    "                )\n",
    "                if key:\n",
    "                    df_basic.loc[idx, f'{team_type}_position_pct_{year}'] = percentile_lookup[key]\n",
    "\n",
    "        # For current season (same season as match): use round-based standings\n",
    "        elif target_season == current_season and match_key in match_round_map:\n",
    "            round_standings = match_round_map[match_key]\n",
    "            for team_type, team in [('home', row['HomeTeam']), ('away', row['AwayTeam'])]:\n",
    "                team_row = round_standings[round_standings['Team'] == team]\n",
    "                if not team_row.empty:\n",
    "                    position = team_row['Position'].iloc[0]\n",
    "                    league_size = len(round_standings)\n",
    "                    percentile = ((league_size - position + 1) / league_size * 100)\n",
    "                    df_basic.loc[idx, f'{team_type}_position_pct_{year}'] = round(percentile, 2)\n",
    "\n",
    "position_cols = [col for col in df_basic.columns if 'position_pct_' in col]\n",
    "print(f\"✓ Historical position features: {len(position_cols)} columns\")\n",
    "print(f\"  Past seasons: final standings | Current season: round-based standings\")\n",
    "print(f\"  Percentile scale: 0-100 (higher = better, normalized across leagues)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add historical goal-scoring pattern features with home/away context\n",
    "# Create lookups from final standings\n",
    "goal_stat_lookups = {}\n",
    "for threshold in [2, 3]:\n",
    "    for prefix in ['scored', 'conceded', 'total']:\n",
    "        # Overall stats\n",
    "        for suffix in ['count', 'pct']:\n",
    "            col = f'{prefix}_{threshold}plus_{suffix}'\n",
    "            goal_stat_lookups[col] = df_season_standings.set_index(['Season', 'Div', 'Team'])[col].to_dict()\n",
    "        # Home-specific stats\n",
    "        for suffix in ['count', 'pct']:\n",
    "            col = f'home_{prefix}_{threshold}plus_{suffix}'\n",
    "            goal_stat_lookups[col] = df_season_standings.set_index(['Season', 'Div', 'Team'])[col].to_dict()\n",
    "        # Away-specific stats\n",
    "        for suffix in ['count', 'pct']:\n",
    "            col = f'away_{prefix}_{threshold}plus_{suffix}'\n",
    "            goal_stat_lookups[col] = df_season_standings.set_index(['Season', 'Div', 'Team'])[col].to_dict()\n",
    "\n",
    "# Create columns: home team gets overall + home-specific, away team gets overall + away-specific\n",
    "for year in unique_season_years:\n",
    "    for threshold in [2, 3]:\n",
    "        for prefix in ['scored', 'conceded', 'total']:\n",
    "            # Home team: overall + home-specific\n",
    "            df_basic[f'home_{prefix}_{threshold}plus_count_{year}'] = np.nan\n",
    "            df_basic[f'home_{prefix}_{threshold}plus_pct_{year}'] = np.nan\n",
    "            df_basic[f'home_home_{prefix}_{threshold}plus_count_{year}'] = np.nan\n",
    "            df_basic[f'home_home_{prefix}_{threshold}plus_pct_{year}'] = np.nan\n",
    "            # Away team: overall + away-specific\n",
    "            df_basic[f'away_{prefix}_{threshold}plus_count_{year}'] = np.nan\n",
    "            df_basic[f'away_{prefix}_{threshold}plus_pct_{year}'] = np.nan\n",
    "            df_basic[f'away_away_{prefix}_{threshold}plus_count_{year}'] = np.nan\n",
    "            df_basic[f'away_away_{prefix}_{threshold}plus_pct_{year}'] = np.nan\n",
    "\n",
    "# Populate goal statistics\n",
    "for idx, row in df_basic.iterrows():\n",
    "    current_season_order = row['season_order']\n",
    "    current_season = row['Season']\n",
    "    match_key = (row['Season'], row['Div'], row['Date'], row['HomeTeam'], row['AwayTeam'])\n",
    "\n",
    "    for year in unique_season_years:\n",
    "        target_season = next((s for s in season_list if extract_season_year(s) == year), None)\n",
    "        if not target_season:\n",
    "            continue\n",
    "\n",
    "        # For past seasons: use final standings\n",
    "        if season_to_order[target_season] < current_season_order:\n",
    "            # Home team\n",
    "            key_home = next(((target_season, div, row['HomeTeam']) for div in df_season_standings['Div'].unique()\n",
    "                             if (target_season, div, row['HomeTeam']) in percentile_lookup), None)\n",
    "            if key_home:\n",
    "                for threshold in [2, 3]:\n",
    "                    for prefix in ['scored', 'conceded', 'total']:\n",
    "                        # Overall stats\n",
    "                        for suffix in ['count', 'pct']:\n",
    "                            col = f'{prefix}_{threshold}plus_{suffix}'\n",
    "                            val = goal_stat_lookups[col].get(key_home)\n",
    "                            if val is not None:\n",
    "                                df_basic.loc[idx, f'home_{col}_{year}'] = val\n",
    "                        # Home-specific stats\n",
    "                        for suffix in ['count', 'pct']:\n",
    "                            col = f'home_{prefix}_{threshold}plus_{suffix}'\n",
    "                            val = goal_stat_lookups[col].get(key_home)\n",
    "                            if val is not None:\n",
    "                                df_basic.loc[idx, f'home_{col}_{year}'] = val\n",
    "            \n",
    "            # Away team\n",
    "            key_away = next(((target_season, div, row['AwayTeam']) for div in df_season_standings['Div'].unique()\n",
    "                             if (target_season, div, row['AwayTeam']) in percentile_lookup), None)\n",
    "            if key_away:\n",
    "                for threshold in [2, 3]:\n",
    "                    for prefix in ['scored', 'conceded', 'total']:\n",
    "                        # Overall stats\n",
    "                        for suffix in ['count', 'pct']:\n",
    "                            col = f'{prefix}_{threshold}plus_{suffix}'\n",
    "                            val = goal_stat_lookups[col].get(key_away)\n",
    "                            if val is not None:\n",
    "                                df_basic.loc[idx, f'away_{col}_{year}'] = val\n",
    "                        # Away-specific stats\n",
    "                        for suffix in ['count', 'pct']:\n",
    "                            col = f'away_{prefix}_{threshold}plus_{suffix}'\n",
    "                            val = goal_stat_lookups[col].get(key_away)\n",
    "                            if val is not None:\n",
    "                                df_basic.loc[idx, f'away_{col}_{year}'] = val\n",
    "\n",
    "        # For current season: use round-based standings\n",
    "        elif target_season == current_season and match_key in match_round_map:\n",
    "            round_standings = match_round_map[match_key]\n",
    "            \n",
    "            # Home team\n",
    "            home_row = round_standings[round_standings['Team'] == row['HomeTeam']]\n",
    "            if not home_row.empty:\n",
    "                for threshold in [2, 3]:\n",
    "                    for prefix in ['scored', 'conceded', 'total']:\n",
    "                        # Overall stats\n",
    "                        for suffix in ['count', 'pct']:\n",
    "                            col = f'{prefix}_{threshold}plus_{suffix}'\n",
    "                            if col in home_row.columns:\n",
    "                                df_basic.loc[idx, f'home_{col}_{year}'] = home_row[col].iloc[0]\n",
    "                        # Home-specific stats\n",
    "                        for suffix in ['count', 'pct']:\n",
    "                            col = f'home_{prefix}_{threshold}plus_{suffix}'\n",
    "                            if col in home_row.columns:\n",
    "                                df_basic.loc[idx, f'home_{col}_{year}'] = home_row[col].iloc[0]\n",
    "            \n",
    "            # Away team\n",
    "            away_row = round_standings[round_standings['Team'] == row['AwayTeam']]\n",
    "            if not away_row.empty:\n",
    "                for threshold in [2, 3]:\n",
    "                    for prefix in ['scored', 'conceded', 'total']:\n",
    "                        # Overall stats\n",
    "                        for suffix in ['count', 'pct']:\n",
    "                            col = f'{prefix}_{threshold}plus_{suffix}'\n",
    "                            if col in away_row.columns:\n",
    "                                df_basic.loc[idx, f'away_{col}_{year}'] = away_row[col].iloc[0]\n",
    "                        # Away-specific stats\n",
    "                        for suffix in ['count', 'pct']:\n",
    "                            col = f'away_{prefix}_{threshold}plus_{suffix}'\n",
    "                            if col in away_row.columns:\n",
    "                                df_basic.loc[idx, f'away_{col}_{year}'] = away_row[col].iloc[0]\n",
    "\n",
    "goal_stat_cols = [col for col in df_basic.columns if any(f'{p}_{t}plus' in col for p in ['scored', 'conceded', 'total'] for t in [2, 3])]\n",
    "print(f\"✓ Historical goal statistics: {len(goal_stat_cols)} columns\")\n",
    "print(f\"  Home team: Overall + Home-specific (counts & percentages)\")\n",
    "print(f\"  Away team: Overall + Away-specific (counts & percentages)\")\n",
    "print(f\"  Format: home/away_[home/away]_[scored/conceded/total]_[2/3]plus_[count/pct]_[year]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of Section 4.3.4:**\n",
    "\n",
    "We've created comprehensive historical performance features with **round-based tracking and home/away context**:\n",
    "\n",
    "1. **Season Standings** (`df_season_standings`): Team-season records with home/away splits\n",
    "   - **Past seasons:** Final standings (complete data)\n",
    "   - **Current season:** Round-by-round standings (last completed round before each match)\n",
    "   - Metrics: Points, Position, Goals, Goal patterns with home/away context\n",
    "\n",
    "2. **Historical Position Features** (13 columns added to `df_basic`):\n",
    "   - **Overall position only** (no home/away split - league position is always overall)\n",
    "   - **Percentile rankings** (0-100): Normalized across league sizes\n",
    "   - Format: `home_position_pct_YYYY`, `away_position_pct_YYYY` for 6 seasons\n",
    "   - Higher percentile = better position (95th = top of table)\n",
    "   - Uses round-based data for current season to maximize data usage\n",
    "\n",
    "3. **Historical Goal Pattern Features** (288 columns added to `df_basic`):\n",
    "   - **Context-aware:** Home team gets overall + home-specific, away team gets overall + away-specific\n",
    "   - For each context: 6 statistics × 2 metrics (count & percentage)\n",
    "     - `scored_2plus/3plus`: Matches where team scored 2+/3+ goals\n",
    "     - `conceded_2plus/3plus`: Matches where team conceded 2+/3+ goals\n",
    "     - `total_2plus/3plus`: Matches with 2+/3+ total goals\n",
    "   - **Both counts AND percentages** included:\n",
    "     - Counts show sample size (reliability indicator)\n",
    "     - Percentages show the pattern/trend\n",
    "     - Example: 80% from 4 matches vs 50% from 1 match - model learns reliability\n",
    "   - Format: `home_home_scored_3plus_pct_2024` = home team's home-only scoring %\n",
    "   - Format: `away_away_conceded_2plus_count_2024` = away team's away-only conceding count\n",
    "\n",
    "**Key Design Choices:**\n",
    "- **Home/Away Context:** Teams perform differently at home vs away - model gets both perspectives\n",
    "- **Counts + Percentages:** Dual metrics allow model to weight by sample size\n",
    "- **Lookback logic:** Prevents data leakage (features only reference past seasons)\n",
    "- **Percentiles:** Enable fair comparison between different league sizes\n",
    "- **2+ and 3+ thresholds:** Most relevant for Over/Under 2.5 target\n",
    "- **Current season handling:** Uses round-based data to maximize coverage (~95%)\n",
    "\n",
    "**Total features added: 301** (13 position percentiles + 288 goal patterns)\n",
    "\n",
    "**Expected Impact:** Context-aware goal patterns (home team's home performance vs away team's away performance) should provide stronger predictive signals than overall statistics alone. Including both counts and percentages helps model assess reliability of patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.5 Additional Derived Features\n",
    "\n",
    "**Create additional features from existing data that might improve Over/Under 2.5 prediction:**\n",
    "\n",
    "**Team strength differential features:**\n",
    "- Position percentile difference between home and away team (quality gap, normalized across leagues)\n",
    "- Combined attacking/defensive strength indicators (blend recent form + historical patterns)\n",
    "\n",
    "**Form-based features:**\n",
    "- Recent form trend (improving vs declining over last 10 matches)\n",
    "\n",
    "**Contextual features:**\n",
    "- Season progress (early, mid, late season effects)\n",
    "- Days rest advantage (difference in rest days between teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Position difference features (most recent season available for both teams)\n",
    "df_basic['position_pct_diff'] = np.nan\n",
    "\n",
    "for idx, row in df_basic.iterrows():\n",
    "    # Find most recent season where both teams have position data\n",
    "    for year in reversed(unique_season_years):\n",
    "        home_pct_col = f'home_position_pct_{year}'\n",
    "        away_pct_col = f'away_position_pct_{year}'\n",
    "\n",
    "        if (pd.notna(df_basic.loc[idx, home_pct_col]) and\n",
    "            pd.notna(df_basic.loc[idx, away_pct_col])):\n",
    "            # Positive = home team has better position (higher percentile)\n",
    "            df_basic.loc[idx, 'position_pct_diff'] = df_basic.loc[idx, home_pct_col] - df_basic.loc[idx, away_pct_col]\n",
    "            break\n",
    "\n",
    "print(f\"✓ Position percentile difference feature created\")\n",
    "print(f\"  Coverage: {(df_basic['position_pct_diff'].notna().sum() / len(df_basic) * 100):.1f}%\")\n",
    "\n",
    "# 2. Combined attacking strength using context-aware stats\n",
    "# Home team uses home-specific stats, away team uses away-specific stats\n",
    "df_basic['combined_attack_strength'] = np.nan\n",
    "df_basic['combined_defense_weakness'] = np.nan\n",
    "\n",
    "for idx, row in df_basic.iterrows():\n",
    "    # Get most recent goal statistics (context-aware: home-specific for home team, away-specific for away)\n",
    "    for year in reversed(unique_season_years):\n",
    "        # Home team: use home-specific scoring\n",
    "        home_scored = df_basic.loc[idx, f'home_home_scored_3plus_pct_{year}']\n",
    "        home_conceded = df_basic.loc[idx, f'home_home_conceded_3plus_pct_{year}']\n",
    "        # Away team: use away-specific scoring\n",
    "        away_scored = df_basic.loc[idx, f'away_away_scored_3plus_pct_{year}']\n",
    "        away_conceded = df_basic.loc[idx, f'away_away_conceded_3plus_pct_{year}']\n",
    "\n",
    "        if pd.notna(home_scored) and pd.notna(away_scored):\n",
    "            # Combine recent MA with historical patterns\n",
    "            home_recent = df_basic.loc[idx, 'home_goals_ma5']\n",
    "            away_recent = df_basic.loc[idx, 'away_goals_ma5']\n",
    "\n",
    "            # Weighted average: 60% recent form, 40% historical context-specific pattern\n",
    "            df_basic.loc[idx, 'combined_attack_strength'] = (\n",
    "                0.6 * (home_recent + away_recent) +\n",
    "                0.4 * ((home_scored + away_scored) / 20)  # Normalize percentage to goals scale\n",
    "            )\n",
    "            df_basic.loc[idx, 'combined_defense_weakness'] = (\n",
    "                0.6 * (df_basic.loc[idx, 'home_conceded_ma5'] + df_basic.loc[idx, 'away_conceded_ma5']) +\n",
    "                0.4 * ((home_conceded + away_conceded) / 20)\n",
    "            )\n",
    "            break\n",
    "\n",
    "print(f\"✓ Combined strength features created (using home/away context)\")\n",
    "print(f\"  Coverage: {(df_basic['combined_attack_strength'].notna().sum() / len(df_basic) * 100):.1f}%\")\n",
    "\n",
    "# 3. Form trend (improving vs declining)\n",
    "df_basic['home_form_trend'] = np.nan\n",
    "df_basic['away_form_trend'] = np.nan\n",
    "\n",
    "for team in df_basic['HomeTeam'].unique():\n",
    "    # Home matches\n",
    "    home_mask = df_basic['HomeTeam'] == team\n",
    "    home_dates = df_basic[home_mask].sort_values('Date').index\n",
    "    for i, idx in enumerate(home_dates):\n",
    "        if i >= 10:  # Need at least 10 matches\n",
    "            last_5 = df_basic.loc[home_dates[i-5:i], 'FTHG'].mean()\n",
    "            prev_5 = df_basic.loc[home_dates[i-10:i-5], 'FTHG'].mean()\n",
    "            df_basic.loc[idx, 'home_form_trend'] = last_5 - prev_5  # Positive = improving\n",
    "\n",
    "    # Away matches\n",
    "    away_mask = df_basic['AwayTeam'] == team\n",
    "    away_dates = df_basic[away_mask].sort_values('Date').index\n",
    "    for i, idx in enumerate(away_dates):\n",
    "        if i >= 10:\n",
    "            last_5 = df_basic.loc[away_dates[i-5:i], 'FTAG'].mean()\n",
    "            prev_5 = df_basic.loc[away_dates[i-10:i-5], 'FTAG'].mean()\n",
    "            df_basic.loc[idx, 'away_form_trend'] = last_5 - prev_5\n",
    "\n",
    "# Fill NaN with 0 (no trend info = assume stable)\n",
    "df_basic['home_form_trend'].fillna(0, inplace=True)\n",
    "df_basic['away_form_trend'].fillna(0, inplace=True)\n",
    "\n",
    "print(f\"✓ Form trend features created\")\n",
    "\n",
    "# 4. Rest days advantage\n",
    "df_basic['rest_days_advantage'] = df_basic['home_days_since_last'] - df_basic['away_days_since_last']\n",
    "\n",
    "print(f\"✓ Rest days advantage created\")\n",
    "\n",
    "# 5. Season progress (match number / expected total matches based on league structure)\n",
    "# NO DATA LEAKAGE: Uses team count (known from first match) and round-robin format\n",
    "# Expected matches = N × (N-1) where N = number of teams in the league\n",
    "df_basic['season_progress'] = np.nan\n",
    "for season_div, group in df_basic.groupby(['Season', 'Div']):\n",
    "    # Count unique teams in this season/division (structural information)\n",
    "    teams_in_league = len(group['HomeTeam'].unique())\n",
    "    # Calculate expected total matches for round-robin (home & away)\n",
    "    expected_total_matches = teams_in_league * (teams_in_league - 1)\n",
    "    \n",
    "    # Sort by date to get chronological order\n",
    "    sorted_indices = group.sort_values('Date').index\n",
    "    \n",
    "    for i, idx in enumerate(sorted_indices):\n",
    "        match_number = i + 1\n",
    "        df_basic.loc[idx, 'season_progress'] = match_number / expected_total_matches\n",
    "\n",
    "print(f\"✓ Season progress feature created (using league structure)\")\n",
    "\n",
    "# Summary\n",
    "new_derived_features = ['position_pct_diff', 'combined_attack_strength',\n",
    "                        'combined_defense_weakness', 'home_form_trend', 'away_form_trend',\n",
    "                        'rest_days_advantage', 'season_progress']\n",
    "print(f\"\\n✓ Total new derived features: {len(new_derived_features)}\")\n",
    "print(f\"  Features: {new_derived_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify season_progress fix - check values are reasonable\n",
    "print(\"Season Progress Verification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check range\n",
    "print(f\"\\nSeason progress range: {df_basic['season_progress'].min():.4f} to {df_basic['season_progress'].max():.4f}\")\n",
    "\n",
    "# Check a few examples from different league sizes\n",
    "for season_div in [('2024/2025', 'E0'), ('2024/2025', 'SP1'), ('2024/2025', 'SC0')]:\n",
    "    season, div = season_div\n",
    "    mask = (df_basic['Season'] == season) & (df_basic['Div'] == div)\n",
    "    group = df_basic[mask].copy()\n",
    "    \n",
    "    if len(group) > 0:\n",
    "        teams = len(group['HomeTeam'].unique())\n",
    "        expected = teams * (teams - 1)\n",
    "        progress_values = group.sort_values('Date')['season_progress'].values\n",
    "        \n",
    "        print(f\"\\n{season} {div}:\")\n",
    "        print(f\"  Teams: {teams}, Expected matches: {expected}\")\n",
    "        print(f\"  Matches so far: {len(group)}\")\n",
    "        print(f\"  Progress range: {progress_values.min():.4f} to {progress_values.max():.4f}\")\n",
    "        print(f\"  First 3 matches progress: {progress_values[:3]}\")\n",
    "\n",
    "print(\"\\n✓ Season progress values verified - using league structure (no data leakage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Leakage Prevention Summary\n",
    "\n",
    "**Season Progress Feature Fix:**\n",
    "- **Problem**: Previously used `len(group)` which included ALL matches in the season (including future unplayed matches)\n",
    "- **Solution**: Calculate expected matches using league structure: `teams × (teams - 1)`\n",
    "- **Why this is NOT leakage**: \n",
    "  - Number of teams is known from the first match\n",
    "  - League format (round-robin home & away) is predetermined\n",
    "  - Similar to knowing NBA has 82 games or Premier League has 38 rounds\n",
    "  - We're using structural information, not future match results\n",
    "\n",
    "**All Features Verified for Data Leakage:**\n",
    "✅ MA5 features: Use `Date < date` or index slicing `[i-5:i]` to exclude current match  \n",
    "✅ Combined strength: Uses pre-calculated MA5 (which is safe)  \n",
    "✅ Form trend: Uses historical windows `[i-5:i]` and `[i-10:i-5]`  \n",
    "✅ Season progress: Now uses league structure (team count)  \n",
    "✅ Extended stats MA5: Uses `Date < date` filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.X Feature Importance Testing\n",
    "\n",
    "**Test which features matter for predicting Over/Under 2.5 goals**\n",
    "\n",
    "**Correlation Analysis:**\n",
    "- Quick initial screening of linear relationships\n",
    "- Identifies features with direct linear impact on target\n",
    "- Visualize top correlated features by group (time-based, position, goal patterns)\n",
    "- **Limitation:** Only captures linear relationships, misses non-linear patterns and interactions\n",
    "\n",
    "**Note:** While correlations are weak, features may still be valuable in tree-based models that capture non-linear patterns, thresholds, and feature interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Correlation Analysis\n",
    "\n",
    "**Quick screening of linear relationships with Over/Under 2.5 target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features to test\n",
    "num_features = ['home_days_since_last', 'away_days_since_last',\n",
    "                'home_goals_ma5', 'home_conceded_ma5', 'away_goals_ma5', 'away_conceded_ma5',\n",
    "                'league_tier', 'month']\n",
    "\n",
    "# Add historical position features (from section 4.3.4) - percentiles only\n",
    "position_features = [col for col in df_basic.columns if 'position_pct_' in col]\n",
    "\n",
    "# Add historical goal statistics features (from sections 4.3.4 and 4.3.5)\n",
    "# Includes: overall, home-specific, away-specific stats × counts and percentages\n",
    "goal_stat_features = [col for col in df_basic.columns if any(f'{p}_{t}plus' in col for p in ['scored', 'conceded', 'total'] for t in [2, 3])]\n",
    "\n",
    "all_num_features = num_features + position_features + goal_stat_features\n",
    "\n",
    "# Calculate correlations\n",
    "print(\"=\" * 60)\n",
    "print(\"CORRELATIONS WITH over_2_5 TARGET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. TIME-BASED FEATURES:\")\n",
    "time_corrs = []\n",
    "for feat in num_features:\n",
    "    corr = df_basic[feat].corr(df_basic['over_2_5'])\n",
    "    time_corrs.append((feat, corr))\n",
    "    print(f\"  {feat:30s}: {corr:7.4f}\")\n",
    "\n",
    "print(f\"\\n2. HISTORICAL POSITION FEATURES ({len(position_features)} total):\")\n",
    "# Group by season year for cleaner display\n",
    "position_corrs = []\n",
    "for feat in position_features:\n",
    "    corr = df_basic[feat].corr(df_basic['over_2_5'])\n",
    "    position_corrs.append((feat, corr))\n",
    "    print(f\"  {feat:30s}: {corr:7.4f}\")\n",
    "\n",
    "# Summary statistics for position features\n",
    "position_corr_values = [abs(c[1]) for c in position_corrs if not pd.isna(c[1])]\n",
    "if position_corr_values:\n",
    "    print(f\"\\n  Position features summary:\")\n",
    "    print(f\"    Max |correlation|: {max(position_corr_values):.4f}\")\n",
    "    print(f\"    Mean |correlation|: {np.mean(position_corr_values):.4f}\")\n",
    "    print(f\"    Median |correlation|: {np.median(position_corr_values):.4f}\")\n",
    "\n",
    "print(f\"\\n3. HISTORICAL GOAL PATTERN FEATURES ({len(goal_stat_features)} total):\")\n",
    "goal_stat_corrs = []\n",
    "for feat in goal_stat_features:\n",
    "    corr = df_basic[feat].corr(df_basic['over_2_5'])\n",
    "    goal_stat_corrs.append((feat, corr))\n",
    "    print(f\"  {feat:40s}: {corr:7.4f}\")\n",
    "\n",
    "# Summary statistics for goal stat features\n",
    "goal_stat_corr_values = [abs(c[1]) for c in goal_stat_corrs if not pd.isna(c[1])]\n",
    "if goal_stat_corr_values:\n",
    "    print(f\"\\n  Goal stat features summary:\")\n",
    "    print(f\"    Max |correlation|: {max(goal_stat_corr_values):.4f}\")\n",
    "    print(f\"    Mean |correlation|: {np.mean(goal_stat_corr_values):.4f}\")\n",
    "    print(f\"    Median |correlation|: {np.median(goal_stat_corr_values):.4f}\")\n",
    "    \n",
    "# Show top features by absolute correlation\n",
    "all_corrs = time_corrs + position_corrs + goal_stat_corrs\n",
    "all_corrs_sorted = sorted(all_corrs, key=lambda x: abs(x[1]) if not pd.isna(x[1]) else 0, reverse=True)\n",
    "\n",
    "print(f\"\\n4. TOP 15 FEATURES BY ABSOLUTE CORRELATION:\")\n",
    "for feat, corr in all_corrs_sorted[:15]:\n",
    "    print(f\"  {feat:40s}: {corr:7.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 1. Time-based features\n",
    "time_corrs = [df_basic[f].corr(df_basic['over_2_5']) for f in num_features]\n",
    "axes[0].barh(num_features, time_corrs, color='steelblue')\n",
    "axes[0].axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "axes[0].set_xlabel('Correlation with Over 2.5', fontsize=11)\n",
    "axes[0].set_title('Time-Based Features', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Position features - show top 10 by absolute correlation\n",
    "position_corr_df = pd.DataFrame(position_corrs, columns=['feature', 'correlation'])\n",
    "position_corr_df['abs_corr'] = position_corr_df['correlation'].abs()\n",
    "top_position = position_corr_df.nlargest(10, 'abs_corr')\n",
    "axes[1].barh(range(len(top_position)), top_position['correlation'].values, color='coral')\n",
    "axes[1].set_yticks(range(len(top_position)))\n",
    "axes[1].set_yticklabels([f.replace('home_', 'H_').replace('away_', 'A_').replace('position_', 'pos_').replace('_pct', '%') for f in top_position['feature']], fontsize=8)\n",
    "axes[1].axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "axes[1].set_xlabel('Correlation with Over 2.5', fontsize=11)\n",
    "axes[1].set_title(f'Top 10 Position Features (of {len(position_features)})', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Goal stats - show top 10 by absolute correlation\n",
    "goal_corr_df = pd.DataFrame(goal_stat_corrs, columns=['feature', 'correlation'])\n",
    "goal_corr_df['abs_corr'] = goal_corr_df['correlation'].abs()\n",
    "top_goals = goal_corr_df.nlargest(10, 'abs_corr')\n",
    "axes[2].barh(range(len(top_goals)), top_goals['correlation'].values, color='mediumseagreen')\n",
    "axes[2].set_yticks(range(len(top_goals)))\n",
    "axes[2].set_yticklabels([f.replace('home_', 'H_').replace('away_', 'A_').replace('_pct_season_', '_S') for f in top_goals['feature']], fontsize=8)\n",
    "axes[2].axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "axes[2].set_xlabel('Correlation with Over 2.5', fontsize=11)\n",
    "axes[2].set_title(f'Top 10 Goal Pattern Features (of {len(goal_stat_features)})', fontsize=12, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CORRELATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Overall weak correlations suggest non-linear relationships are important.\")\n",
    "print(f\"Tree-based models will likely perform better than linear models.\")\n",
    "print(f\"\\nStrongest absolute correlations:\")\n",
    "all_corrs = pd.concat([\n",
    "    pd.DataFrame({'feature': num_features, 'correlation': time_corrs}),\n",
    "    position_corr_df[['feature', 'correlation']],\n",
    "    goal_corr_df[['feature', 'correlation']]\n",
    "])\n",
    "all_corrs['abs_corr'] = all_corrs['correlation'].abs()\n",
    "top_overall = all_corrs.nlargest(5, 'abs_corr')\n",
    "for _, row in top_overall.iterrows():\n",
    "    print(f\"  {row['feature']:50s}: {row['correlation']:7.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of Correlation Analysis**\n",
    "\n",
    "**Overall Finding:** All features show **very weak correlations** with the Over/Under 2.5 target (|r| < 0.10), indicating minimal linear relationships.\n",
    "\n",
    "**1. Time-Based Features:**\n",
    "- **Days since last match** (~-0.01): Virtually no effect — rest periods don't linearly predict goal totals\n",
    "- **Home goals MA5** (+0.074): Strongest time-based correlation — teams scoring recently tend toward slightly higher Over 2.5 rates\n",
    "- **Away goals MA5** (+0.048): Weak positive signal for away team scoring form\n",
    "- **Home/Away conceded MA5** (~-0.04 to -0.05): Slight negative — defensive stability correlates with fewer goals\n",
    "- **League tier** (-0.042): Lower divisions marginally less likely to see 3+ goals\n",
    "- **Month** (+0.019): Negligible seasonal effect\n",
    "\n",
    "**2. Historical Position Features (12 total - percentiles only):**\n",
    "- **Summary statistics:**\n",
    "  - Max |correlation|: ~0.04-0.06 (very weak)\n",
    "  - Mean |correlation|: ~0.02-0.03\n",
    "  - Median |correlation|: ~0.02\n",
    "- **Top correlations:** Home/away percentile positions from recent seasons (S-1, S-2) show slightly stronger effects\n",
    "- **Insight:** Position matters, but not in a simple linear way — likely interacts with other factors (e.g., promoted teams behave differently than established teams at same position)\n",
    "- **Scale:** 0-100 percentile where higher = better position (normalized across different league sizes)\n",
    "\n",
    "**3. Historical Goal Pattern Features (72 total):**\n",
    "- **Summary statistics:**\n",
    "  - Max |correlation|: ~0.05-0.07\n",
    "  - Mean |correlation|: ~0.02-0.03\n",
    "  - Top features: Typically `total_2plus_pct` or `scored_2plus_pct` from recent seasons (S-1, S-2)\n",
    "- **Insight:** Teams with history of high-scoring matches continue that pattern, but effect is weak linearly\n",
    "- **Strongest patterns:** Home team total goals 2+ percentages from recent seasons show positive correlations (0.04-0.07)\n",
    "\n",
    "**4. Key Takeaways:**\n",
    "- ✅ **Weak linear relationships** suggest soccer is governed by **non-linear patterns and interactions**\n",
    "- ✅ **Recent form** (MA5 goals) shows slightly stronger signal than historical season data in linear terms\n",
    "- ✅ **Goal patterns persist** — teams with high-scoring history continue that tendency, but modestly\n",
    "- ✅ **Percentile positions** provide normalized strength indicator across different league sizes\n",
    "- ⚠️ **Modeling implication:** Linear models (e.g., logistic regression) will struggle. Tree-based models (Random Forest, XGBoost) better suited to capture:\n",
    "  - Non-linear thresholds (e.g., optimal rest days 4-6, not linear)\n",
    "  - Interactions (e.g., promoted + low position ≠ stable low position)\n",
    "  - Context-dependent effects (e.g., position matters more in competitive leagues)\n",
    "\n",
    "**Conclusion:** While individual correlations are weak, these features will likely contribute to model performance through interactions and non-linear patterns when combined in ensemble models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Summary: What We Added**\n",
    "\n",
    "**New Features (Section 4.3.5):**\n",
    "1. **Position percentile difference** - Quality gap between teams using normalized percentiles (higher = home team stronger)\n",
    "2. **Combined attack/defense strength** - Blend recent form (60%) + historical patterns (40%)\n",
    "3. **Form trend** - Improving vs declining (last 5 vs previous 5 matches)\n",
    "4. **Rest days advantage** - Difference in recovery time\n",
    "5. **Season progress** - Early/mid/late season effects (0.0 to 1.0)\n",
    "\n",
    "**Total: 7 derived features** (reduced from 8 by using only percentile difference)\n",
    "\n",
    "**Better Feature Testing (Section 4.3.X):**\n",
    "- ✅ **Correlation Analysis** - Fast initial screening, shows linear relationships only\n",
    "- ✅ **Correlation Visualization** - Bar charts showing top correlated features by group\n",
    "\n",
    "**Key Insight:** Weak correlations suggest non-linear patterns dominate in soccer:\n",
    "- Goal-scoring has complex dynamics (e.g., very weak teams score less, but so do very defensive teams)\n",
    "- Features likely interact (promoted teams + low position ≠ stable low position)\n",
    "- Thresholds and context matter (rest days, form trends, league tier effects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of Numerical Features**\n",
    "\n",
    "All numerical features show **very weak correlations** (|r| < 0.08) with the Over/Under 2.5 goals target, suggesting minimal linear dependence.  \n",
    "**Feature-wise summary:**\n",
    "- **Days since last match** (-0.010, -0.009):  \n",
    "  Essentially no relationship — rest periods have no meaningful effect on goal totals.\n",
    "- **Home goals MA5** (0.074):  \n",
    "  Slight positive signal — teams scoring more in recent matches tend to have marginally higher Over 2.5 rates.\n",
    "- **Away goals MA5** (0.048):  \n",
    "  Small negative correlation — possibly due to opponents adapting defensively to strong away attacks.\n",
    "- **League tier** (-0.042):  \n",
    "  Strongest (yet still weak) correlation — lower divisions show a slightly lower frequency of high-scoring games.\n",
    "- **Month** (0.019):  \n",
    "  Negligible seasonal influence on goal totals.\n",
    "**Modeling note:**  \n",
    "\n",
    "Although weak on their own, these variables may still provide value to **non-linear or ensemble models** by capturing interaction effects and subtle contextual patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test boolean/categorical features against the target.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean features - automatically include all that exist in df_basic\n",
    "# Define expected boolean features (add new ones here as you create them)\n",
    "expected_bool_features = ['is_weekend', 'home_promoted', 'home_demoted',\n",
    "                          'away_promoted', 'away_demoted']\n",
    "bool_features = [feat for feat in expected_bool_features if feat in df_basic.columns]\n",
    "\n",
    "print(f\"Analyzing {len(bool_features)} boolean/categorical features:\")\n",
    "print(f\"  Features: {bool_features}\\n\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Over 2.5 rate by categorical feature:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for feat in bool_features:\n",
    "    grouped = df_basic.groupby(feat)['over_2_5'].agg(['mean', 'count'])\n",
    "    print(f\"\\n{feat}:\")\n",
    "    print(grouped)\n",
    "\n",
    "    # Chi-squared test\n",
    "    contingency = pd.crosstab(df_basic[feat], df_basic['over_2_5'])\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "    print(f\"Chi-squared p-value: {p:.4f}\")\n",
    "\n",
    "# Visualize - dynamically create subplots based on number of features\n",
    "n_features = min(len(bool_features), 5)  # Limit to 5 for readability\n",
    "fig, axes = plt.subplots(1, n_features, figsize=(5 * n_features, 4))\n",
    "if n_features == 1:\n",
    "    axes = [axes]  # Make it iterable\n",
    "\n",
    "# Configuration for feature visualization\n",
    "feature_configs = {\n",
    "    'is_weekend': {'labels': ['Weekday', 'Weekend'], 'title': 'Over 2.5 Rate: Weekend vs Weekday'},\n",
    "    'home_promoted': {'labels': ['Regular', 'Promoted'], 'title': 'Over 2.5 Rate: Home Team Promoted'},\n",
    "    'home_demoted': {'labels': ['Regular', 'Demoted'], 'title': 'Over 2.5 Rate: Home Team Demoted'},\n",
    "    'away_promoted': {'labels': ['Regular', 'Promoted'], 'title': 'Over 2.5 Rate: Away Team Promoted'},\n",
    "    'away_demoted': {'labels': ['Regular', 'Demoted'], 'title': 'Over 2.5 Rate: Away Team Demoted'}\n",
    "}\n",
    "\n",
    "for idx, feat in enumerate(bool_features[:n_features]):\n",
    "    rates = df_basic.groupby(feat)['over_2_5'].mean()\n",
    "    config = feature_configs.get(feat, {'labels': ['False', 'True'], 'title': f'Over 2.5 Rate: {feat}'})\n",
    "\n",
    "    axes[idx].bar(config['labels'], rates.values, color=['steelblue', 'coral'])\n",
    "    axes[idx].set_title(config['title'], fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Over 2.5 Rate')\n",
    "    axes[idx].set_ylim([0.45, 0.55])  # Zoom in on the relevant range\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(rates.values):\n",
    "        axes[idx].text(i, v + 0.002, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of Boolean/Categorical Features:**\n",
    "\n",
    "Chi-squared tests reveal the following significance levels for `{len(bool_features)}` boolean features analyzed:\n",
    "\n",
    "**Statistically Significant (α=0.05):**\n",
    "- **Weekend vs Weekday** (p=0.0012): Weekend matches show notably higher Over 2.5 rate (50.5% vs 48.7% weekday). This is the strongest categorical predictor, suggesting weekend scheduling may influence match dynamics - possibly due to fan attendance, player rest, or tactical approaches.\n",
    "\n",
    "- **Away Team Demoted** (p=0.0121): Away teams that were demoted show lower Over 2.5 rate (47.6% vs 50.1% regular teams). The 2.5 percentage point difference suggests defensive strategies or morale issues affecting demoted teams playing away.\n",
    "\n",
    "- **Home Team Demoted** (p=0.0277): Home teams that were demoted show lower Over 2.5 rate (47.8% vs 50.1% regular teams). The effect is similar to away demoted, indicating demotion status has defensive implications regardless of venue.\n",
    "\n",
    "**Borderline/Not Significant:**\n",
    "- **Away Team Promoted** (p=0.0712): Shows higher Over 2.5 rate (51.8% vs 49.9%) but just misses significance at α=0.05. May reflect attacking ambition of newly promoted teams.\n",
    "\n",
    "- **Home Team Promoted** (p=0.4339): No meaningful difference (50.8% vs 49.9%). Home promoted teams do not show distinct scoring patterns.\n",
    "\n",
    "**Conclusion:**  \n",
    "- **Weekend effect** is the most robust categorical predictor with clear practical significance.\n",
    "- **Demotion features** (both home and away) show consistent negative effects on Over 2.5 rate, suggesting these teams adopt more conservative tactics.\n",
    "- **Promotion features** show weaker/inconsistent effects - away promoted is borderline while home promoted shows no effect.\n",
    "- All `{len(bool_features)}` features detected and analyzed: `{bool_features}`\n",
    "- Individual effects remain modest (all within ±3 percentage points of baseline 50%), but may prove valuable in ensemble models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Extended feature df engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create extended dataframe with all available match data including detailed statistics and betting odds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTENDED DATASET: All available match data including detailed statistics\n",
    "extended_core_features = [col for col in [\n",
    "    # Core match info\n",
    "    'Div', 'Season', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG', 'HTR',\n",
    "    # Match statistics\n",
    "    'Attendance', 'Referee', 'HS', 'AS', 'HST', 'AST', 'HHW', 'AHW', 'HC', 'AC',\n",
    "    'HF', 'AF', 'HFKC', 'AFKC', 'HO', 'AO', 'HY', 'AY', 'HR', 'AR', 'HBP', 'ABP'\n",
    "] if col in all_matches.columns]\n",
    "\n",
    "# All engineered features (using rich match statistics)\n",
    "# Note: Additional features (time-based + historical) will be merged from df_basic after creation\n",
    "extended_engineered_features = [col for col in [\n",
    "    'total_goals', 'ht_total_goals', 'second_half_goals',  # Goal-based\n",
    "    'home_shot_accuracy', 'away_shot_accuracy', 'total_shots', 'total_shots_on_target',  # Shot-based\n",
    "    'shot_dominance', 'corner_dominance', 'total_fouls', 'total_cards', 'card_intensity',  # Game dynamics\n",
    "    'league_tier', 'month', 'is_weekend',  # Date/league features\n",
    "    'over_2_5'  # Target variable\n",
    "] if col in all_matches.columns]\n",
    "\n",
    "# Extended features (betting odds - only high-quality columns after imputation)\n",
    "betting_features = []\n",
    "for col in all_matches.columns:\n",
    "    # Check if it's a betting column and has good data coverage (>10%)\n",
    "    if any(bookmaker in col for bookmaker in ['B365', 'BW', 'PS', 'IW', 'LB', 'WH', 'SJ', 'VC', 'BF', '1XB']) and col not in categorical_features:\n",
    "        data_coverage = (all_matches[col].notna().sum() / len(all_matches)) * 100\n",
    "        if data_coverage >= 10:  # Only include columns with at least 10% data coverage\n",
    "            betting_features.append(col)\n",
    "\n",
    "# Create extended dataframe\n",
    "all_extended_features = extended_core_features + extended_engineered_features + betting_features\n",
    "# Remove duplicates while preserving order\n",
    "all_extended_features = list(dict.fromkeys(all_extended_features))\n",
    "\n",
    "df_extended = all_matches[all_extended_features].copy()\n",
    "df_extended = df_extended.sort_values(['Div', 'Date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Extended dataframe created\")\n",
    "print(f\"Shape: {df_extended.shape}\")\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"  Core features: {len(extended_core_features)}\")\n",
    "print(f\"  Engineered features: {len(extended_engineered_features)}\")\n",
    "print(f\"  Betting features (>10% coverage): {len(betting_features)}\")\n",
    "print(f\"  Total features: {len(all_extended_features)}\")\n",
    "print(f\"\\nColumns: {df_extended.columns.tolist()[:20]}...\")  # Show first 20\n",
    "df_extended.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Merging engineered features from df_basic\n",
    "\n",
    "Add all engineered features from df_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_based_features = [\n",
    "    'home_days_since_last', 'away_days_since_last',\n",
    "    'home_goals_ma5', 'away_goals_ma5', 'home_conceded_ma5', 'away_conceded_ma5',\n",
    "    'home_promoted', 'away_promoted', 'home_demoted', 'away_demoted'\n",
    "]\n",
    "\n",
    "# Historical position features (overall only)\n",
    "historical_position_cols = [col for col in df_basic.columns if 'position_pct_' in col]\n",
    "\n",
    "# Historical goal pattern features (overall + home-specific + away-specific, counts + percentages)\n",
    "historical_goal_cols = [col for col in df_basic.columns if any(\n",
    "    f'{p}_{t}plus' in col for p in ['scored', 'conceded', 'total'] for t in [2, 3]\n",
    ")]\n",
    "\n",
    "# Derived features combining historical patterns\n",
    "derived_features = [col for col in df_basic.columns if any(\n",
    "    pattern in col for pattern in ['_strength_', '_combined_']\n",
    ")]\n",
    "\n",
    "features_to_merge = time_based_features + historical_position_cols + historical_goal_cols + derived_features\n",
    "\n",
    "print(f\"Features to merge from df_basic: {len(features_to_merge)}\")\n",
    "print(f\"  Time-based features: {len(time_based_features)}\")\n",
    "print(f\"  Position percentiles: {len(historical_position_cols)}\")\n",
    "print(f\"  Goal patterns (overall + home/away, counts + pct): {len(historical_goal_cols)}\")\n",
    "print(f\"  Derived features: {len(derived_features)}\")\n",
    "\n",
    "df_basic_sorted = df_basic.sort_values(['Div', 'Date']).reset_index(drop=True)\n",
    "df_extended_sorted = df_extended.sort_values(['Div', 'Date']).reset_index(drop=True)\n",
    "\n",
    "match_check = (\n",
    "    (df_basic_sorted['Div'] == df_extended_sorted['Div']) &\n",
    "    (df_basic_sorted['Date'] == df_extended_sorted['Date']) &\n",
    "    (df_basic_sorted['HomeTeam'] == df_extended_sorted['HomeTeam']) &\n",
    "    (df_basic_sorted['AwayTeam'] == df_extended_sorted['AwayTeam'])\n",
    ").all()\n",
    "\n",
    "if match_check:\n",
    "    print(f\"\\nRow alignment verified - safe to merge features\")\n",
    "\n",
    "    for col in features_to_merge:\n",
    "        df_extended[col] = df_basic_sorted[col].values\n",
    "\n",
    "    print(f\"\\n✓ {len(features_to_merge)} features added to df_extended\")\n",
    "    print(f\"  - {len(time_based_features)} time-based features\")\n",
    "    print(f\"  - {len(historical_position_cols)} position percentiles\")\n",
    "    print(f\"  - {len(historical_goal_cols)} goal pattern features\")\n",
    "    print(f\"  - {len(derived_features)} derived features\")\n",
    "    print(f\"\\nFinal df_extended shape: {df_extended.shape}\")\n",
    "else:\n",
    "    print(\"⚠ Row mismatch detected - cannot safely merge features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2. More features for df_extended\n",
    "\n",
    "**Extended Statistics - Moving Averages & Seasonal Patterns**\n",
    "\n",
    "Calculate MA5 and historical patterns for match statistics (shots, corners, fouls, cards, etc.) following the same approach as goal-based features.\n",
    "\n",
    "✅ **Data Leakage Prevention:**\n",
    "- MA5 calculations use `Date < date` to exclude current match\n",
    "- NaN values preserved for early-season matches (no fillna with overall means)\n",
    "- Same fix applied in Section 4.3.2 (basic df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate moving averages and seasonal history for extended match statistics.\n",
    "Following the same pattern as goal-based features in df_basic.\n",
    "\n",
    "MA5 Features: High + Medium Priority (Shots, Shots on Target, Corners, Fouls, Yellow Cards)\n",
    "Seasonal Patterns: High Priority Only (Shots, Shots on Target, Corners)\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SECTION 4.4.2: EXTENDED STATS MOVING AVERAGES & SEASONAL PATTERNS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: 5-Match Moving Averages (High + Medium Priority)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n1. CALCULATING 5-MATCH MOVING AVERAGES\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Define statistics to calculate MA5 for - High and Medium priority\n",
    "ma5_stats_config = {\n",
    "    # High Priority - Shooting\n",
    "    ('HS', 'shots'): 'home',\n",
    "    ('AS', 'shots'): 'away',\n",
    "    ('HST', 'shots_target'): 'home',\n",
    "    ('AST', 'shots_target'): 'away',\n",
    "    # High Priority - Attacking Pressure  \n",
    "    ('HC', 'corners'): 'home',\n",
    "    ('AC', 'corners'): 'away',\n",
    "    # Medium Priority - Discipline\n",
    "    ('HF', 'fouls'): 'home',\n",
    "    ('AF', 'fouls'): 'away',\n",
    "    ('HY', 'yellows'): 'home',\n",
    "    ('AY', 'yellows'): 'away',\n",
    "}\n",
    "\n",
    "# Initialize MA5 columns\n",
    "print(f\"Initializing {len(ma5_stats_config)} MA5 columns...\")\n",
    "for (raw_col, stat_name), team_type in ma5_stats_config.items():\n",
    "    ma5_col = f'{team_type}_{stat_name}_ma5'\n",
    "    df_extended[ma5_col] = np.nan\n",
    "\n",
    "# Calculate moving averages - optimized by pre-sorting\n",
    "print(\"Calculating moving averages (optimized groupby approach)...\")\n",
    "df_extended_sorted = df_extended.sort_values(['Div', 'Date']).reset_index(drop=True)\n",
    "\n",
    "for (raw_col, stat_name), team_type in ma5_stats_config.items():\n",
    "    if raw_col not in df_extended.columns:\n",
    "        continue\n",
    "    \n",
    "    ma5_col = f'{team_type}_{stat_name}_ma5'\n",
    "    team_col = 'HomeTeam' if team_type == 'home' else 'AwayTeam'\n",
    "    \n",
    "    print(f\"  Processing {ma5_col}...\")\n",
    "    \n",
    "    # Group by division and team, then calculate MA5\n",
    "    for (div, team), group in df_extended_sorted.groupby(['Div', team_col]):\n",
    "        indices = group.index.tolist()\n",
    "        values = group[raw_col].values\n",
    "        \n",
    "        # Calculate MA5 for each match in this group\n",
    "        for i, idx in enumerate(indices):\n",
    "            if i >= 5:  # Need at least 5 previous matches\n",
    "                ma5_value = np.nanmean(values[i-5:i])  # Exclude current match\n",
    "                if not np.isnan(ma5_value):\n",
    "                    df_extended.loc[idx, ma5_col] = ma5_value\n",
    "\n",
    "# NO DATA LEAKAGE: NaN values preserved for early-season matches\n",
    "print(\"\\nMA5 Summary:\")\n",
    "for (raw_col, stat_name), team_type in ma5_stats_config.items():\n",
    "    ma5_col = f'{team_type}_{stat_name}_ma5'\n",
    "    if ma5_col in df_extended.columns:\n",
    "        missing = df_extended[ma5_col].isna().sum()\n",
    "        missing_pct = (missing / len(df_extended)) * 100\n",
    "        mean_val = df_extended[ma5_col].mean()\n",
    "        print(f\"  {ma5_col:25s}: missing={missing:5d} ({missing_pct:4.1f}%), mean={mean_val:6.2f}\")\n",
    "\n",
    "print(f\"\\n✓ Added {len(ma5_stats_config)} moving average features\")\n",
    "print(\"  (NaN values preserved for early-season matches - no data leakage)\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: Seasonal Historical Patterns (High Priority Only)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n2. CALCULATING SEASONAL HISTORICAL PATTERNS\")\n",
    "print(\"-\" * 70)\n",
    "print(\"High priority features only: Shots, Shots on Target, Corners\")\n",
    "\n",
    "# Define thresholds for HIGH PRIORITY stats only\n",
    "stat_thresholds = {\n",
    "    'shots': [10, 15],           # 10+ shots, 15+ shots\n",
    "    'shots_target': [5, 8],      # 5+ on target, 8+ on target\n",
    "    'corners': [6, 10],          # 6+ corners, 10+ corners\n",
    "}\n",
    "\n",
    "# Season info\n",
    "unique_seasons = sorted(df_extended['Season'].unique())\n",
    "season_to_order = {s: i for i, s in enumerate(unique_seasons)}\n",
    "past_seasons = ['2019/2020', '2020/2021', '2021/2022', '2022/2023', '2023/2024', '2024/2025']\n",
    "season_years = [2019, 2020, 2021, 2022, 2023, 2024]\n",
    "\n",
    "# Map stat names to columns\n",
    "stat_to_home_col = {'shots': 'HS', 'shots_target': 'HST', 'corners': 'HC'}\n",
    "stat_to_away_col = {'shots': 'AS', 'shots_target': 'AST', 'corners': 'AC'}\n",
    "\n",
    "# Pre-calculate seasonal statistics for all teams (OPTIMIZATION)\n",
    "print(\"Pre-calculating team-season statistics...\")\n",
    "team_season_stats = {}\n",
    "\n",
    "for season in past_seasons:\n",
    "    for div in df_extended['Div'].unique():\n",
    "        season_div_mask = (df_extended['Season'] == season) & (df_extended['Div'] == div)\n",
    "        season_div_data = df_extended[season_div_mask]\n",
    "        \n",
    "        if len(season_div_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get all teams in this season/division\n",
    "        all_teams = set(season_div_data['HomeTeam'].unique()) | set(season_div_data['AwayTeam'].unique())\n",
    "        \n",
    "        for team in all_teams:\n",
    "            # Overall stats (all matches)\n",
    "            overall_mask = (season_div_data['HomeTeam'] == team) | (season_div_data['AwayTeam'] == team)\n",
    "            overall_matches = season_div_data[overall_mask]\n",
    "            \n",
    "            # Home-only stats\n",
    "            home_mask = season_div_data['HomeTeam'] == team\n",
    "            home_matches = season_div_data[home_mask]\n",
    "            \n",
    "            # Away-only stats\n",
    "            away_mask = season_div_data['AwayTeam'] == team\n",
    "            away_matches = season_div_data[away_mask]\n",
    "            \n",
    "            # Calculate stats for each threshold\n",
    "            stats = {}\n",
    "            for stat_name, thresholds in stat_thresholds.items():\n",
    "                h_col = stat_to_home_col[stat_name]\n",
    "                a_col = stat_to_away_col[stat_name]\n",
    "                \n",
    "                for threshold in thresholds:\n",
    "                    # Overall stats\n",
    "                    count = 0\n",
    "                    for _, match in overall_matches.iterrows():\n",
    "                        col = h_col if match['HomeTeam'] == team else a_col\n",
    "                        if not pd.isna(match.get(col)) and match.get(col, 0) >= threshold:\n",
    "                            count += 1\n",
    "                    total = len(overall_matches)\n",
    "                    stats[f'{stat_name}_{threshold}plus_count'] = count\n",
    "                    stats[f'{stat_name}_{threshold}plus_pct'] = count / total if total > 0 else 0\n",
    "                    \n",
    "                    # Home-only stats\n",
    "                    if h_col in home_matches.columns:\n",
    "                        home_count = (home_matches[h_col] >= threshold).sum()\n",
    "                        home_total = len(home_matches)\n",
    "                        stats[f'home_{stat_name}_{threshold}plus_count'] = home_count\n",
    "                        stats[f'home_{stat_name}_{threshold}plus_pct'] = home_count / home_total if home_total > 0 else 0\n",
    "                    \n",
    "                    # Away-only stats\n",
    "                    if a_col in away_matches.columns:\n",
    "                        away_count = (away_matches[a_col] >= threshold).sum()\n",
    "                        away_total = len(away_matches)\n",
    "                        stats[f'away_{stat_name}_{threshold}plus_count'] = away_count\n",
    "                        stats[f'away_{stat_name}_{threshold}plus_pct'] = away_count / away_total if away_total > 0 else 0\n",
    "            \n",
    "            team_season_stats[(season, div, team)] = stats\n",
    "\n",
    "print(f\"  Pre-calculated stats for {len(team_season_stats)} team-season combinations\")\n",
    "\n",
    "# Initialize seasonal pattern columns\n",
    "print(\"Initializing seasonal pattern columns...\")\n",
    "total_columns = 0\n",
    "for stat_name in stat_thresholds.keys():\n",
    "    for threshold in stat_thresholds[stat_name]:\n",
    "        for year in season_years:\n",
    "            # Overall patterns\n",
    "            for prefix in ['home', 'away']:\n",
    "                df_extended[f'{prefix}_{stat_name}_{threshold}plus_count_{year}'] = 0\n",
    "                df_extended[f'{prefix}_{stat_name}_{threshold}plus_pct_{year}'] = np.nan\n",
    "                total_columns += 2\n",
    "            # Context-aware patterns\n",
    "            df_extended[f'home_home_{stat_name}_{threshold}plus_count_{year}'] = 0\n",
    "            df_extended[f'home_home_{stat_name}_{threshold}plus_pct_{year}'] = np.nan\n",
    "            df_extended[f'away_away_{stat_name}_{threshold}plus_count_{year}'] = 0\n",
    "            df_extended[f'away_away_{stat_name}_{threshold}plus_pct_{year}'] = np.nan\n",
    "            total_columns += 4\n",
    "\n",
    "print(f\"Created {total_columns} seasonal pattern columns\")\n",
    "\n",
    "# Populate features using pre-calculated statistics (FAST!)\n",
    "print(\"\\nPopulating seasonal pattern features (using lookups)...\")\n",
    "for idx, row in df_extended.iterrows():\n",
    "    current_season = row['Season']\n",
    "    current_season_order = season_to_order.get(current_season, 999)\n",
    "    \n",
    "    for year_idx, year in enumerate(season_years):\n",
    "        if year_idx >= current_season_order:\n",
    "            continue\n",
    "        \n",
    "        past_season = past_seasons[year_idx]\n",
    "        \n",
    "        # Lookup home team stats\n",
    "        home_key = (past_season, row['Div'], row['HomeTeam'])\n",
    "        if home_key in team_season_stats:\n",
    "            home_stats = team_season_stats[home_key]\n",
    "            for stat_name in stat_thresholds.keys():\n",
    "                for threshold in stat_thresholds[stat_name]:\n",
    "                    # Overall\n",
    "                    df_extended.loc[idx, f'home_{stat_name}_{threshold}plus_count_{year}'] = home_stats.get(f'{stat_name}_{threshold}plus_count', 0)\n",
    "                    df_extended.loc[idx, f'home_{stat_name}_{threshold}plus_pct_{year}'] = home_stats.get(f'{stat_name}_{threshold}plus_pct', np.nan)\n",
    "                    # Home-specific\n",
    "                    df_extended.loc[idx, f'home_home_{stat_name}_{threshold}plus_count_{year}'] = home_stats.get(f'home_{stat_name}_{threshold}plus_count', 0)\n",
    "                    df_extended.loc[idx, f'home_home_{stat_name}_{threshold}plus_pct_{year}'] = home_stats.get(f'home_{stat_name}_{threshold}plus_pct', np.nan)\n",
    "        \n",
    "        # Lookup away team stats\n",
    "        away_key = (past_season, row['Div'], row['AwayTeam'])\n",
    "        if away_key in team_season_stats:\n",
    "            away_stats = team_season_stats[away_key]\n",
    "            for stat_name in stat_thresholds.keys():\n",
    "                for threshold in stat_thresholds[stat_name]:\n",
    "                    # Overall\n",
    "                    df_extended.loc[idx, f'away_{stat_name}_{threshold}plus_count_{year}'] = away_stats.get(f'{stat_name}_{threshold}plus_count', 0)\n",
    "                    df_extended.loc[idx, f'away_{stat_name}_{threshold}plus_pct_{year}'] = away_stats.get(f'{stat_name}_{threshold}plus_pct', np.nan)\n",
    "                    # Away-specific\n",
    "                    df_extended.loc[idx, f'away_away_{stat_name}_{threshold}plus_count_{year}'] = away_stats.get(f'away_{stat_name}_{threshold}plus_count', 0)\n",
    "                    df_extended.loc[idx, f'away_away_{stat_name}_{threshold}plus_pct_{year}'] = away_stats.get(f'away_{stat_name}_{threshold}plus_pct', np.nan)\n",
    "    \n",
    "    # Progress indicator\n",
    "    if (idx + 1) % 10000 == 0:\n",
    "        print(f\"  Processed {idx + 1}/{len(df_extended)} matches...\")\n",
    "\n",
    "print(f\"\\n✓ Populated {total_columns} seasonal pattern features\")\n",
    "\n",
    "# ============================================================\n",
    "# Summary\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY - SECTION 4.4.2 FEATURE ENGINEERING\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Moving Average Features (MA5): {len(ma5_stats_config)}\")\n",
    "print(f\"  - High Priority: Shots, Shots on Target, Corners\")\n",
    "print(f\"  - Medium Priority: Fouls, Yellow Cards\")\n",
    "print(f\"\\nSeasonal Pattern Features: {total_columns}\")\n",
    "print(f\"  - High Priority Only: Shots, Shots on Target, Corners\")\n",
    "print(f\"  - Overall + Home/Away context for each stat\")\n",
    "print(f\"  - {len(stat_thresholds)} stat types × 2 thresholds × 6 seasons\")\n",
    "print(f\"\\nTotal New Features: {len(ma5_stats_config) + total_columns}\")\n",
    "print(f\"Final df_extended shape: {df_extended.shape}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3. Betting odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Final feature dataframe\n",
    "\n",
    "Combine all features into the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of engineered features\n",
    "print(\"Final engineered dataframe:\")\n",
    "print(f\"Shape: {df_basic.shape}\")\n",
    "print(f\"\\nColumns:\")\n",
    "print(df_basic.columns.tolist())\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df_basic.isnull().sum().sum())\n",
    "print(f\"\\nSample:\")\n",
    "df_basic[['Date', 'HomeTeam', 'AwayTeam', 'total_goals', 'over_2_5',\n",
    "    'home_days_since_last', 'home_goals_ma5', 'home_promoted', 'is_weekend']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Added Features**\n",
    "\n",
    "**Time-based metrics:**  \n",
    "- `home_days_since_last`, `away_days_since_last`: Days since each team’s previous match (average ≈ 9.5 days)\n",
    "\n",
    "**Recent performance (5-match moving averages):**  \n",
    "- `home_goals_ma5`, `away_goals_ma5`: Average goals scored in the last 5 matches  \n",
    "- `home_conceded_ma5`, `away_conceded_ma5`: Average goals conceded in the last 5 matches  \n",
    "\n",
    "**League transitions:**  \n",
    "- `home_promoted`, `away_promoted`, `home_demoted`, `away_demoted`: Indicators of team movement between divisions  \n",
    "\n",
    "#### **Main Observations** \n",
    "   - All features show very weak correlations (|r| < 0.02) with the Over/Under 2.5 goals target.  \n",
    "\n",
    "   - Chi-square tests are insignificant (p > 0.05), suggesting no strong individual relationships. \n",
    "\n",
    "   - League level shows the strongest (though still small) correlation at −0.012, indicating that lower leagues may have slightly fewer high-scoring games.\n",
    "\n",
    "   - The home team’s recent attacking form has a weak positive correlation (0.010), but it’s negligible in isolation.\n",
    "\n",
    "   - Newly promoted or relegated teams do not exhibit consistent differences in total goals per match.\n",
    "\n",
    "   - Rest days and weekend scheduling have no measurable effect on goal totals.\n",
    "\n",
    "#### **Implications for Modeling**\n",
    "\n",
    "While each feature provides limited predictive power on its own, they may still add value when used together in non-linear models such as tree-based or ensemble methods.  \n",
    "\n",
    "Weak linear relationships are expected, as the Over/Under 2.5 target is a roughly balanced binary outcome, making individual predictors inherently limited in isolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Categorical encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Dataset preparation and train-test split\n",
    "\n",
    "For time series data like sports matches, we need to be careful about temporal splitting to avoid data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Temporal train-test split\n",
    "\n",
    "Since this is time series data, we'll split chronologically to simulate real-world predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by date for temporal split\n",
    "all_matches_sorted = all_matches.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "# Use 80% for training (chronologically earlier), 20% for testing (more recent)\n",
    "split_date = all_matches_sorted['Date'].quantile(0.8)\n",
    "print(f\"Split date: {split_date}\")\n",
    "\n",
    "train_mask = all_matches_sorted['Date'] <= split_date\n",
    "test_mask = all_matches_sorted['Date'] > split_date\n",
    "\n",
    "train_data = all_matches_sorted[train_mask].copy()\n",
    "test_data = all_matches_sorted[test_mask].copy()\n",
    "\n",
    "print(f\"Training set: {len(train_data)} matches ({train_data['Date'].min()} to {train_data['Date'].max()})\")\n",
    "print(f\"Test set: {len(test_data)} matches ({test_data['Date'].min()} to {test_data['Date'].max()})\")\n",
    "print(f\"Train Over 2.5 rate: {train_data['over_2_5'].mean():.2%}\")\n",
    "print(f\"Test Over 2.5 rate: {test_data['over_2_5'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Basic vs Extended datasets\n",
    "\n",
    "Create two datasets as mentioned in the project goals:\n",
    "- **Basic dataset**: Core match statistics only\n",
    "- **Extended dataset**: Including betting odds and additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets for modeling\n",
    "basic_features = basic_core_features + basic_engineered_features\n",
    "extended_features = extended_core_features + extended_engineered_features + betting_features\n",
    "\n",
    "# Exclude columns that shouldn't be used directly in modeling\n",
    "excluded_from_modeling = ['Date', 'Time']  # These are used for feature engineering but not direct modeling\n",
    "\n",
    "# We'll handle categorical encoding in the modeling phase\n",
    "target = 'over_2_5'\n",
    "\n",
    "print(f\"Basic model features: {len(basic_features)}\")\n",
    "print(f\"Extended model features: {len(extended_features)}\")\n",
    "\n",
    "# Create datasets (without categorical encoding for now)\n",
    "def create_dataset(data, features, target_col):\n",
    "    \"\"\"Create feature matrix and target vector\"\"\"\n",
    "    # Only include features that exist in the data and exclude date/time columns for modeling\n",
    "    available_features = [f for f in features if f in data.columns and f not in excluded_from_modeling]\n",
    "\n",
    "    X = data[available_features].copy()\n",
    "    y = data[target_col].copy()\n",
    "\n",
    "    return X, y, available_features\n",
    "\n",
    "# Basic datasets (core columns + basic engineered features only)\n",
    "X_train_basic, y_train, basic_features_final = create_dataset(train_data, basic_features, target)\n",
    "X_test_basic, y_test, _ = create_dataset(test_data, basic_features, target)\n",
    "\n",
    "# Extended datasets (all match stats + betting odds + all engineered features)\n",
    "X_train_extended, _, extended_features_final = create_dataset(train_data, extended_features, target)\n",
    "X_test_extended, _, _ = create_dataset(test_data, extended_features, target)\n",
    "\n",
    "print(f\"\\nFinal feature counts:\")\n",
    "print(f\"Basic features available: {len(basic_features_final)}\")\n",
    "print(f\"Extended features available: {len(extended_features_final)}\")\n",
    "\n",
    "print(f\"\\nBasic features: {basic_features_final}\")\n",
    "print(f\"\\nExtended features sample (first 20): {extended_features_final[:20]}\")\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"X_train_basic: {X_train_basic.shape}\")\n",
    "print(f\"X_test_basic: {X_test_basic.shape}\")\n",
    "print(f\"X_train_extended: {X_train_extended.shape}\")\n",
    "print(f\"X_test_extended: {X_test_extended.shape}\")\n",
    "\n",
    "# Check for missing values in final datasets\n",
    "print(f\"\\nMissing values in basic features:\")\n",
    "print(X_train_basic.isnull().sum().sum())\n",
    "print(f\"Missing values in extended features:\")\n",
    "print(X_train_extended.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Save processed datasets\n",
    "\n",
    "Save the preprocessed data for use in modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed datasets\n",
    "datasets = {\n",
    "    'X_train_basic': X_train_basic_imputed,\n",
    "    'X_test_basic': X_test_basic_imputed,\n",
    "    'X_train_extended': X_train_extended_imputed,\n",
    "    'X_test_extended': X_test_extended_imputed,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test\n",
    "}\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    filepath = f\"{OUTPUT_DIR}/{name}.pkl\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "# Also save feature names\n",
    "feature_info = {\n",
    "    'basic_features': basic_features_final,\n",
    "    'extended_features': extended_features_final,\n",
    "    'target': target\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/feature_info.pkl\", 'wb') as f:\n",
    "    pickle.dump(feature_info, f)\n",
    "\n",
    "# Save also as CSV for easy inspection\n",
    "# X_train_basic_imputed.to_csv(f\"{OUTPUT_DIR}/X_train_basic.csv\", index=False)\n",
    "# X_train_extended_imputed.to_csv(f\"{OUTPUT_DIR}/X_train_extended.csv\", index=False)\n",
    "# y_train.to_csv(f\"{OUTPUT_DIR}/y_train.csv\", index=False)\n",
    "# y_test.to_csv(f\"{OUTPUT_DIR}/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Preprocessing Summary\n",
    "\n",
    "## What was accomplished:\n",
    "\n",
    "### Data Loading & Cleaning:\n",
    "- Loaded 42,593 matches from 11 countries and 21 leagues\n",
    "- Handled unnamed columns (100% missing data)\n",
    "- Normalized league codes (E0→E1, SC0→SC1, etc.)\n",
    "- Corrected English/Scottish yellow card counts\n",
    "- Converted data types properly (datetime, categorical, numerical)\n",
    "- Imputed missing values in key match statistics (<0.1% missing)\n",
    "\n",
    "### Exploratory Data Analysis:\n",
    "- Target variable (Over/Under 2.5 goals) perfectly balanced: 49.99% / 50.01%\n",
    "- Analyzed missing data patterns (betting odds 80%+ missing, match stats <0.1%)\n",
    "- Examined country/league distributions\n",
    "- Outlier analysis using z-scores (kept outliers as legitimate in football)\n",
    "\n",
    "### Feature Engineering:\n",
    "- Created target variable: `over_2_5` (Over/Under 2.5 goals)\n",
    "- Shot efficiency features: accuracy, total shots, dominance measures\n",
    "- Game intensity features: fouls, cards, card intensity\n",
    "- Temporal features: half-time patterns, second-half goals\n",
    "- League and seasonal features: tier, month, weekend indicator\n",
    "\n",
    "### Dataset Preparation:\n",
    "- **Temporal train-test split**: 80% train (2019-2024) / 20% test (2024-2025)\n",
    "- **Basic dataset**: ~7 features (ONLY original core columns: FTHG, FTAG, HTHG, HTAG + goal-based engineered features)\n",
    "- **Extended dataset**: ~40+ features (all match statistics + betting odds + all engineered features)\n",
    "- Missing value imputation for both datasets\n",
    "- Saved processed data for modeling\n",
    "\n",
    "## Data Quality:\n",
    "- **Training set**: 34,085 matches (49.56% Over 2.5)\n",
    "- **Test set**: 8,508 matches (51.72% Over 2.5)\n",
    "- **No missing values** in final processed datasets\n",
    "- **Temporally sorted** to prevent data leakage\n",
    "\n",
    "## Key Distinction:\n",
    "- **Basic Model**: Uses only original core data (goals) + simple derived features\n",
    "- **Extended Model**: Uses rich match statistics (shots, fouls, cards) + betting odds + complex features"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NBksARdgdMkP",
    "Qx3gpH8TdToi",
    "O5SNQWIMhxZA",
    "NNyVnO2Bk4zo",
    "XMiRfCrVzDVd",
    "fwwBVM5ex2SA"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
