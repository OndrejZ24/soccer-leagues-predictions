{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML2 Semestral Project - Football O/U 2.5\n",
    "**Authors:** Phuong Nhi Tranová, Vít Maruniak, Šimon Slánský, Radim Škoukal, Ondřej Zetek, Martin Kareš, Jan Korčák, Jakub Maličkay, Jáchym Janouch  \n",
    "**Course:** FIS 4IT344 Machine Learning 2 (2025/2026)  \n",
    "**Goal:** Compare baseline (current features) vs extended (richer features) models for O/U 2.5 goals across markets; translate accuracy gains into optimal profit and **maximum data subscription price per country** *.  \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "***maximum data subscription price per country**\n",
    "- the most money our company should be willing to pay for that country's additional data\n",
    "- that's how much extra profit the improved model generates\n",
    "- baseline model → accuracy = A₀\n",
    "    - Generates profit Π*(A₀)\n",
    "- extended model → accuracy = A₁\n",
    "    - Generates profit Π*(A₁)\n",
    "- profit improvement = ΔΠ = Π(A₁) − Π(A₀)*\n",
    "    - basically how much more money the comany earns each year by using the better data\n",
    "- the maximum data subscription price per country = ΔΠ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports and paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore, chi2_contingency\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Library parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "OUTPUT_DIR = f\"./processed\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_matches(data_dir: str) -> pd.DataFrame:\n",
    "    csv_files = glob.glob(os.path.join(data_dir, \"**\", \"*.csv\"), recursive=True)\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSV files found under {data_dir}\")\n",
    "\n",
    "    frames = []\n",
    "    for fp in csv_files:\n",
    "        # extract path info\n",
    "        rel = os.path.relpath(fp, data_dir)\n",
    "        parts = Path(rel).parts\n",
    "        country = parts[0] if len(parts) >= 1 else None\n",
    "        league  = parts[1] if len(parts) >= 2 else None\n",
    "        season_file = parts[2] if len(parts) >= 3 else None\n",
    "        season_code = os.path.splitext(season_file)[0] if season_file else None\n",
    "\n",
    "        # read and rename\n",
    "        try:\n",
    "            df = pd.read_csv(fp, low_memory=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {fp}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Format season as YYYY/YYYY format\n",
    "        if season_code and len(season_code) == 4 and season_code.isdigit():\n",
    "            # Handle formats like \"1920\" or \"2021\"\n",
    "            year1 = int(season_code[:2])\n",
    "            year2 = int(season_code[2:])\n",
    "\n",
    "            # Determine century based on year range\n",
    "            if year1 >= 19 and year1 <= 24:  # 19-24 maps to 2019-2024\n",
    "                year1_full = 2000 + year1\n",
    "            else:\n",
    "                year1_full = 1900 + year1\n",
    "\n",
    "            if year2 >= 19 and year2 <= 99:\n",
    "                if year2 < year1:  # Next year (e.g., 19->20, 23->24)\n",
    "                    year2_full = 2000 + year2\n",
    "                else:\n",
    "                    year2_full = 2000 + year2\n",
    "            else:\n",
    "                year2_full = 1900 + year2\n",
    "\n",
    "            season_formatted = f\"{year1_full}/{year2_full}\"\n",
    "        else:\n",
    "            season_formatted = season_code  # Fallback to original if format is unexpected\n",
    "\n",
    "        # Add Season column right after Div (if Div exists)\n",
    "        if 'Div' in df.columns:\n",
    "            div_idx = df.columns.get_loc('Div')\n",
    "            df.insert(div_idx + 1, 'Season', season_formatted)\n",
    "        else:\n",
    "            df['Season'] = season_formatted\n",
    "\n",
    "        frames.append(df)\n",
    "\n",
    "    all_df = pd.concat(frames, ignore_index=True, sort=False)\n",
    "    return all_df\n",
    "\n",
    "# run the loader\n",
    "all_matches = pd.DataFrame(load_all_matches(DATA_DIR))\n",
    "print(all_matches.columns.tolist())\n",
    "print(all_matches.shape)\n",
    "display(all_matches.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis\n",
    "\n",
    "Before proceeding with data cleaning, let's understand our data better through comprehensive exploratory data analysis. This will help us make informed decisions about preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Shape and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset shape: {all_matches.shape}\")\n",
    "print(f\"Number of seasons/countries covered:\")\n",
    "print(f\"Countries: {all_matches['Div'].str[:-1].nunique()}\")\n",
    "print(f\"Leagues: {all_matches['Div'].nunique()}\")\n",
    "print(f\"Date range: {all_matches['Date'].min()} to {all_matches['Date'].max()}\")\n",
    "\n",
    "# Check basic statistics\n",
    "print(f\"\\nBasic goal statistics:\")\n",
    "print(f\"Total goals per match stats:\")\n",
    "total_goals = all_matches['FTHG'] + all_matches['FTAG']\n",
    "print(total_goals.describe())\n",
    "\n",
    "print(f\"\\nOver/Under 2.5 goals distribution:\")\n",
    "over_2_5 = (total_goals > 2.5).astype(int)\n",
    "print(f\"Over 2.5: {over_2_5.sum()} ({over_2_5.mean():.2%})\")\n",
    "print(f\"Under 2.5: {(~over_2_5.astype(bool)).sum()} ({(1-over_2_5.mean()):.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our target variable (Over/Under 2.5 goals) is perfectly balanced with almost exactly 50/50 split, which is ideal for classification. Mainly because the model won't be biased toward either class and we can use standard accuracy but also because we won't have to do any kind of resampling or rebalancing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed missing values analysis\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'column': all_matches.columns,\n",
    "    'missing_count': all_matches.isnull().sum(),\n",
    "    'missing_percentage': (all_matches.isnull().sum() / len(all_matches)) * 100,\n",
    "    'dtype': all_matches.dtypes\n",
    "})\n",
    "\n",
    "# Filter to show only columns with missing values\n",
    "missing_analysis = missing_analysis[missing_analysis['missing_count'] > 0].sort_values('missing_percentage', ascending=False)\n",
    "\n",
    "print(f\"Columns with missing values: {len(missing_analysis)}\")\n",
    "print(f\"Total columns: {len(all_matches.columns)}\")\n",
    "print(f\"\\nTop 20 columns with highest missing percentage:\")\n",
    "display(missing_analysis.head(20))\n",
    "\n",
    "# Check missing patterns in key variables\n",
    "key_stats = ['HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "print(f\"\\nMissing data in key match statistics:\")\n",
    "for stat in key_stats:\n",
    "    if stat in all_matches.columns:\n",
    "        missing_pct = (all_matches[stat].isnull().sum() / len(all_matches)) * 100\n",
    "        print(f\"{stat}: {missing_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing data analysis reveals that:\n",
    "1. **Betting odds** have the highest missing percentages (80%+) - this is expected as not all bookmakers operate in all leagues/seasons\n",
    "2. **Key match statistics** (shots, corners, fouls, cards) have very low missing rates (<0.1%), which is excellent for our modeling\n",
    "3. Most missing data is in betting-related columns, which we can handle appropriately\n",
    "\n",
    "also we have found 4 unnamed columns that are 100% missing. they're most likely artifacts from csv exports so they're definitely safe to drop outright"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets do a bit more of a in depth analysis, shall we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = all_matches.copy()\n",
    "\n",
    "# missingness flag\n",
    "stats_cols = ['HS','AS','HST','AST','HF','AF','HC','AC','HY','AY','HR','AR']\n",
    "for c in stats_cols:\n",
    "    if c in raw.columns:\n",
    "        raw[f'isna_{c}'] = raw[c].isna().astype(int)\n",
    "\n",
    "# Row-level summary: how many of the 12 stats are missing in the same row?\n",
    "flag_cols = [f'isna_{c}' for c in stats_cols if f'isna_{c}' in raw.columns]\n",
    "raw['missing_count_stats'] = raw[flag_cols].sum(axis=1)\n",
    "\n",
    "# Quick overview\n",
    "print(raw['missing_count_stats'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the rows seem to have no missigness/ However, there are 41 rows that have are missing all 12 variables, which seems pretty clustered. Suggesting that the missing data likely stem from a specific data source or a batch issue rather than random omission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single-stat missing % (already computed as flags)\n",
    "single_rates = (raw[flag_cols].mean() * 100)\n",
    "single_rates.index = [c.replace('isna_', '') for c in single_rates.index]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "ax.bar(single_rates.index, single_rates.values)\n",
    "ax.set_title('Missingness by variables (%)')\n",
    "ax.set_ylabel('% missing')\n",
    "ax.set_xlabel('stat')\n",
    "ax.set_xticklabels(single_rates.index, rotation=45, ha='right')\n",
    "for i, v in enumerate(single_rates.values):\n",
    "    ax.text(i, v, f'{v:.3f}%', ha='center', va='bottom', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "misigness seems uniformly low across all variables, there seems to be no issue with a variable specific collection issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "cmap = 'plasma'\n",
    "\n",
    "# Extract country from Div column (e.g., 'E1' -> 'E', 'SP2' -> 'SP')\n",
    "raw['country_code'] = raw['Div'].str[:-1]\n",
    "\n",
    "# 1️⃣ Country × Stat\n",
    "if 'country_code' in raw.columns:\n",
    "    M1 = raw.groupby('country_code')[flag_cols].mean().mul(100)\n",
    "    order = M1.mean(axis=1).sort_values(ascending=False).index\n",
    "    M1 = M1.loc[order]\n",
    "    M1.columns = [c.replace('isna_', '') for c in M1.columns]\n",
    "\n",
    "    im1 = axes[0].imshow(M1.values, aspect='auto', cmap=cmap)\n",
    "    axes[0].set_xticks(np.arange(M1.shape[1]))\n",
    "    axes[0].set_xticklabels(M1.columns, rotation=45, ha='right')\n",
    "    axes[0].set_yticks(np.arange(M1.shape[0]))\n",
    "    axes[0].set_yticklabels(M1.index)\n",
    "    axes[0].set_title('Country × Stat')\n",
    "    fig.colorbar(im1, ax=axes[0], label='% missing')\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, \"Missing 'Div' column\", ha='center', va='center')\n",
    "    axes[0].set_axis_off()\n",
    "\n",
    "# 2️⃣ Year × Stat (using Date column)\n",
    "if 'Date' in raw.columns:\n",
    "    raw['year'] = pd.to_datetime(raw['Date']).dt.year\n",
    "    M2 = raw.groupby('year')[flag_cols].mean().mul(100)\n",
    "    order = M2.mean(axis=1).sort_values(ascending=False).index\n",
    "    M2 = M2.loc[order]\n",
    "    M2.columns = [c.replace('isna_', '') for c in M2.columns]\n",
    "\n",
    "    im2 = axes[1].imshow(M2.values, aspect='auto', cmap=cmap)\n",
    "    axes[1].set_xticks(np.arange(M2.shape[1]))\n",
    "    axes[1].set_xticklabels(M2.columns, rotation=45, ha='right')\n",
    "    axes[1].set_yticks(np.arange(M2.shape[0]))\n",
    "    axes[1].set_yticklabels(M2.index.astype(int))\n",
    "    axes[1].set_title('Year × Stat')\n",
    "    fig.colorbar(im2, ax=axes[1], label='% missing')\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, \"Missing 'Date' column\", ha='center', va='center')\n",
    "    axes[1].set_axis_off()\n",
    "\n",
    "# 3️⃣ Year × Country\n",
    "needed = {'year', 'country_code'}\n",
    "if needed.issubset(raw.columns):\n",
    "    G = raw.groupby(['year','country_code'])[flag_cols].mean().mul(100)\n",
    "    G['avg_missing'] = G.mean(axis=1)\n",
    "    year_order  = G['avg_missing'].groupby(level=0).mean().sort_values(ascending=False).index\n",
    "    country_order = G['avg_missing'].groupby(level=1).mean().sort_values(ascending=False).index\n",
    "    P3 = (G['avg_missing'].unstack('country_code')\n",
    "          .reindex(index=year_order, columns=country_order)\n",
    "          .fillna(0))\n",
    "\n",
    "    im3 = axes[2].imshow(P3.values, aspect='auto', cmap=cmap)\n",
    "    axes[2].set_xticks(np.arange(P3.shape[1]))\n",
    "    axes[2].set_xticklabels(P3.columns, rotation=45, ha='right')\n",
    "    axes[2].set_yticks(np.arange(P3.shape[0]))\n",
    "    axes[2].set_yticklabels(P3.index.astype(int))\n",
    "    axes[2].set_title('Year × Country')\n",
    "    fig.colorbar(im3, ax=axes[2], label='% missing')\n",
    "else:\n",
    "    axes[2].text(0.5, 0.5, \"Missing required columns\", ha='center', va='center')\n",
    "    axes[2].set_axis_off()\n",
    "\n",
    "# 4️⃣ Country × League\n",
    "needed = {'country_code', 'Div'}\n",
    "if needed.issubset(raw.columns):\n",
    "    G = raw.groupby(['country_code', 'Div'])[flag_cols].mean().mul(100)\n",
    "    G['avg_missing'] = G.mean(axis=1)\n",
    "    P4 = (G['avg_missing'].unstack('Div').fillna(0))\n",
    "    country_order = P4.mean(axis=1).sort_values(ascending=False).index\n",
    "    league_order  = P4.mean(axis=0).sort_values(ascending=False).index\n",
    "    P4 = P4.loc[country_order, league_order]\n",
    "\n",
    "    im4 = axes[3].imshow(P4.values, aspect='auto', cmap=cmap)\n",
    "    axes[3].set_xticks(np.arange(P4.shape[1]))\n",
    "    axes[3].set_xticklabels(P4.columns, rotation=45, ha='right')\n",
    "    axes[3].set_yticks(np.arange(P4.shape[0]))\n",
    "    axes[3].set_yticklabels(P4.index)\n",
    "    axes[3].set_title('Country × League')\n",
    "    fig.colorbar(im4, ax=axes[3], label='% missing')\n",
    "else:\n",
    "    axes[3].text(0.5, 0.5, \"Missing required columns\", ha='center', va='center')\n",
    "    axes[3].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first heatmap shows missing data by country. Turkey has the most missing data by far, with over 1.4 percent missing on average. All other countries have very little missing data, less than 0.5 percent each.\n",
    "\n",
    "The second heatmap shows missing data by year. The years 2023 has slightly more missing data than the other years.\n",
    "\n",
    "The third heatmap combines year and country together. It shows that Turkey has most missing values in 2023. In other years, the missingness is not so bad.\n",
    "\n",
    "The fourth heatmap shows missing data by country and league division. Again, Turkey stands out with the highest missing data. Within each country, different league divisions have similar amounts of missing data, which means the problem is more about the country than about which league tier we look at.\n",
    "\n",
    "Overall, the missing data is not random. It is concentrated mainly in Turkey and in the year 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_cols  = [f'isna_{c}' for c in stats_cols if f'isna_{c}' in raw.columns]\n",
    "top_n      = 15\n",
    "min_matches_ref = 50   # ignore refs with tiny sample sizes\n",
    "\n",
    "def group_missing_rate(df, key):\n",
    "    \"\"\"Return DataFrame with avg % missing across 12 stats, plus counts.\"\"\"\n",
    "    grp = df.groupby(key)[flag_cols]\n",
    "    rate = grp.mean().mul(100).mean(axis=1)\n",
    "    cnt  = df.groupby(key).size()\n",
    "    out  = pd.DataFrame({'rate': rate, 'n': cnt}).sort_values('rate', ascending=False)\n",
    "    return out\n",
    "\n",
    "# 1️⃣ Home, Away, Referee\n",
    "home_df = group_missing_rate(raw, 'HomeTeam') if 'HomeTeam' in raw.columns else pd.DataFrame()\n",
    "away_df = group_missing_rate(raw, 'AwayTeam') if 'AwayTeam' in raw.columns else pd.DataFrame()\n",
    "ref_df  = group_missing_rate(raw, 'Referee')  if 'Referee'  in raw.columns else pd.DataFrame()\n",
    "if not ref_df.empty:\n",
    "    ref_df = ref_df[ref_df['n'] >= min_matches_ref].sort_values('rate', ascending=False)\n",
    "\n",
    "# 2️⃣ Merge for Home vs Away comparison (teams present in both)\n",
    "both = pd.DataFrame()\n",
    "if not home_df.empty and not away_df.empty:\n",
    "    both = (home_df[['rate']].rename(columns={'rate': 'home_rate'})\n",
    "            .merge(away_df[['rate']], left_index=True, right_index=True, how='inner')\n",
    "            .rename(columns={'rate': 'away_rate'}))\n",
    "    both['diff'] = both['home_rate'] - both['away_rate']\n",
    "    both = both.sort_values('home_rate', ascending=False).head(top_n)\n",
    "\n",
    "# =======================\n",
    "# FIGURE 1 — Home & Away\n",
    "# =======================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# A) Top Home teams\n",
    "if not home_df.empty:\n",
    "    htop = home_df.head(top_n)[::-1]\n",
    "    axes[0].barh(htop.index.astype(str), htop['rate'].values, color='#8c564b')\n",
    "    axes[0].set_title('Missingness by HomeTeam (avg % across stats)')\n",
    "    axes[0].set_xlabel('% missing')\n",
    "    for y, (r, n) in enumerate(zip(htop['rate'].values, htop['n'].values)):\n",
    "        axes[0].text(r, y, f'  {r:.2f}% (n={n})', va='center', ha='left', fontsize=9)\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, \"HomeTeam column not found\", ha='center', va='center')\n",
    "    axes[0].set_axis_off()\n",
    "\n",
    "# B) Top Away teams\n",
    "if not away_df.empty:\n",
    "    atop = away_df.head(top_n)[::-1]\n",
    "    axes[1].barh(atop.index.astype(str), atop['rate'].values, color='#1f77b4')\n",
    "    axes[1].set_title('Missingness by AwayTeam (avg % across stats)')\n",
    "    axes[1].set_xlabel('% missing')\n",
    "    for y, (r, n) in enumerate(zip(atop['rate'].values, atop['n'].values)):\n",
    "        axes[1].text(r, y, f'  {r:.2f}% (n={n})', va='center', ha='left', fontsize=9)\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, \"AwayTeam column not found\", ha='center', va='center')\n",
    "    axes[1].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization compares the average percentage of missing match statistics for each team when playing at home (brown dots) versus away (blue dots). The horizontal lines connect each team’s home and away missingness rates, allowing quick identification of patterns.\n",
    "\n",
    "Most teams show very little difference between home and away games, suggesting that data gaps are not related to the venue. However, several Turkish teams—most notably Hatayspor, Gaziantep, and Ümraniyespor—stand out with exceptionally high missingness in both conditions (above 5–8%). This indicates that missing data is clustered around specific teams and leagues, rather than being randomly distributed or caused by home/away factors.\n",
    "\n",
    "Overall, the visualization reinforces that the missingness originates from systematic collection or feed issues affecting particular teams or competitions, rather than isolated recording errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missigness for referees\n",
    "if not ref_df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    rtop = ref_df.head(top_n)[::-1]\n",
    "    ax.barh(rtop.index.astype(str), rtop['rate'].values, color='#9467bd')\n",
    "    ax.set_title(f'Missingness by Referee (avg % across stats, n≥{min_matches_ref})')\n",
    "    ax.set_xlabel('% missing')\n",
    "    for y, (r, n) in enumerate(zip(rtop['rate'].values, rtop['n'].values)):\n",
    "        ax.text(r, y, f'  {r:.2f}% (n={n})', va='center', ha='left', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No referees pass the sample-size filter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper: % missing by group (avg across rows)\n",
    "def pct_missing_by(group_cols, cols):\n",
    "    G = raw.groupby(group_cols)[cols].mean().mul(100)   # % per stat\n",
    "    return G\n",
    "\n",
    "# =========================\n",
    "# Figure A — Year × Stat\n",
    "# =========================\n",
    "if 'year' in raw.columns:\n",
    "    YS = pct_missing_by(['year'], flag_cols)\n",
    "    # order years by overall missingness (desc)\n",
    "    order = YS.mean(axis=1).sort_values(ascending=False).index\n",
    "    YS = YS.loc[order]\n",
    "    YS.columns = [c.replace('isna_', '') for c in YS.columns]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    im = ax.imshow(YS.values, aspect='auto')\n",
    "    ax.set_xticks(np.arange(YS.shape[1]))\n",
    "    ax.set_xticklabels(YS.columns, rotation=45, ha='right')\n",
    "    ax.set_yticks(np.arange(YS.shape[0]))\n",
    "    ax.set_yticklabels(YS.index.astype(int))\n",
    "    ax.set_title('Missingness heatmap (%) — Year × Stat')\n",
    "    fig.colorbar(im, ax=ax, label='% missing')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================\n",
    "# Figure B — Year trend (avg across all stats)\n",
    "# =============================================\n",
    "if 'year' in raw.columns:\n",
    "    Y_avg = (raw.groupby('year')[flag_cols].mean().mul(100).mean(axis=1)\n",
    "             .sort_index())\n",
    "    fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "    ax.plot(Y_avg.index.astype(int), Y_avg.values, marker='o')\n",
    "    for x, y in zip(Y_avg.index, Y_avg.values):\n",
    "        ax.text(x, y, f'{y:.2f}%', va='bottom', ha='center', fontsize=9)\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('% missing (avg across stats)')\n",
    "    ax.set_title('Missingness over time (yearly average)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =========================\n",
    "# Figure C — Hour × Stat\n",
    "# =========================\n",
    "if 'hour' in raw.columns:\n",
    "    # drop hours that are NaN (unparseable)\n",
    "    HH = raw.dropna(subset=['hour']).copy()\n",
    "    HH['hour'] = HH['hour'].astype(int)\n",
    "    HS = (HH.groupby('hour')[flag_cols].mean().mul(100))\n",
    "    # ensure 0–23 present (fill with zeros if absent)\n",
    "    HS = HS.reindex(range(0, 24), fill_value=0)\n",
    "    HS.columns = [c.replace('isna_', '') for c in HS.columns]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    im = ax.imshow(HS.values, aspect='auto')\n",
    "    ax.set_xticks(np.arange(HS.shape[1]))\n",
    "    ax.set_xticklabels(HS.columns, rotation=45, ha='right')\n",
    "    ax.set_yticks(np.arange(HS.shape[0]))\n",
    "    ax.set_yticklabels(HS.index.astype(int))\n",
    "    ax.set_title('Missingness heatmap (%) — Hour × Stat')\n",
    "    fig.colorbar(im, ax=ax, label='% missing')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Sanity Checks\n",
    "\n",
    "Before moving forward, we need to verify that our data makes logical sense. We will check if the relationships between different columns are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_issues = []\n",
    "\n",
    "# Check 1: Full time goals should be >= half time goals\n",
    "print(\"\\nFull Time Goals >= Half Time Goals\")\n",
    "ht_ft_home_check = all_matches['FTHG'] >= all_matches['HTHG']\n",
    "ht_ft_away_check = all_matches['FTAG'] >= all_matches['HTAG']\n",
    "home_violations = (~ht_ft_home_check).sum()\n",
    "away_violations = (~ht_ft_away_check).sum()\n",
    "print(f\"   Home goals violations: {home_violations}\")\n",
    "print(f\"   Away goals violations: {away_violations}\")\n",
    "if home_violations > 0 or away_violations > 0:\n",
    "    sanity_issues.append(f\"FT goals < HT goals: {home_violations + away_violations} cases\")\n",
    "\n",
    "# Check 2: Full time result should match actual goals\n",
    "print(\"\\nFull Time Result matches actual goals\")\n",
    "ftr_check = pd.Series(index=all_matches.index, dtype=bool)\n",
    "ftr_check = (\n",
    "    ((all_matches['FTR'] == 'H') & (all_matches['FTHG'] > all_matches['FTAG'])) |\n",
    "    ((all_matches['FTR'] == 'A') & (all_matches['FTAG'] > all_matches['FTHG'])) |\n",
    "    ((all_matches['FTR'] == 'D') & (all_matches['FTHG'] == all_matches['FTAG']))\n",
    ")\n",
    "ftr_violations = (~ftr_check).sum()\n",
    "print(f\"   FTR mismatches: {ftr_violations}\")\n",
    "if ftr_violations > 0:\n",
    "    sanity_issues.append(f\"FTR doesn't match goals: {ftr_violations} cases\")\n",
    "\n",
    "# Check 3: Half time result should match half time goals\n",
    "print(\"\\nChecking: Half Time Result matches half time goals\")\n",
    "htr_check = pd.Series(index=all_matches.index, dtype=bool)\n",
    "htr_check = (\n",
    "    ((all_matches['HTR'] == 'H') & (all_matches['HTHG'] > all_matches['HTAG'])) |\n",
    "    ((all_matches['HTR'] == 'A') & (all_matches['HTAG'] > all_matches['HTHG'])) |\n",
    "    ((all_matches['HTR'] == 'D') & (all_matches['HTHG'] == all_matches['HTAG']))\n",
    ")\n",
    "htr_violations = (~htr_check).sum()\n",
    "print(f\"   HTR mismatches: {htr_violations}\")\n",
    "if htr_violations > 0:\n",
    "    sanity_issues.append(f\"HTR doesn't match HT goals: {htr_violations} cases\")\n",
    "\n",
    "# Check 4: Shots on target should be <= total shots\n",
    "print(\"\\nShots on Target <= Total Shots\")\n",
    "home_shot_check = all_matches['HST'] <= all_matches['HS']\n",
    "away_shot_check = all_matches['AST'] <= all_matches['AS']\n",
    "home_shot_violations = (~home_shot_check).sum()\n",
    "away_shot_violations = (~away_shot_check).sum()\n",
    "print(f\"   Home shots violations: {home_shot_violations}\")\n",
    "print(f\"   Away shots violations: {away_shot_violations}\")\n",
    "if home_shot_violations > 0 or away_shot_violations > 0:\n",
    "    sanity_issues.append(f\"Shots on target > total shots: {home_shot_violations + away_shot_violations} cases\")\n",
    "\n",
    "# Check 5: Goals should be <= shots on target (generally, but not always)\n",
    "print(\"\\nGoals <= Shots on Target (usually)\")\n",
    "home_goals_shots_check = all_matches['FTHG'] <= all_matches['HST']\n",
    "away_goals_shots_check = all_matches['FTAG'] <= all_matches['AST']\n",
    "home_goals_violations = (~home_goals_shots_check).sum()\n",
    "away_goals_violations = (~away_goals_shots_check).sum()\n",
    "print(f\"   Home goals > shots on target: {home_goals_violations}\")\n",
    "print(f\"   Away goals > shots on target: {away_goals_violations}\")\n",
    "print(f\"   Note: Some violations are possible due to own goals or deflections\")\n",
    "if home_goals_violations > 10 or away_goals_violations > 10:\n",
    "    sanity_issues.append(f\"Goals > shots on target: {home_goals_violations + away_goals_violations} cases (check if excessive)\")\n",
    "\n",
    "# Check 6: Red cards should be <= yellow cards + red cards\n",
    "print(\"\\nCard counts are reasonable\")\n",
    "home_red_check = all_matches['HR'] <= (all_matches['HY'] + all_matches['HR'])\n",
    "away_red_check = all_matches['AR'] <= (all_matches['AY'] + all_matches['AR'])\n",
    "print(f\"   Home card logic violations: {(~home_red_check).sum()}\")\n",
    "print(f\"   Away card logic violations: {(~away_red_check).sum()}\")\n",
    "\n",
    "# Check 7: Negative values check\n",
    "print(\"\\nNo negative values in count columns\")\n",
    "count_columns = ['FTHG', 'FTAG', 'HTHG', 'HTAG', 'HS', 'AS', 'HST', 'AST',\n",
    "                'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "negative_found = False\n",
    "for col in count_columns:\n",
    "    if col in all_matches.columns:\n",
    "        negative_count = (all_matches[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            print(f\"   {col}: {negative_count} negative values\")\n",
    "            negative_found = True\n",
    "            sanity_issues.append(f\"{col} has {negative_count} negative values\")\n",
    "if not negative_found:\n",
    "    print(f\"   No negative values found\")\n",
    "\n",
    "# Check 8: Extreme values check\n",
    "print(\"\\nExtreme values that might be data errors\")\n",
    "extreme_checks = {\n",
    "    'FTHG': 15,\n",
    "    'FTAG': 15,\n",
    "    'HS': 50,\n",
    "    'AS': 50,\n",
    "    'HC': 30,\n",
    "    'AC': 30,\n",
    "    'HY': 10,\n",
    "    'AY': 10,\n",
    "    'HR': 5,\n",
    "    'AR': 5\n",
    "}\n",
    "for col, threshold in extreme_checks.items():\n",
    "    if col in all_matches.columns:\n",
    "        extreme_count = (all_matches[col] > threshold).sum()\n",
    "        if extreme_count > 0:\n",
    "            max_value = all_matches[col].max()\n",
    "            print(f\"   {col} > {threshold}: {extreme_count} cases (max: {max_value})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sanity checks help us verify that the data is internally consistent. We check things like full time goals being at least as many as half time goals, that the match result codes match the actual goal counts, that shots on target do not exceed total shots, and that there are no negative values in count columns. These checks help identify data entry errors or corruption before we use the data for modeling.\n",
    "\n",
    "Our data passed most checks well. Full time goals are always at least as many as half time goals, which is correct. The full time result codes match the actual scores perfectly.\n",
    "\n",
    "We found 41 matches where the half time result code does not match the half time goals. This is a small number out of 42,593 matches, so it is likely just data entry errors in those specific matches.\n",
    "\n",
    "We found 6 matches where shots on target are higher than total shots. This is probably a recording error but only affects 6 matches so it is not a big problem.\n",
    "\n",
    "We found 234 matches where a team scored more goals than they had shots on target. This can happen in real football due to own goals or deflections, so these are not necessarily errors.\n",
    "\n",
    "We found one match where a team got 9 red cards. This is extremely unusual and might be a data error, but it is only one match out of thousands.\n",
    "\n",
    "Overall, the data quality is very good. The few issues we found affect less than 1 percent of matches and will not significantly impact our model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 League and Country Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# League distribution\n",
    "league_counts = all_matches['Div'].value_counts()\n",
    "print(\"League distribution:\")\n",
    "display(league_counts)\n",
    "\n",
    "# Country mapping for better understanding\n",
    "country_mapping = {\n",
    "    'E': 'England', 'SC': 'Scotland', 'SP': 'Spain', 'I': 'Italy',\n",
    "    'D': 'Germany', 'F': 'France', 'N': 'Netherlands', 'B': 'Belgium',\n",
    "    'P': 'Portugal', 'T': 'Turkey', 'G': 'Greece'\n",
    "}\n",
    "\n",
    "all_matches['Country'] = all_matches['Div'].str[:-1].map(country_mapping)\n",
    "country_counts = all_matches['Country'].value_counts()\n",
    "print(f\"\\nMatches per country:\")\n",
    "display(country_counts)\n",
    "\n",
    "# Visualize the distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Country distribution\n",
    "country_counts_sorted = country_counts.dropna().sort_values(ascending=False)\n",
    "bars = ax1.bar(country_counts_sorted.index, country_counts_sorted.values, color='skyblue')\n",
    "ax1.set_title('Matches per Country')\n",
    "ax1.set_xlabel('Country')\n",
    "ax1.set_ylabel('Number of Matches')\n",
    "ax1.tick_params(axis='x', rotation=30)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# data labels\n",
    "for rect in bars:\n",
    "    height = rect.get_height()\n",
    "    ax1.text(rect.get_x() + rect.get_width()/2, height, f\"{int(height):,}\",\n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Goals distribution\n",
    "total_goals = all_matches['FTHG'] + all_matches['FTAG']\n",
    "max_g = int(np.nanmax(total_goals))\n",
    "bins = np.arange(-0.5, max(10, max_g) + 1.5, 1)\n",
    "\n",
    "ax2.hist(total_goals, bins=bins, color='lightcoral', alpha=0.7)\n",
    "ax2.axvline(x=2.5, linestyle='--', linewidth=2, label='2.5 goals threshold')\n",
    "ax2.set_title('Distribution of Total Goals per Match')\n",
    "ax2.set_xlabel('Total Goals')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_xticks(range(0, max(10, max_g) + 1))\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "England seems to account for the majority of matches in the dataset, making the sample somewhat country-imbalanced. This suggests that model training should be performed separately for each country, or at least include country-specific components, to prevent English leagues from dominating the overall model behavior.  \n",
    "\n",
    "When building time-aware models, it would also be beneficial to use chronological splits within each country and consider assigning higher weights to more recent matches, since they better reflect current team dynamics and scoring trends.  \n",
    "\n",
    "Alse, the distribution of total goals per match is right-skewed, with mode around 2–3 goals. The red dashed line at 2.5 goals marks the classification threshold for our target variable. Visually, the mass on either side of this threshold is roughly equal, which confirms the balanced 50/50 split observed in the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Handling csv issues\n",
    "It seems like the renaming and loading went smoothly! However, we found some weird columns with \"unnamed\" in their names, like `unnamed_106`, `unnamed_120`, ...  \n",
    "That sometimes happens when excel files have extra blank columns. We'll take a quick look to see if they have any data, and if they're totally empty (full of NaNs), we'll just get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnamed_cols = [c for c in all_matches.columns if c.lower().startswith(\"unnamed\")]\n",
    "all_matches[unnamed_cols].isna().mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're 100% full of NaNs so we can now safely drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = all_matches.drop(columns=unnamed_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Normalizing league codes\n",
    "Let's normalize the leagues, as English and Scottish leagues have the best leagues interpreted as E0, SC0, respectively. All other countries mark the best league as CountryCode1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = all_matches['Div'].str.startswith(('E', 'SC'))\n",
    "all_matches.loc[mask, 'Div'] = all_matches.loc[mask, 'Div'].apply(\n",
    "    lambda x: f\"{x[:-1]}{int(x[-1]) + 1}\"\n",
    ")\n",
    "\n",
    "print(all_matches['Div'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Handling English and Scottish yellow cards\n",
    "We need to take care of the first note in notex.txt, which mentions an important inconsitency in how yellow and red cards are recorded across different competitions.  \n",
    "\n",
    "In English and Scottish leagues, when a player receives a second yellow card that leads to a red card, the initial yellow card is not counted in the match statistics, only the red card is recorded. However, European and international competitions record both: the second yellow is counted as an additional yellow card plus a red card \n",
    "\n",
    "As a result, yellow card totals in English and Scottish matches can underestimate the true number of yellow cards compared to other leagues. To correct for this and ensure consistency across competitions, we applied a simple adjustment:\n",
    "- whenever a team has exactly one red card and one yellow card, we add one additional yellow card.\n",
    "- and if a team has 0 reds, 2 or more reds, or 1 red but no yellows, we make no adjustment.\n",
    "\n",
    "We acknowledge that this rule is an approximation, our adjustment may not always be the case and it may introduce some bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = all_matches['Div'].str.startswith(('E', 'SC'))\n",
    "red_mask = mask & ((all_matches['HR'] == 1) | (all_matches['AR'] == 1))\n",
    "\n",
    "print(\"Before adjustment (sample):\")\n",
    "print(all_matches.loc[red_mask, ['Div', 'HY', 'HR', 'AY', 'AR']].head())\n",
    "\n",
    "all_matches.loc[mask & (all_matches['HR'] == 1) & (all_matches['HY'] == 0), 'HY'] += 1\n",
    "all_matches.loc[mask & (all_matches['AR'] == 1) & (all_matches['AY'] == 0), 'AY'] += 1\n",
    "\n",
    "print(\"\\nAfter adjustment (sample):\")\n",
    "print(all_matches.loc[red_mask, ['Div', 'HY', 'HR', 'AY', 'AR']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Correcting data types\n",
    "Now, let's inspect the data types of our columns. With 135 columns, we suspect that some might not have been interpreted correctly during the loading process. Checking the data types is an important step before proceeding with any further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, dtype in all_matches.dtypes.items():\n",
    "    print(f\"{col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_columns = ['Date', 'Time']\n",
    "\n",
    "category_columns = ['Div', 'HomeTeam', 'AwayTeam', 'FTR', 'HTR', 'Referee', 'Country']\n",
    "\n",
    "int_columns = ['FTHG', 'FTAG', 'HTHG', 'HTAG', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "\n",
    "float_columns = ['B365CH', 'BWCA', '1XBH']\n",
    "\n",
    "for col in time_columns:\n",
    "    if col == 'Date':\n",
    "        all_matches[col] = pd.to_datetime(all_matches[col])\n",
    "    else:\n",
    "        all_matches[col] = pd.to_datetime(all_matches[col], format='%H:%M').dt.time\n",
    "\n",
    "for col in category_columns:\n",
    "    all_matches[col] = all_matches[col].astype('category')\n",
    "\n",
    "for col in int_columns:\n",
    "    all_matches[col] = pd.to_numeric(all_matches[col], errors='coerce').astype('Int64')\n",
    "\n",
    "for col in float_columns:\n",
    "    all_matches[col] = pd.to_numeric(all_matches[col], errors='coerce').astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, dtype in all_matches.dtypes.items():\n",
    "    print(f\"{col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Missing value imputation with domain-specific strategies\n",
    "\n",
    "Based on our EDA, we'll handle missing values with different strategies based on data characteristics:\n",
    "\n",
    "1. **Key match statistics**: Very few missing values (~0.1%) - use SimpleImputer with median strategy\n",
    "2. **Betting odds**: High missingness (80%+) but match-specific - use cross-bookmaker median imputation per match, then overall median fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns with low missingness that need imputation\n",
    "low_missingness_cols = ['HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "\n",
    "# Check current missing values before imputation\n",
    "print(\"Missing values before imputation:\")\n",
    "for col in low_missingness_cols:\n",
    "    if col in all_matches.columns:\n",
    "        missing_count = all_matches[col].isnull().sum()\n",
    "        missing_pct = (missing_count / len(all_matches)) * 100\n",
    "        print(f\"{col}: {missing_count} ({missing_pct:.3f}%)\")\n",
    "\n",
    "# Apply median imputation for numerical match statistics\n",
    "if any(all_matches[col].isnull().sum() > 0 for col in low_missingness_cols if col in all_matches.columns):\n",
    "    match_stats_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "    # Only impute columns that actually exist and have missing values\n",
    "    cols_to_impute = [col for col in low_missingness_cols if col in all_matches.columns and all_matches[col].isnull().sum() > 0]\n",
    "\n",
    "    if cols_to_impute:\n",
    "        print(f\"\\nApplying median imputation to: {cols_to_impute}\")\n",
    "        all_matches[cols_to_impute] = match_stats_imputer.fit_transform(all_matches[cols_to_impute])\n",
    "\n",
    "        print(\"Imputation completed. Median values used:\")\n",
    "        for col in cols_to_impute:\n",
    "            median_val = all_matches[col].median()\n",
    "            print(f\"  {col}: {median_val}\")\n",
    "    else:\n",
    "        print(\"No missing values found in match statistics columns.\")\n",
    "else:\n",
    "    print(\"No missing values found in match statistics columns.\")\n",
    "\n",
    "# Handle categorical columns separately\n",
    "if 'Referee' in all_matches.columns:\n",
    "    referee_missing = all_matches['Referee'].isnull().sum()\n",
    "    if referee_missing > 0:\n",
    "        if 'Unknown' not in all_matches['Referee'].cat.categories:\n",
    "            all_matches['Referee'] = all_matches['Referee'].cat.add_categories(['Unknown'])\n",
    "        all_matches['Referee'] = all_matches['Referee'].fillna('Unknown')\n",
    "        print(f\"Filled {referee_missing} missing referees with 'Unknown'\")\n",
    "\n",
    "if 'Time' in all_matches.columns:\n",
    "    time_missing = all_matches['Time'].isnull().sum()\n",
    "    if time_missing > 0:\n",
    "        all_matches['Time'] = all_matches['Time'].fillna(pd.to_datetime('15:00', format='%H:%M').time())\n",
    "        print(f\"Filled {time_missing} missing times with '15:00'\")\n",
    "\n",
    "# Verify no missing values remain in core match statistics\n",
    "print(f\"\\nVerification - remaining missing values in core columns:\")\n",
    "verification_cols = low_missingness_cols + ['Referee', 'Time']\n",
    "total_missing = 0\n",
    "for col in verification_cols:\n",
    "    if col in all_matches.columns:\n",
    "        missing = all_matches[col].isnull().sum()\n",
    "        if missing > 0:\n",
    "            print(f\"{col}: {missing}\")\n",
    "            total_missing += missing\n",
    "\n",
    "if total_missing == 0:\n",
    "    print(\"All core match statistics successfully imputed - no missing values remain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle the gaps, we used median imputation for the numerical match statistics because they're robust to outliers and preserve the central distribution of the data. For categorical fields, missing Referee entries were replaced with ‘Unknown’, and missing Time values were set to 15:00, which should be the typical match kickoff time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle betting odds imputation with proper market categorization\n",
    "# Based on notes.txt, betting odds are organized by market type (1X2, O/U 2.5, Asian Handicap)\n",
    "\n",
    "# Identify betting odds columns\n",
    "betting_cols = [col for col in all_matches.columns if any(bookmaker in col for bookmaker in ['B365', 'BW', 'PS', 'IW', 'LB', 'WH', 'SJ', 'VC', 'BF', '1XB', 'CL', 'GB', 'SO', 'SB', 'SY', 'Max', 'Avg', 'Bb'])]\n",
    "\n",
    "print(f\"Found {len(betting_cols)} betting odds columns\")\n",
    "\n",
    "# Analyze missing patterns in betting odds\n",
    "betting_missing_analysis = []\n",
    "high_missing_cols = []\n",
    "for col in betting_cols:\n",
    "    missing_count = all_matches[col].isnull().sum()\n",
    "    missing_pct = (missing_count / len(all_matches)) * 100\n",
    "    betting_missing_analysis.append({\n",
    "        'column': col,\n",
    "        'missing_count': missing_count,\n",
    "        'missing_pct': missing_pct\n",
    "    })\n",
    "    if missing_pct > 80:  # Track columns with very high missingness\n",
    "        high_missing_cols.append(col)\n",
    "\n",
    "print(f\"Columns with >80% missing values: {len(high_missing_cols)}\")\n",
    "print(\"Sample of betting odds with lower missingness (<80%):\")\n",
    "for item in sorted(betting_missing_analysis, key=lambda x: x['missing_pct'])[:15]:\n",
    "    if item['missing_pct'] < 80:\n",
    "        print(f\"  {item['column']}: {item['missing_pct']:.1f}%\")\n",
    "\n",
    "# Categorize betting odds by market type based on notes.txt\n",
    "def categorize_betting_market(col_name):\n",
    "    \"\"\"Categorize betting column by market type based on column name patterns\"\"\"\n",
    "    col = col_name.upper()\n",
    "\n",
    "    # 1X2 Market (Home/Draw/Away win)\n",
    "    if col.endswith('H') and not any(x in col for x in ['AH', '>', '<']):\n",
    "        return 'home_win'\n",
    "    elif col.endswith('D') and not any(x in col for x in ['AH', '>', '<']):\n",
    "        return 'draw'\n",
    "    elif col.endswith('A') and not any(x in col for x in ['AH', '>', '<']):\n",
    "        return 'away_win'\n",
    "\n",
    "    # Over/Under 2.5 Goals Market\n",
    "    elif '>2.5' in col or 'O2.5' in col:\n",
    "        return 'over_2_5'\n",
    "    elif '<2.5' in col or 'U2.5' in col:\n",
    "        return 'under_2_5'\n",
    "\n",
    "    # Asian Handicap Market\n",
    "    elif 'AH' in col and col.endswith('H'):\n",
    "        return 'ah_home'\n",
    "    elif 'AH' in col and col.endswith('A'):\n",
    "        return 'ah_away'\n",
    "    elif 'AH' in col and not col.endswith(('H', 'A')):\n",
    "        return 'ah_handicap'\n",
    "\n",
    "    # Other markets\n",
    "    elif 'C>' in col:  # Corner markets\n",
    "        return 'corners'\n",
    "    elif any(x in col for x in ['FKCH', 'FKCA']):  # Free kicks\n",
    "        return 'free_kicks'\n",
    "\n",
    "    return 'other'\n",
    "\n",
    "# Group betting columns by market type\n",
    "market_groups = {}\n",
    "for col in betting_cols:\n",
    "    market_type = categorize_betting_market(col)\n",
    "    market_groups.setdefault(market_type, []).append(col)\n",
    "\n",
    "print(f\"\\nBetting odds grouped by market type:\")\n",
    "for market_type, columns in market_groups.items():\n",
    "    avg_missing = np.mean([item['missing_pct'] for item in betting_missing_analysis if item['column'] in columns])\n",
    "    print(f\"  {market_type}: {len(columns)} columns (avg missing: {avg_missing:.1f}%)\")\n",
    "\n",
    "# Apply cross-bookmaker median imputation within each market for each match\n",
    "total_imputed = 0\n",
    "markets_processed = []\n",
    "\n",
    "for market_type, columns in market_groups.items():\n",
    "    if len(columns) > 1 and market_type != 'other':  # Only process markets with multiple bookmakers\n",
    "        print(f\"\\nProcessing {market_type} market ({len(columns)} columns)...\")\n",
    "        markets_processed.append(market_type)\n",
    "\n",
    "        # Check how much data we have for this market\n",
    "        market_data_availability = []\n",
    "        for col in columns:\n",
    "            non_missing = all_matches[col].notna().sum()\n",
    "            market_data_availability.append(non_missing)\n",
    "\n",
    "        if max(market_data_availability) > 1000:  # Only process if we have reasonable data\n",
    "            match_imputed = 0\n",
    "\n",
    "            # Process each match individually\n",
    "            for idx in all_matches.index:\n",
    "                # Get odds for this match across all bookmakers for this market\n",
    "                match_odds = all_matches.loc[idx, columns]\n",
    "\n",
    "                # If any values are missing but others exist, use median of available bookmakers\n",
    "                if match_odds.isnull().any() and not match_odds.isnull().all():\n",
    "                    match_median = match_odds.median()\n",
    "\n",
    "                    # Fill missing values with the cross-bookmaker median for this match\n",
    "                    for col in columns:\n",
    "                        if pd.isnull(all_matches.loc[idx, col]):\n",
    "                            all_matches.loc[idx, col] = match_median\n",
    "                            total_imputed += 1\n",
    "                            match_imputed += 1\n",
    "\n",
    "            print(f\"  {market_type}: {match_imputed} values imputed using cross-bookmaker median\")\n",
    "\n",
    "print(f\"\\nCross-bookmaker imputation completed: {total_imputed} values imputed across {len(markets_processed)} markets\")\n",
    "\n",
    "# For remaining missing values, apply conservative strategy\n",
    "# Only use overall median fallback for markets with reasonable data coverage\n",
    "remaining_imputed = 0\n",
    "columns_fully_imputed = []\n",
    "\n",
    "for market_type, columns in market_groups.items():\n",
    "    if market_type in ['home_win', 'draw', 'away_win', 'over_2_5', 'under_2_5']:  # Core markets only\n",
    "        for col in columns:\n",
    "            missing_before = all_matches[col].isnull().sum()\n",
    "            data_coverage = (all_matches[col].notna().sum() / len(all_matches)) * 100\n",
    "\n",
    "            # Only apply fallback imputation if we have at least 10% data coverage\n",
    "            if missing_before > 0 and data_coverage >= 10:\n",
    "                overall_median = all_matches[col].median()\n",
    "                all_matches[col] = all_matches[col].fillna(overall_median)\n",
    "                remaining_imputed += missing_before\n",
    "                columns_fully_imputed.append(col)\n",
    "\n",
    "print(f\"Overall median fallback applied to {len(columns_fully_imputed)} columns: {remaining_imputed} values imputed\")\n",
    "\n",
    "# For columns with <10% data coverage, we'll exclude them from modeling rather than impute\n",
    "excluded_cols = []\n",
    "for col in betting_cols:\n",
    "    data_coverage = (all_matches[col].notna().sum() / len(all_matches)) * 100\n",
    "    if data_coverage < 10:\n",
    "        excluded_cols.append(col)\n",
    "\n",
    "print(f\"\\nColumns excluded due to <10% data coverage: {len(excluded_cols)}\")\n",
    "print(\"These will be excluded from the extended dataset to avoid poor imputation quality\")\n",
    "\n",
    "# Verify imputation results for key markets\n",
    "print(f\"\\nVerification - missing values after imputation for key betting markets:\")\n",
    "key_betting_cols = [col for col in betting_cols if col not in excluded_cols][:15]  # Check sample\n",
    "final_missing = 0\n",
    "for col in key_betting_cols:\n",
    "    missing = all_matches[col].isnull().sum()\n",
    "    if missing > 0:\n",
    "        data_coverage = (all_matches[col].notna().sum() / len(all_matches)) * 100\n",
    "        print(f\"  {col}: {missing} missing ({data_coverage:.1f}% coverage)\")\n",
    "        final_missing += missing\n",
    "\n",
    "if final_missing == 0:\n",
    "    print(\"Key betting odds successfully imputed\")\n",
    "else:\n",
    "    print(f\"⚠ {final_missing} missing values remain in key betting columns\")\n",
    "\n",
    "# Update betting features list to exclude low-coverage columns\n",
    "print(f\"\\nUpdating betting features list:\")\n",
    "print(f\"Original betting columns: {len(betting_cols)}\")\n",
    "print(f\"Excluded low-coverage columns: {len(excluded_cols)}\")\n",
    "print(f\"Final betting columns for modeling: {len(betting_cols) - len(excluded_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Outlier detection and handling\n",
    "\n",
    "Following the methodology from Week1 (house pricing), we'll use z-score analysis to detect outliers in match statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical columns for outlier detection\n",
    "match_stats_cols = ['HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']\n",
    "numerical_cols = ['FTHG', 'FTAG', 'HTHG', 'HTAG'] + match_stats_cols\n",
    "\n",
    "# Calculate z-scores for numerical columns\n",
    "print(\"Outlier analysis using z-score > 3:\")\n",
    "outlier_counts = {}\n",
    "\n",
    "for col in numerical_cols:\n",
    "    if col in all_matches.columns:\n",
    "        z_scores = np.abs(zscore(all_matches[col].dropna()))\n",
    "        outliers = (z_scores > 3).sum()\n",
    "        outlier_counts[col] = outliers\n",
    "        if outliers > 0:\n",
    "            print(f\"{col}: {outliers} outliers ({outliers/len(all_matches)*100:.2f}%)\")\n",
    "\n",
    "# Look at extreme cases\n",
    "print(f\"\\nExamples of potential outliers:\")\n",
    "print(f\"Highest total goals: {all_matches['FTHG'].max() + all_matches['FTAG'].max()}\")\n",
    "print(f\"Most shots in a match: {all_matches['HS'].max() + all_matches['AS'].max()}\")\n",
    "print(f\"Most cards in a match: {all_matches['HY'].max() + all_matches['AY'].max()}\")\n",
    "\n",
    "# Visualize outliers for key variables\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "key_vars = ['FTHG', 'FTAG', 'HS', 'AS']\n",
    "\n",
    "for i, var in enumerate(key_vars):\n",
    "    row, col = i // 2, i % 2\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    # Box plot to show outliers\n",
    "    all_matches[var].plot(kind='box', ax=ax)\n",
    "    ax.set_title(f'Box Plot of {var}')\n",
    "    ax.set_ylabel(var)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# For football data, we'll be more conservative with outlier removal\n",
    "# as extreme scores can be legitimate (unlike house prices)\n",
    "print(f\"\\nDecision: Keep outliers for football data as high scores/stats can be legitimate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature engineering\n",
    "\n",
    "Based on soccer domain knowledge and the course materials, we'll create meaningful features that could help predict Over/Under 2.5 goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Target variable creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the main target variable: Over/Under 2.5 goals\n",
    "all_matches['total_goals'] = all_matches['FTHG'] + all_matches['FTAG']\n",
    "all_matches['over_2_5'] = (all_matches['total_goals'] > 2.5).astype(int)\n",
    "\n",
    "print(\"Target variable distribution:\")\n",
    "print(all_matches['over_2_5'].value_counts())\n",
    "print(f\"Over 2.5 rate: {all_matches['over_2_5'].mean():.2%}\")\n",
    "\n",
    "# Also create alternative targets for analysis\n",
    "all_matches['over_1_5'] = (all_matches['total_goals'] > 1.5).astype(int)\n",
    "all_matches['over_3_5'] = (all_matches['total_goals'] > 3.5).astype(int)\n",
    "\n",
    "print(f\"\\nOther thresholds:\")\n",
    "print(f\"Over 1.5 rate: {all_matches['over_1_5'].mean():.2%}\")\n",
    "print(f\"Over 3.5 rate: {all_matches['over_3_5'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Basic feature engineering\n",
    "\n",
    "Creating features that capture match dynamics and team performance patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Shot efficiency features\n",
    "all_matches['home_shot_accuracy'] = all_matches['HST'] / (all_matches['HS'] + 0.001)  # avoid division by zero\n",
    "all_matches['away_shot_accuracy'] = all_matches['AST'] / (all_matches['AS'] + 0.001)\n",
    "all_matches['total_shots'] = all_matches['HS'] + all_matches['AS']\n",
    "all_matches['total_shots_on_target'] = all_matches['HST'] + all_matches['AST']\n",
    "\n",
    "# 2. Attacking vs Defensive balance\n",
    "all_matches['shot_dominance'] = (all_matches['HS'] - all_matches['AS']) / (all_matches['HS'] + all_matches['AS'] + 0.001)\n",
    "all_matches['corner_dominance'] = (all_matches['HC'] - all_matches['AC']) / (all_matches['HC'] + all_matches['AC'] + 0.001)\n",
    "\n",
    "# 3. Game intensity features\n",
    "all_matches['total_fouls'] = all_matches['HF'] + all_matches['AF']\n",
    "all_matches['total_cards'] = all_matches['HY'] + all_matches['AY'] + all_matches['HR'] + all_matches['AR']\n",
    "all_matches['card_intensity'] = all_matches['total_cards'] / (all_matches['total_fouls'] + 0.001)\n",
    "\n",
    "# 4. Half-time patterns\n",
    "all_matches['ht_total_goals'] = all_matches['HTHG'] + all_matches['HTAG']\n",
    "all_matches['second_half_goals'] = all_matches['total_goals'] - all_matches['ht_total_goals']\n",
    "\n",
    "# 5. League tier (lower tiers might have different patterns)\n",
    "all_matches['league_tier'] = all_matches['Div'].str[-1].astype(int)\n",
    "\n",
    "# 6. Season timing features\n",
    "all_matches['month'] = all_matches['Date'].dt.month\n",
    "all_matches['is_weekend'] = all_matches['Date'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "\n",
    "print(\"Created basic engineered features:\")\n",
    "new_features = ['home_shot_accuracy', 'away_shot_accuracy', 'total_shots', 'total_shots_on_target',\n",
    "               'shot_dominance', 'corner_dominance', 'total_fouls', 'total_cards', 'card_intensity',\n",
    "               'ht_total_goals', 'second_half_goals', 'league_tier', 'month', 'is_weekend']\n",
    "\n",
    "for feature in new_features:\n",
    "    print(f\"- {feature}: mean={all_matches[feature].mean():.3f}, std={all_matches[feature].std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check what columns we actually have available\n",
    "available_cols = all_matches.columns.tolist()\n",
    "\n",
    "# Check for core match info columns\n",
    "core_match_info = ['Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG', 'HTR']\n",
    "\n",
    "# Check for match statistics columns from notes.txt\n",
    "match_stats_from_notes = ['Attendance', 'Referee', 'HS', 'AS', 'HST', 'AST', 'HHW', 'AHW', 'HC', 'AC',\n",
    "                         'HF', 'AF', 'HFKC', 'AFKC', 'HO', 'AO', 'HY', 'AY', 'HR', 'AR', 'HBP', 'ABP']\n",
    "\n",
    "# Identify all available core and match statistics columns\n",
    "basic_available = [col for col in core_match_info if col in available_cols]\n",
    "extended_available = [col for col in core_match_info + match_stats_from_notes if col in available_cols]\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Basic dataset core columns available: {len(basic_available)}\")\n",
    "print(f\"Extended dataset core columns available: {len(extended_available)}\")\n",
    "\n",
    "# Identify categorical and numerical columns for modeling\n",
    "categorical_features = ['Div', 'HomeTeam', 'AwayTeam', 'Country', 'FTR', 'HTR', 'Referee']  # League division, teams, and results\n",
    "ordinal_features = ['league_tier', 'month']  # Features with natural ordering\n",
    "\n",
    "# BASIC DATASET: Core match information (what would be available from basic match reports)\n",
    "basic_core_features = [col for col in ['Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR'] if col in all_matches.columns]\n",
    "\n",
    "# Engineered features that can be created from basic core columns only\n",
    "basic_engineered_features = [\n",
    "    'total_goals',   # Goal-based features\n",
    "    'league_tier', 'month', 'is_weekend'  # Date/league features\n",
    "]\n",
    "\n",
    "# EXTENDED DATASET: All available match data including detailed statistics\n",
    "extended_core_features = [col for col in [\n",
    "    # Core match info\n",
    "    'Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG', 'HTR',\n",
    "    # Match statistics\n",
    "    'Attendance', 'Referee', 'HS', 'AS', 'HST', 'AST', 'HHW', 'AHW', 'HC', 'AC',\n",
    "    'HF', 'AF', 'HFKC', 'AFKC', 'HO', 'AO', 'HY', 'AY', 'HR', 'AR', 'HBP', 'ABP'\n",
    "] if col in all_matches.columns]\n",
    "\n",
    "# All engineered features (using rich match statistics)\n",
    "extended_engineered_features = [\n",
    "    'total_goals', 'ht_total_goals', 'second_half_goals',  # Goal-based\n",
    "    'home_shot_accuracy', 'away_shot_accuracy', 'total_shots', 'total_shots_on_target',  # Shot-based\n",
    "    'shot_dominance', 'corner_dominance', 'total_fouls', 'total_cards', 'card_intensity',  # Game dynamics\n",
    "    'league_tier', 'month', 'is_weekend'  # Date/league features\n",
    "]\n",
    "\n",
    "# Extended features (betting odds - only high-quality columns after imputation)\n",
    "betting_features = []\n",
    "for col in all_matches.columns:\n",
    "    # Check if it's a betting column and has good data coverage (>10%)\n",
    "    if any(bookmaker in col for bookmaker in ['B365', 'BW', 'PS', 'IW', 'LB', 'WH', 'SJ', 'VC', 'BF', '1XB']) and col not in categorical_features:\n",
    "        data_coverage = (all_matches[col].notna().sum() / len(all_matches)) * 100\n",
    "        if data_coverage >= 10:  # Only include columns with at least 10% data coverage\n",
    "            betting_features.append(col)\n",
    "\n",
    "print(f\"\\nBASIC DATASET:\")\n",
    "print(f\"  Core features: {len(basic_core_features)} - {basic_core_features}\")\n",
    "print(f\"  Engineered features: {len(basic_engineered_features)} - {basic_engineered_features}\")\n",
    "print(f\"  Total basic features: {len(basic_core_features + basic_engineered_features)}\")\n",
    "\n",
    "print(f\"\\nEXTENDED DATASET:\")\n",
    "print(f\"  Core features: {len(extended_core_features)}\")\n",
    "print(f\"  Engineered features: {len(extended_engineered_features)}\")\n",
    "print(f\"  Betting features with >10% coverage: {len(betting_features)}\")\n",
    "print(f\"  Total extended features: {len(extended_core_features + extended_engineered_features + betting_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Base feature df engineering\n",
    "\n",
    "Start with core + basic engineered features as the foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with core and basic engineered features\n",
    "df_basic = all_matches[['Div', 'Season', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR',\n",
    "                   'total_goals', 'league_tier', 'month', 'is_weekend', \"over_2_5\"]].copy()\n",
    "# by date for time-based features\n",
    "df_basic = df_basic.sort_values(['Div', 'Date']).reset_index(drop=True)\n",
    "print(f\"Columns: {df_basic.columns.tolist()}\")\n",
    "df_basic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Days since last match\n",
    "\n",
    "\n",
    "**Calculate days since last match for both home and away teams.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days since last match for each team\n",
    "df_basic['home_days_since_last'] = np.nan\n",
    "df_basic['away_days_since_last'] = np.nan\n",
    "for team in df_basic['HomeTeam'].unique():\n",
    "    home_mask = df_basic['HomeTeam'] == team\n",
    "    away_mask = df_basic['AwayTeam'] == team\n",
    "    team_matches = df_basic[home_mask | away_mask].sort_values('Date')\n",
    "    #days between matches\n",
    "    team_matches['days_diff'] = team_matches['Date'].diff().dt.days\n",
    "    # to home/away columns\n",
    "    for idx, row in team_matches.iterrows():\n",
    "        if df_basic.loc[idx, 'HomeTeam'] == team:\n",
    "            df_basic.loc[idx, 'home_days_since_last'] = row['days_diff']\n",
    "        else:\n",
    "            df_basic.loc[idx, 'away_days_since_last'] = row['days_diff']\n",
    "# first matches filling with median\n",
    "df_basic['home_days_since_last'].fillna(df_basic['home_days_since_last'].median(), inplace=True)\n",
    "df_basic['away_days_since_last'].fillna(df_basic['away_days_since_last'].median(), inplace=True)\n",
    "\n",
    "print(f\"Home days since last - mean: {df_basic['home_days_since_last'].mean():.1f}, median: {df_basic['home_days_since_last'].median():.1f}\")\n",
    "print(f\"Away days since last - mean: {df_basic['away_days_since_last'].mean():.1f}, median: {df_basic['away_days_since_last'].median():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Everything seems ok, the usual pause is one week (expected), but they are visible pauses between individual seasons.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 5-match Moving Averages\n",
    "\n",
    "**Calculate 5-match moving averages for goals scored and conceded.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basic['home_goals_ma5'] = np.nan\n",
    "df_basic['home_conceded_ma5'] = np.nan\n",
    "df_basic['away_goals_ma5'] = np.nan\n",
    "df_basic['away_conceded_ma5'] = np.nan\n",
    "\n",
    "for team in df_basic['HomeTeam'].unique():\n",
    "\n",
    "    # Home matches\n",
    "    home_mask = df_basic['HomeTeam'] == team\n",
    "    home_dates = df_basic[home_mask].sort_values('Date').index\n",
    "    for i, idx in enumerate(home_dates):\n",
    "        if i >= 5:\n",
    "            last_5_home = df_basic.loc[home_dates[i-5:i]]\n",
    "            df_basic.loc[idx, 'home_goals_ma5'] = last_5_home['FTHG'].mean()\n",
    "            df_basic.loc[idx, 'home_conceded_ma5'] = last_5_home['FTAG'].mean()\n",
    "\n",
    "    # Away matches\n",
    "    away_mask = df_basic['AwayTeam'] == team\n",
    "    away_dates = df_basic[away_mask].sort_values('Date').index\n",
    "\n",
    "    for i, idx in enumerate(away_dates):\n",
    "        if i >= 5:\n",
    "            last_5_away = df_basic.loc[away_dates[i-5:i]]\n",
    "            df_basic.loc[idx, 'away_goals_ma5'] = last_5_away['FTAG'].mean()\n",
    "            df_basic.loc[idx, 'away_conceded_ma5'] = last_5_away['FTHG'].mean()\n",
    "\n",
    "# Filling NA with overall means\n",
    "df_basic['home_goals_ma5'].fillna(df_basic['FTHG'].mean(), inplace=True)\n",
    "df_basic['home_conceded_ma5'].fillna(df_basic['FTAG'].mean(), inplace=True)\n",
    "df_basic['away_goals_ma5'].fillna(df_basic['FTAG'].mean(), inplace=True)\n",
    "df_basic['away_conceded_ma5'].fillna(df_basic['FTHG'].mean(), inplace=True)\n",
    "\n",
    "print(f\"Home goals MA5 - mean: {df_basic['home_goals_ma5'].mean():.2f}\")\n",
    "print(f\"Away goals MA5 - mean: {df_basic['away_goals_ma5'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Seems fine too, averages for first matches were replaced with overall means**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Promoted/Demoted\n",
    "\n",
    "**Detect teams that changed leagues between seasons using the Season column (e.g., 2019/2020 → 2020/2021).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# promotion/demotion flags\n",
    "df_basic['home_promoted'] = 0\n",
    "df_basic['home_demoted'] = 0\n",
    "df_basic['away_promoted'] = 0\n",
    "df_basic['away_demoted'] = 0\n",
    "\n",
    "# For each team, check if they changed tier between seasons\n",
    "for team in df_basic['HomeTeam'].unique():\n",
    "    team_data = df_basic[(df_basic['HomeTeam'] == team) | (df_basic['AwayTeam'] == team)].sort_values('Date')\n",
    "\n",
    "    # Grouping by season and get the league tier for each season\n",
    "    season_tiers = team_data.groupby('Season')['league_tier'].first()\n",
    "\n",
    "    for i in range(1, len(season_tiers)):\n",
    "        season = season_tiers.index[i]\n",
    "        prev_tier = season_tiers.iloc[i-1]\n",
    "        curr_tier = season_tiers.iloc[i]\n",
    "\n",
    "        if curr_tier < prev_tier:  # Lower tier number = higher division\n",
    "            promoted = 1\n",
    "            demoted = 0\n",
    "        elif curr_tier > prev_tier:\n",
    "            promoted = 0\n",
    "            demoted = 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        season_mask = (df_basic['Season'] == season)\n",
    "        home_mask = season_mask & (df_basic['HomeTeam'] == team)\n",
    "        away_mask = season_mask & (df_basic['AwayTeam'] == team)\n",
    "\n",
    "        df_basic.loc[home_mask, 'home_promoted'] = promoted\n",
    "        df_basic.loc[home_mask, 'home_demoted'] = demoted\n",
    "        df_basic.loc[away_mask, 'away_promoted'] = promoted\n",
    "        df_basic.loc[away_mask, 'away_demoted'] = demoted\n",
    "\n",
    "print(f\"Home teams promoted: {df_basic['home_promoted'].sum()}\")\n",
    "print(f\"Home teams demoted: {df_basic['home_demoted'].sum()}\")\n",
    "print(f\"Away teams promoted: {df_basic['away_promoted'].sum()}\")\n",
    "print(f\"Away teams demoted: {df_basic['away_demoted'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Promotion/demotion detection works correctly by comparing league tiers across consecutive seasons (not calendar years).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4 Historical Season Positions\n",
    "\n",
    "**Calculate where each team finished in previous seasons.**\n",
    "\n",
    "For each match, we want to know the league position teams achieved in past seasons. This provides context about team strength and recent trajectory.\n",
    "\n",
    "**Methodology:**\n",
    "- Calculate final standings for each season/division based on points (Win=3, Draw=1, Loss=0)\n",
    "- Use goal difference as tiebreaker\n",
    "- Create lookback features for all previous seasons\n",
    "- NaN for first season (no historical data)\n",
    "- Include positions even when teams were promoted/demoted (shows relative strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final league standings for each season/division\n",
    "def calculate_season_standings(df):\n",
    "    \"\"\"Calculate final league positions for each team in each season/division.\"\"\"\n",
    "    standings_list = []\n",
    "\n",
    "    for (season, div), group in df.groupby(['Season', 'Div']):\n",
    "        team_stats = {}\n",
    "\n",
    "        # Process each match\n",
    "        for _, match in group.iterrows():\n",
    "            home_team, away_team = match['HomeTeam'], match['AwayTeam']\n",
    "            home_goals, away_goals = match['FTHG'], match['FTAG']\n",
    "\n",
    "            # Initialize teams\n",
    "            for team in [home_team, away_team]:\n",
    "                if team not in team_stats:\n",
    "                    team_stats[team] = {'points': 0, 'gf': 0, 'ga': 0, 'matches': 0}\n",
    "\n",
    "            # Update goals and matches\n",
    "            team_stats[home_team]['gf'] += home_goals\n",
    "            team_stats[home_team]['ga'] += away_goals\n",
    "            team_stats[away_team]['gf'] += away_goals\n",
    "            team_stats[away_team]['ga'] += home_goals\n",
    "            team_stats[home_team]['matches'] += 1\n",
    "            team_stats[away_team]['matches'] += 1\n",
    "\n",
    "            # Assign points\n",
    "            if match['FTR'] == 'H':\n",
    "                team_stats[home_team]['points'] += 3\n",
    "            elif match['FTR'] == 'A':\n",
    "                team_stats[away_team]['points'] += 3\n",
    "            else:  # Draw\n",
    "                team_stats[home_team]['points'] += 1\n",
    "                team_stats[away_team]['points'] += 1\n",
    "\n",
    "        # Create standings dataframe\n",
    "        season_df = pd.DataFrame([\n",
    "            {\n",
    "                'Season': season, 'Div': div, 'Team': team,\n",
    "                'Points': stats['points'], 'Goals_For': stats['gf'],\n",
    "                'Goals_Against': stats['ga'], 'Goal_Diff': stats['gf'] - stats['ga'],\n",
    "                'Matches': stats['matches']\n",
    "            }\n",
    "            for team, stats in team_stats.items()\n",
    "        ])\n",
    "\n",
    "        # Sort and assign positions\n",
    "        season_df = season_df.sort_values(\n",
    "            by=['Points', 'Goal_Diff', 'Goals_For'],\n",
    "            ascending=[False, False, False]\n",
    "        ).reset_index(drop=True)\n",
    "        season_df['Position'] = range(1, len(season_df) + 1)\n",
    "\n",
    "        standings_list.append(season_df)\n",
    "\n",
    "    return pd.concat(standings_list, ignore_index=True)\n",
    "\n",
    "# Calculate standings\n",
    "df_season_standings = calculate_season_standings(df_basic)\n",
    "print(f\"✓ Season standings: {len(df_season_standings)} team-season records across {df_season_standings['Season'].nunique()} seasons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add goal-scoring pattern statistics to standings\n",
    "# Calculate % of matches with 2+ and 3+ goals (scored/conceded/total)\n",
    "# Only keep 2+ and 3+ as they're most relevant for Over/Under 2.5 prediction\n",
    "temp_counts = {f'{p}_{t}plus': [0] * len(df_season_standings)\n",
    "               for p in ['scored', 'conceded', 'total'] for t in [2, 3]}\n",
    "\n",
    "for (season, div), group in df_basic.groupby(['Season', 'Div']):\n",
    "    for _, match in group.iterrows():\n",
    "        total_goals = match['FTHG'] + match['FTAG']\n",
    "\n",
    "        for team, goals_for, goals_against in [\n",
    "            (match['HomeTeam'], match['FTHG'], match['FTAG']),\n",
    "            (match['AwayTeam'], match['FTAG'], match['FTHG'])\n",
    "        ]:\n",
    "            team_idx = df_season_standings[\n",
    "                (df_season_standings['Season'] == season) &\n",
    "                (df_season_standings['Div'] == div) &\n",
    "                (df_season_standings['Team'] == team)\n",
    "            ].index\n",
    "\n",
    "            if len(team_idx) > 0:\n",
    "                idx = team_idx[0]\n",
    "                for t in [2, 3]:\n",
    "                    if goals_for >= t:\n",
    "                        temp_counts[f'scored_{t}plus'][idx] += 1\n",
    "                    if goals_against >= t:\n",
    "                        temp_counts[f'conceded_{t}plus'][idx] += 1\n",
    "                    if total_goals >= t:\n",
    "                        temp_counts[f'total_{t}plus'][idx] += 1\n",
    "\n",
    "# Convert counts to percentages\n",
    "for threshold in [2, 3]:\n",
    "    for prefix in ['scored', 'conceded', 'total']:\n",
    "        key = f'{prefix}_{threshold}plus'\n",
    "        df_season_standings[f'{key}_pct'] = (\n",
    "            pd.Series(temp_counts[key]) / df_season_standings['Matches'] * 100\n",
    "        ).round(1)\n",
    "\n",
    "print(f\"✓ Goal pattern statistics added: 6 percentage columns (scored/conceded/total for 2+ and 3+ goals)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create historical position features with lookback logic\n",
    "season_list = sorted(df_basic['Season'].unique())\n",
    "season_to_order = {season: idx for idx, season in enumerate(season_list)}\n",
    "df_basic['season_order'] = df_basic['Season'].map(season_to_order)\n",
    "\n",
    "# Extract season years and create position lookup\n",
    "def extract_season_year(season_str):\n",
    "    return int(season_str.split('/')[0])\n",
    "\n",
    "df_season_standings['season_year'] = df_season_standings['Season'].apply(extract_season_year)\n",
    "unique_season_years = sorted(df_season_standings['season_year'].unique())\n",
    "\n",
    "# Add percentile rankings for cross-league comparability\n",
    "df_season_standings['league_size'] = df_season_standings.groupby(['Season', 'Div'])['Position'].transform('max')\n",
    "df_season_standings['Position_Percentile'] = (\n",
    "    (df_season_standings['league_size'] - df_season_standings['Position'] + 1) /\n",
    "    df_season_standings['league_size'] * 100\n",
    ").round(2)\n",
    "percentile_lookup = df_season_standings.set_index(['Season', 'Div', 'Team'])['Position_Percentile'].to_dict()\n",
    "\n",
    "# Create columns for percentile positions only (not raw positions)\n",
    "for year in unique_season_years:\n",
    "    df_basic[f'home_position_pct_{year}'] = np.nan\n",
    "    df_basic[f'away_position_pct_{year}'] = np.nan\n",
    "\n",
    "# Populate historical features (only look backward, never current/future seasons)\n",
    "for idx, row in df_basic.iterrows():\n",
    "    current_season_order = row['season_order']\n",
    "\n",
    "    for year in unique_season_years:\n",
    "        target_season = next((s for s in season_list if extract_season_year(s) == year), None)\n",
    "        if not target_season or season_to_order[target_season] >= current_season_order:\n",
    "            continue  # Skip if no season found or not in the past\n",
    "\n",
    "        for team_type, team in [('home', row['HomeTeam']), ('away', row['AwayTeam'])]:\n",
    "            # Find team in any division during target season\n",
    "            key = next(\n",
    "                ((target_season, div, team) for div in df_season_standings['Div'].unique()\n",
    "                 if (target_season, div, team) in percentile_lookup),\n",
    "                None\n",
    "            )\n",
    "\n",
    "            if key:\n",
    "                df_basic.loc[idx, f'{team_type}_position_pct_{year}'] = percentile_lookup[key]\n",
    "\n",
    "position_cols = [col for col in df_basic.columns if 'position_pct_' in col]\n",
    "print(f\"✓ Historical position features: {len(position_cols)} columns (percentiles for {len(unique_season_years)} seasons)\")\n",
    "print(f\"  Years covered: {unique_season_years}\")\n",
    "print(f\"  Percentile scale: 0-100 (higher = better position, normalized across league sizes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add historical goal-scoring pattern features to df_basic\n",
    "# Create lookups for goal statistics\n",
    "goal_stat_lookups = {}\n",
    "for threshold in [2, 3]:\n",
    "    for prefix in ['scored', 'conceded', 'total']:\n",
    "        col_name = f'{prefix}_{threshold}plus_pct'\n",
    "        goal_stat_lookups[col_name] = df_season_standings.set_index(['Season', 'Div', 'Team'])[col_name].to_dict()\n",
    "\n",
    "# Create columns for goal statistics\n",
    "for year in unique_season_years:\n",
    "    for threshold in [2, 3]:\n",
    "        for prefix in ['scored', 'conceded', 'total']:\n",
    "            df_basic[f'home_{prefix}_{threshold}plus_pct_{year}'] = np.nan\n",
    "            df_basic[f'away_{prefix}_{threshold}plus_pct_{year}'] = np.nan\n",
    "\n",
    "# Populate goal statistics (only look backward, never current/future seasons)\n",
    "for idx, row in df_basic.iterrows():\n",
    "    current_season_order = row['season_order']\n",
    "\n",
    "    for year in unique_season_years:\n",
    "        target_season = next((s for s in season_list if extract_season_year(s) == year), None)\n",
    "        if not target_season or season_to_order[target_season] >= current_season_order:\n",
    "            continue  # Skip if no season found or not in the past\n",
    "\n",
    "        for team_type, team in [('home', row['HomeTeam']), ('away', row['AwayTeam'])]:\n",
    "            # Find team in any division during target season\n",
    "            key = next(\n",
    "                ((target_season, div, team) for div in df_season_standings['Div'].unique()\n",
    "                 if (target_season, div, team) in percentile_lookup),\n",
    "                None\n",
    "            )\n",
    "\n",
    "            if key:\n",
    "                # Add all goal statistics for this team/season\n",
    "                for threshold in [2, 3]:\n",
    "                    for prefix in ['scored', 'conceded', 'total']:\n",
    "                        col_name = f'{prefix}_{threshold}plus_pct'\n",
    "                        stat_value = goal_stat_lookups[col_name].get(key)\n",
    "                        if stat_value is not None:\n",
    "                            df_basic.loc[idx, f'{team_type}_{col_name}_{year}'] = stat_value\n",
    "\n",
    "goal_stat_cols = [col for col in df_basic.columns if any(f'{p}_{t}plus_pct_' in col for p in ['scored', 'conceded', 'total'] for t in [2, 3])]\n",
    "print(f\"✓ Historical goal statistics features: {len(goal_stat_cols)} columns (scored/conceded/total 2+,3+ for {len(unique_season_years)} seasons)\")\n",
    "print(f\"  Format: home/away_[scored/conceded/total]_[2/3]plus_pct_[year]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of Section 4.3.4:**\n",
    "\n",
    "We've created comprehensive historical performance features:\n",
    "\n",
    "1. **Season Standings Table** (`df_season_standings`): 2,264 team-season records containing:\n",
    "   - Final league positions based on points (Win=3, Draw=1, Loss=0)\n",
    "   - Goal statistics (For, Against, Difference)\n",
    "   - Goal-scoring patterns: % of matches with 2+ and 3+ goals (scored/conceded/total) = 6 features\n",
    "\n",
    "2. **Historical Position Features** (added to `df_basic`): 12 columns\n",
    "   - **Percentile rankings** (0-100): Normalized positions for cross-league comparison\n",
    "   - Format: `home_position_pct_YYYY` and `away_position_pct_YYYY` for 6 historical seasons\n",
    "   - Higher percentile = better position (e.g., 95th percentile = near top of table)\n",
    "   - **Why percentiles only?** Standardized scale across different league sizes (18 vs 20 teams), avoids multicollinearity with raw positions\n",
    "\n",
    "3. **Historical Goal Pattern Features** (added to `df_basic`): 72 columns\n",
    "   - For each team (home/away) and season (6 years): 6 statistics\n",
    "   - `scored_2plus_pct`: % of matches where team scored 2+ goals\n",
    "   - `scored_3plus_pct`: % of matches where team scored 3+ goals\n",
    "   - `conceded_2plus_pct`: % of matches where team conceded 2+ goals\n",
    "   - `conceded_3plus_pct`: % of matches where team conceded 3+ goals\n",
    "   - `total_2plus_pct`: % of matches with 2+ total goals\n",
    "   - `total_3plus_pct`: % of matches with 3+ total goals\n",
    "\n",
    "**Key Design Choices:**\n",
    "- Lookback logic prevents data leakage (features only reference past seasons)\n",
    "- Percentiles enable fair comparison between leagues of different sizes (18 vs 20 teams)\n",
    "- 2+ and 3+ thresholds chosen for relevance to Over/Under 2.5 target (exclude 1+, 4+, 5+ to reduce dimensionality)\n",
    "- \"Total\" goals included as direct signal for high-scoring match tendency\n",
    "- NaN values for teams with no historical data (first season or promoted from outside dataset)\n",
    "- Positions from all divisions included to capture relative team strength\n",
    "\n",
    "**Total features added: 84** (12 position percentiles + 72 goal patterns)\n",
    "\n",
    "**Expected Impact:** Historical percentile positions and goal patterns should provide strong predictive signals about team quality and offensive/defensive tendencies, particularly useful for Over/Under 2.5 goals classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.5 Additional Derived Features\n",
    "\n",
    "**Create additional features from existing data that might improve Over/Under 2.5 prediction:**\n",
    "\n",
    "**Team strength differential features:**\n",
    "- Position percentile difference between home and away team (quality gap, normalized across leagues)\n",
    "- Combined attacking/defensive strength indicators (blend recent form + historical patterns)\n",
    "\n",
    "**Form-based features:**\n",
    "- Recent form trend (improving vs declining over last 10 matches)\n",
    "\n",
    "**Contextual features:**\n",
    "- Season progress (early, mid, late season effects)\n",
    "- Days rest advantage (difference in rest days between teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Position difference features (most recent season available for both teams)\n",
    "df_basic['position_pct_diff'] = np.nan\n",
    "\n",
    "for idx, row in df_basic.iterrows():\n",
    "    # Find most recent season where both teams have position data\n",
    "    for year in reversed(unique_season_years):\n",
    "        home_pct_col = f'home_position_pct_{year}'\n",
    "        away_pct_col = f'away_position_pct_{year}'\n",
    "\n",
    "        if (pd.notna(df_basic.loc[idx, home_pct_col]) and\n",
    "            pd.notna(df_basic.loc[idx, away_pct_col])):\n",
    "            # Positive = home team has better position (higher percentile)\n",
    "            df_basic.loc[idx, 'position_pct_diff'] = df_basic.loc[idx, home_pct_col] - df_basic.loc[idx, away_pct_col]\n",
    "            break\n",
    "\n",
    "print(f\"✓ Position percentile difference feature created\")\n",
    "print(f\"  Coverage: {(df_basic['position_pct_diff'].notna().sum() / len(df_basic) * 100):.1f}%\")\n",
    "\n",
    "# 2. Combined attacking strength (recent goals MA + historical scoring patterns)\n",
    "df_basic['combined_attack_strength'] = np.nan\n",
    "df_basic['combined_defense_weakness'] = np.nan\n",
    "\n",
    "for idx, row in df_basic.iterrows():\n",
    "    # Get most recent goal statistics\n",
    "    for year in reversed(unique_season_years):\n",
    "        home_scored = df_basic.loc[idx, f'home_scored_3plus_pct_{year}']\n",
    "        away_scored = df_basic.loc[idx, f'away_scored_3plus_pct_{year}']\n",
    "        home_conceded = df_basic.loc[idx, f'home_conceded_3plus_pct_{year}']\n",
    "        away_conceded = df_basic.loc[idx, f'away_conceded_3plus_pct_{year}']\n",
    "\n",
    "        if pd.notna(home_scored) and pd.notna(away_scored):\n",
    "            # Combine recent MA with historical patterns\n",
    "            home_recent = df_basic.loc[idx, 'home_goals_ma5']\n",
    "            away_recent = df_basic.loc[idx, 'away_goals_ma5']\n",
    "\n",
    "            # Weighted average: 60% recent form, 40% historical pattern\n",
    "            df_basic.loc[idx, 'combined_attack_strength'] = (\n",
    "                0.6 * (home_recent + away_recent) +\n",
    "                0.4 * ((home_scored + away_scored) / 20)  # Normalize percentage to goals scale\n",
    "            )\n",
    "            df_basic.loc[idx, 'combined_defense_weakness'] = (\n",
    "                0.6 * (df_basic.loc[idx, 'home_conceded_ma5'] + df_basic.loc[idx, 'away_conceded_ma5']) +\n",
    "                0.4 * ((home_conceded + away_conceded) / 20)\n",
    "            )\n",
    "            break\n",
    "\n",
    "print(f\"✓ Combined strength features created\")\n",
    "print(f\"  Coverage: {(df_basic['combined_attack_strength'].notna().sum() / len(df_basic) * 100):.1f}%\")\n",
    "\n",
    "# 3. Form trend (improving vs declining)\n",
    "df_basic['home_form_trend'] = np.nan\n",
    "df_basic['away_form_trend'] = np.nan\n",
    "\n",
    "for team in df_basic['HomeTeam'].unique():\n",
    "    # Home matches\n",
    "    home_mask = df_basic['HomeTeam'] == team\n",
    "    home_dates = df_basic[home_mask].sort_values('Date').index\n",
    "    for i, idx in enumerate(home_dates):\n",
    "        if i >= 10:  # Need at least 10 matches\n",
    "            last_5 = df_basic.loc[home_dates[i-5:i], 'FTHG'].mean()\n",
    "            prev_5 = df_basic.loc[home_dates[i-10:i-5], 'FTHG'].mean()\n",
    "            df_basic.loc[idx, 'home_form_trend'] = last_5 - prev_5  # Positive = improving\n",
    "\n",
    "    # Away matches\n",
    "    away_mask = df_basic['AwayTeam'] == team\n",
    "    away_dates = df_basic[away_mask].sort_values('Date').index\n",
    "    for i, idx in enumerate(away_dates):\n",
    "        if i >= 10:\n",
    "            last_5 = df_basic.loc[away_dates[i-5:i], 'FTAG'].mean()\n",
    "            prev_5 = df_basic.loc[away_dates[i-10:i-5], 'FTAG'].mean()\n",
    "            df_basic.loc[idx, 'away_form_trend'] = last_5 - prev_5\n",
    "\n",
    "# Fill NaN with 0 (no trend info = assume stable)\n",
    "df_basic['home_form_trend'].fillna(0, inplace=True)\n",
    "df_basic['away_form_trend'].fillna(0, inplace=True)\n",
    "\n",
    "print(f\"✓ Form trend features created\")\n",
    "\n",
    "# 4. Rest days advantage\n",
    "df_basic['rest_days_advantage'] = df_basic['home_days_since_last'] - df_basic['away_days_since_last']\n",
    "\n",
    "print(f\"✓ Rest days advantage created\")\n",
    "\n",
    "# 5. Season progress (match number / total matches in season)\n",
    "df_basic['season_progress'] = np.nan\n",
    "for season_div, group in df_basic.groupby(['Season', 'Div']):\n",
    "    total_matches = len(group)\n",
    "    for i, idx in enumerate(group.index):\n",
    "        df_basic.loc[idx, 'season_progress'] = (i + 1) / total_matches\n",
    "\n",
    "print(f\"✓ Season progress feature created\")\n",
    "\n",
    "# Summary\n",
    "new_derived_features = ['position_pct_diff', 'combined_attack_strength',\n",
    "                        'combined_defense_weakness', 'home_form_trend', 'away_form_trend',\n",
    "                        'rest_days_advantage', 'season_progress']\n",
    "print(f\"\\n✓ Total new derived features: {len(new_derived_features)}\")\n",
    "print(f\"  Features: {new_derived_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.X Feature Importance Testing\n",
    "\n",
    "**Test which features matter for predicting Over/Under 2.5 goals**\n",
    "\n",
    "**Correlation Analysis:**\n",
    "- Quick initial screening of linear relationships\n",
    "- Identifies features with direct linear impact on target\n",
    "- Visualize top correlated features by group (time-based, position, goal patterns)\n",
    "- **Limitation:** Only captures linear relationships, misses non-linear patterns and interactions\n",
    "\n",
    "**Note:** While correlations are weak, features may still be valuable in tree-based models that capture non-linear patterns, thresholds, and feature interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Correlation Analysis\n",
    "\n",
    "**Quick screening of linear relationships with Over/Under 2.5 target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features to test\n",
    "num_features = ['home_days_since_last', 'away_days_since_last',\n",
    "                'home_goals_ma5', 'home_conceded_ma5', 'away_goals_ma5', 'away_conceded_ma5',\n",
    "                'league_tier', 'month']\n",
    "\n",
    "# Add historical position features (from section 4.3.4) - percentiles only\n",
    "position_features = [col for col in df_basic.columns if 'position_pct_' in col]\n",
    "\n",
    "# Add historical goal statistics features (from section 4.3.4)\n",
    "goal_stat_features = [col for col in df_basic.columns if any(f'{p}_{t}plus_pct_' in col for p in ['scored', 'conceded', 'total'] for t in [2, 3])]\n",
    "\n",
    "all_num_features = num_features + position_features + goal_stat_features\n",
    "\n",
    "# Calculate correlations\n",
    "print(\"=\" * 60)\n",
    "print(\"CORRELATIONS WITH over_2_5 TARGET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. TIME-BASED FEATURES:\")\n",
    "for feat in num_features:\n",
    "    corr = df_basic[feat].corr(df_basic['over_2_5'])\n",
    "    print(f\"  {feat:30s}: {corr:7.4f}\")\n",
    "\n",
    "print(f\"\\n2. HISTORICAL POSITION FEATURES ({len(position_features)} total):\")\n",
    "# Group by season year for cleaner display\n",
    "position_corrs = []\n",
    "for feat in position_features:\n",
    "    corr = df_basic[feat].corr(df_basic['over_2_5'])\n",
    "    position_corrs.append((feat, corr))\n",
    "    print(f\"  {feat:30s}: {corr:7.4f}\")\n",
    "\n",
    "# Summary statistics for position features\n",
    "position_corr_values = [abs(c[1]) for c in position_corrs if not pd.isna(c[1])]\n",
    "if position_corr_values:\n",
    "    print(f\"\\n  Position features summary:\")\n",
    "    print(f\"    Max |correlation|: {max(position_corr_values):.4f}\")\n",
    "    print(f\"    Mean |correlation|: {np.mean(position_corr_values):.4f}\")\n",
    "    print(f\"    Median |correlation|: {np.median(position_corr_values):.4f}\")\n",
    "\n",
    "print(f\"\\n3. HISTORICAL GOAL PATTERN FEATURES ({len(goal_stat_features)} total):\")\n",
    "goal_stat_corrs = []\n",
    "for feat in goal_stat_features:\n",
    "    corr = df_basic[feat].corr(df_basic['over_2_5'])\n",
    "    goal_stat_corrs.append((feat, corr))\n",
    "    print(f\"  {feat:40s}: {corr:7.4f}\")\n",
    "\n",
    "# Summary statistics for goal pattern features\n",
    "goal_stat_corr_values = [abs(c[1]) for c in goal_stat_corrs if not pd.isna(c[1])]\n",
    "if goal_stat_corr_values:\n",
    "    print(f\"\\n  Goal pattern features summary:\")\n",
    "    print(f\"    Max |correlation|: {max(goal_stat_corr_values):.4f}\")\n",
    "    print(f\"    Mean |correlation|: {np.mean(goal_stat_corr_values):.4f}\")\n",
    "    print(f\"    Median |correlation|: {np.median(goal_stat_corr_values):.4f}\")\n",
    "\n",
    "    # Find strongest correlations\n",
    "    top_5 = sorted(goal_stat_corrs, key=lambda x: abs(x[1]) if not pd.isna(x[1]) else 0, reverse=True)[:5]\n",
    "    print(f\"\\n  Top 5 goal pattern features:\")\n",
    "    for feat, corr in top_5:\n",
    "        print(f\"    {feat:40s}: {corr:7.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 1. Time-based features\n",
    "time_corrs = [df_basic[f].corr(df_basic['over_2_5']) for f in num_features]\n",
    "axes[0].barh(num_features, time_corrs, color='steelblue')\n",
    "axes[0].axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "axes[0].set_xlabel('Correlation with Over 2.5', fontsize=11)\n",
    "axes[0].set_title('Time-Based Features', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Position features - show top 10 by absolute correlation\n",
    "position_corr_df = pd.DataFrame(position_corrs, columns=['feature', 'correlation'])\n",
    "position_corr_df['abs_corr'] = position_corr_df['correlation'].abs()\n",
    "top_position = position_corr_df.nlargest(10, 'abs_corr')\n",
    "axes[1].barh(range(len(top_position)), top_position['correlation'].values, color='coral')\n",
    "axes[1].set_yticks(range(len(top_position)))\n",
    "axes[1].set_yticklabels([f.replace('home_', 'H_').replace('away_', 'A_').replace('position_', 'pos_').replace('_pct', '%') for f in top_position['feature']], fontsize=8)\n",
    "axes[1].axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "axes[1].set_xlabel('Correlation with Over 2.5', fontsize=11)\n",
    "axes[1].set_title(f'Top 10 Position Features (of {len(position_features)})', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Goal stats - show top 10 by absolute correlation\n",
    "goal_corr_df = pd.DataFrame(goal_stat_corrs, columns=['feature', 'correlation'])\n",
    "goal_corr_df['abs_corr'] = goal_corr_df['correlation'].abs()\n",
    "top_goals = goal_corr_df.nlargest(10, 'abs_corr')\n",
    "axes[2].barh(range(len(top_goals)), top_goals['correlation'].values, color='mediumseagreen')\n",
    "axes[2].set_yticks(range(len(top_goals)))\n",
    "axes[2].set_yticklabels([f.replace('home_', 'H_').replace('away_', 'A_').replace('_pct_season_', '_S') for f in top_goals['feature']], fontsize=8)\n",
    "axes[2].axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "axes[2].set_xlabel('Correlation with Over 2.5', fontsize=11)\n",
    "axes[2].set_title(f'Top 10 Goal Pattern Features (of {len(goal_stat_features)})', fontsize=12, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CORRELATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Overall weak correlations suggest non-linear relationships are important.\")\n",
    "print(f\"Tree-based models will likely perform better than linear models.\")\n",
    "print(f\"\\nStrongest absolute correlations:\")\n",
    "all_corrs = pd.concat([\n",
    "    pd.DataFrame({'feature': num_features, 'correlation': time_corrs}),\n",
    "    position_corr_df[['feature', 'correlation']],\n",
    "    goal_corr_df[['feature', 'correlation']]\n",
    "])\n",
    "all_corrs['abs_corr'] = all_corrs['correlation'].abs()\n",
    "top_overall = all_corrs.nlargest(5, 'abs_corr')\n",
    "for _, row in top_overall.iterrows():\n",
    "    print(f\"  {row['feature']:50s}: {row['correlation']:7.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of Correlation Analysis**\n",
    "\n",
    "**Overall Finding:** All features show **very weak correlations** with the Over/Under 2.5 target (|r| < 0.10), indicating minimal linear relationships.\n",
    "\n",
    "**1. Time-Based Features:**\n",
    "- **Days since last match** (~-0.01): Virtually no effect — rest periods don't linearly predict goal totals\n",
    "- **Home goals MA5** (+0.074): Strongest time-based correlation — teams scoring recently tend toward slightly higher Over 2.5 rates\n",
    "- **Away goals MA5** (+0.048): Weak positive signal for away team scoring form\n",
    "- **Home/Away conceded MA5** (~-0.04 to -0.05): Slight negative — defensive stability correlates with fewer goals\n",
    "- **League tier** (-0.042): Lower divisions marginally less likely to see 3+ goals\n",
    "- **Month** (+0.019): Negligible seasonal effect\n",
    "\n",
    "**2. Historical Position Features (12 total - percentiles only):**\n",
    "- **Summary statistics:**\n",
    "  - Max |correlation|: ~0.04-0.06 (very weak)\n",
    "  - Mean |correlation|: ~0.02-0.03\n",
    "  - Median |correlation|: ~0.02\n",
    "- **Top correlations:** Home/away percentile positions from recent seasons (S-1, S-2) show slightly stronger effects\n",
    "- **Insight:** Position matters, but not in a simple linear way — likely interacts with other factors (e.g., promoted teams behave differently than established teams at same position)\n",
    "- **Scale:** 0-100 percentile where higher = better position (normalized across different league sizes)\n",
    "\n",
    "**3. Historical Goal Pattern Features (72 total):**\n",
    "- **Summary statistics:**\n",
    "  - Max |correlation|: ~0.05-0.07\n",
    "  - Mean |correlation|: ~0.02-0.03\n",
    "  - Top features: Typically `total_2plus_pct` or `scored_2plus_pct` from recent seasons (S-1, S-2)\n",
    "- **Insight:** Teams with history of high-scoring matches continue that pattern, but effect is weak linearly\n",
    "- **Strongest patterns:** Home team total goals 2+ percentages from recent seasons show positive correlations (0.04-0.07)\n",
    "\n",
    "**4. Key Takeaways:**\n",
    "- ✅ **Weak linear relationships** suggest soccer is governed by **non-linear patterns and interactions**\n",
    "- ✅ **Recent form** (MA5 goals) shows slightly stronger signal than historical season data in linear terms\n",
    "- ✅ **Goal patterns persist** — teams with high-scoring history continue that tendency, but modestly\n",
    "- ✅ **Percentile positions** provide normalized strength indicator across different league sizes\n",
    "- ⚠️ **Modeling implication:** Linear models (e.g., logistic regression) will struggle. Tree-based models (Random Forest, XGBoost) better suited to capture:\n",
    "  - Non-linear thresholds (e.g., optimal rest days 4-6, not linear)\n",
    "  - Interactions (e.g., promoted + low position ≠ stable low position)\n",
    "  - Context-dependent effects (e.g., position matters more in competitive leagues)\n",
    "\n",
    "**Conclusion:** While individual correlations are weak, these features will likely contribute to model performance through interactions and non-linear patterns when combined in ensemble models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Summary: What We Added**\n",
    "\n",
    "**New Features (Section 4.3.5):**\n",
    "1. **Position percentile difference** - Quality gap between teams using normalized percentiles (higher = home team stronger)\n",
    "2. **Combined attack/defense strength** - Blend recent form (60%) + historical patterns (40%)\n",
    "3. **Form trend** - Improving vs declining (last 5 vs previous 5 matches)\n",
    "4. **Rest days advantage** - Difference in recovery time\n",
    "5. **Season progress** - Early/mid/late season effects (0.0 to 1.0)\n",
    "\n",
    "**Total: 7 derived features** (reduced from 8 by using only percentile difference)\n",
    "\n",
    "**Better Feature Testing (Section 4.3.X):**\n",
    "- ✅ **Correlation Analysis** - Fast initial screening, shows linear relationships only\n",
    "- ✅ **Correlation Visualization** - Bar charts showing top correlated features by group\n",
    "\n",
    "**Key Insight:** Weak correlations suggest non-linear patterns dominate in soccer:\n",
    "- Goal-scoring has complex dynamics (e.g., very weak teams score less, but so do very defensive teams)\n",
    "- Features likely interact (promoted teams + low position ≠ stable low position)\n",
    "- Thresholds and context matter (rest days, form trends, league tier effects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of Numerical Features**\n",
    "\n",
    "All numerical features show **very weak correlations** (|r| < 0.08) with the Over/Under 2.5 goals target, suggesting minimal linear dependence.  \n",
    "**Feature-wise summary:**\n",
    "- **Days since last match** (-0.010, -0.009):  \n",
    "  Essentially no relationship — rest periods have no meaningful effect on goal totals.\n",
    "- **Home goals MA5** (0.074):  \n",
    "  Slight positive signal — teams scoring more in recent matches tend to have marginally higher Over 2.5 rates.\n",
    "- **Away goals MA5** (0.048):  \n",
    "  Small negative correlation — possibly due to opponents adapting defensively to strong away attacks.\n",
    "- **League tier** (-0.042):  \n",
    "  Strongest (yet still weak) correlation — lower divisions show a slightly lower frequency of high-scoring games.\n",
    "- **Month** (0.019):  \n",
    "  Negligible seasonal influence on goal totals.\n",
    "**Modeling note:**  \n",
    "\n",
    "Although weak on their own, these variables may still provide value to **non-linear or ensemble models** by capturing interaction effects and subtle contextual patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test boolean/categorical features against the target.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean features - automatically include all that exist in df_basic\n",
    "# Define expected boolean features (add new ones here as you create them)\n",
    "expected_bool_features = ['is_weekend', 'home_promoted', 'home_demoted',\n",
    "                          'away_promoted', 'away_demoted']\n",
    "bool_features = [feat for feat in expected_bool_features if feat in df_basic.columns]\n",
    "\n",
    "print(f\"Analyzing {len(bool_features)} boolean/categorical features:\")\n",
    "print(f\"  Features: {bool_features}\\n\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Over 2.5 rate by categorical feature:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for feat in bool_features:\n",
    "    grouped = df_basic.groupby(feat)['over_2_5'].agg(['mean', 'count'])\n",
    "    print(f\"\\n{feat}:\")\n",
    "    print(grouped)\n",
    "\n",
    "    # Chi-squared test\n",
    "    contingency = pd.crosstab(df_basic[feat], df_basic['over_2_5'])\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "    print(f\"Chi-squared p-value: {p:.4f}\")\n",
    "\n",
    "# Visualize - dynamically create subplots based on number of features\n",
    "n_features = min(len(bool_features), 5)  # Limit to 5 for readability\n",
    "fig, axes = plt.subplots(1, n_features, figsize=(5 * n_features, 4))\n",
    "if n_features == 1:\n",
    "    axes = [axes]  # Make it iterable\n",
    "\n",
    "# Configuration for feature visualization\n",
    "feature_configs = {\n",
    "    'is_weekend': {'labels': ['Weekday', 'Weekend'], 'title': 'Over 2.5 Rate: Weekend vs Weekday'},\n",
    "    'home_promoted': {'labels': ['Regular', 'Promoted'], 'title': 'Over 2.5 Rate: Home Team Promoted'},\n",
    "    'home_demoted': {'labels': ['Regular', 'Demoted'], 'title': 'Over 2.5 Rate: Home Team Demoted'},\n",
    "    'away_promoted': {'labels': ['Regular', 'Promoted'], 'title': 'Over 2.5 Rate: Away Team Promoted'},\n",
    "    'away_demoted': {'labels': ['Regular', 'Demoted'], 'title': 'Over 2.5 Rate: Away Team Demoted'}\n",
    "}\n",
    "\n",
    "for idx, feat in enumerate(bool_features[:n_features]):\n",
    "    rates = df_basic.groupby(feat)['over_2_5'].mean()\n",
    "    config = feature_configs.get(feat, {'labels': ['False', 'True'], 'title': f'Over 2.5 Rate: {feat}'})\n",
    "\n",
    "    axes[idx].bar(config['labels'], rates.values, color=['steelblue', 'coral'])\n",
    "    axes[idx].set_title(config['title'], fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Over 2.5 Rate')\n",
    "    axes[idx].set_ylim([0.45, 0.55])  # Zoom in on the relevant range\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(rates.values):\n",
    "        axes[idx].text(i, v + 0.002, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of Boolean/Categorical Features:**\n",
    "\n",
    "Chi-squared tests reveal the following significance levels for `{len(bool_features)}` boolean features analyzed:\n",
    "\n",
    "**Statistically Significant (α=0.05):**\n",
    "- **Weekend vs Weekday** (p=0.0012): Weekend matches show notably higher Over 2.5 rate (50.5% vs 48.7% weekday). This is the strongest categorical predictor, suggesting weekend scheduling may influence match dynamics - possibly due to fan attendance, player rest, or tactical approaches.\n",
    "\n",
    "- **Away Team Demoted** (p=0.0121): Away teams that were demoted show lower Over 2.5 rate (47.6% vs 50.1% regular teams). The 2.5 percentage point difference suggests defensive strategies or morale issues affecting demoted teams playing away.\n",
    "\n",
    "- **Home Team Demoted** (p=0.0277): Home teams that were demoted show lower Over 2.5 rate (47.8% vs 50.1% regular teams). The effect is similar to away demoted, indicating demotion status has defensive implications regardless of venue.\n",
    "\n",
    "**Borderline/Not Significant:**\n",
    "- **Away Team Promoted** (p=0.0712): Shows higher Over 2.5 rate (51.8% vs 49.9%) but just misses significance at α=0.05. May reflect attacking ambition of newly promoted teams.\n",
    "\n",
    "- **Home Team Promoted** (p=0.4339): No meaningful difference (50.8% vs 49.9%). Home promoted teams do not show distinct scoring patterns.\n",
    "\n",
    "**Conclusion:**  \n",
    "- **Weekend effect** is the most robust categorical predictor with clear practical significance.\n",
    "- **Demotion features** (both home and away) show consistent negative effects on Over 2.5 rate, suggesting these teams adopt more conservative tactics.\n",
    "- **Promotion features** show weaker/inconsistent effects - away promoted is borderline while home promoted shows no effect.\n",
    "- All `{len(bool_features)}` features detected and analyzed: `{bool_features}`\n",
    "- Individual effects remain modest (all within ±3 percentage points of baseline 50%), but may prove valuable in ensemble models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Extended feature df engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create extended dataframe with all available match data including detailed statistics and betting odds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTENDED DATASET: All available match data including detailed statistics\n",
    "extended_core_features = [col for col in [\n",
    "    # Core match info\n",
    "    'Div', 'Season', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG', 'HTR',\n",
    "    # Match statistics\n",
    "    'Attendance', 'Referee', 'HS', 'AS', 'HST', 'AST', 'HHW', 'AHW', 'HC', 'AC',\n",
    "    'HF', 'AF', 'HFKC', 'AFKC', 'HO', 'AO', 'HY', 'AY', 'HR', 'AR', 'HBP', 'ABP'\n",
    "] if col in all_matches.columns]\n",
    "\n",
    "# All engineered features (using rich match statistics)\n",
    "# Note: Additional features (time-based + historical) will be merged from df_basic after creation\n",
    "extended_engineered_features = [col for col in [\n",
    "    'total_goals', 'ht_total_goals', 'second_half_goals',  # Goal-based\n",
    "    'home_shot_accuracy', 'away_shot_accuracy', 'total_shots', 'total_shots_on_target',  # Shot-based\n",
    "    'shot_dominance', 'corner_dominance', 'total_fouls', 'total_cards', 'card_intensity',  # Game dynamics\n",
    "    'league_tier', 'month', 'is_weekend',  # Date/league features\n",
    "    'over_2_5'  # Target variable\n",
    "] if col in all_matches.columns]\n",
    "\n",
    "# Extended features (betting odds - only high-quality columns after imputation)\n",
    "betting_features = []\n",
    "for col in all_matches.columns:\n",
    "    # Check if it's a betting column and has good data coverage (>10%)\n",
    "    if any(bookmaker in col for bookmaker in ['B365', 'BW', 'PS', 'IW', 'LB', 'WH', 'SJ', 'VC', 'BF', '1XB']) and col not in categorical_features:\n",
    "        data_coverage = (all_matches[col].notna().sum() / len(all_matches)) * 100\n",
    "        if data_coverage >= 10:  # Only include columns with at least 10% data coverage\n",
    "            betting_features.append(col)\n",
    "\n",
    "# Create extended dataframe\n",
    "all_extended_features = extended_core_features + extended_engineered_features + betting_features\n",
    "# Remove duplicates while preserving order\n",
    "all_extended_features = list(dict.fromkeys(all_extended_features))\n",
    "\n",
    "df_extended = all_matches[all_extended_features].copy()\n",
    "df_extended = df_extended.sort_values(['Div', 'Date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Extended dataframe created\")\n",
    "print(f\"Shape: {df_extended.shape}\")\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"  Core features: {len(extended_core_features)}\")\n",
    "print(f\"  Engineered features: {len(extended_engineered_features)}\")\n",
    "print(f\"  Betting features (>10% coverage): {len(betting_features)}\")\n",
    "print(f\"  Total features: {len(all_extended_features)}\")\n",
    "print(f\"\\nColumns: {df_extended.columns.tolist()[:20]}...\")  # Show first 20\n",
    "df_extended.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Merging engineered features from df_basic\n",
    "\n",
    "Add all engineered features from df_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_based_features = [\n",
    "    'home_days_since_last', 'away_days_since_last',\n",
    "    'home_goals_ma5', 'away_goals_ma5', 'home_conceded_ma5', 'away_conceded_ma5',\n",
    "    'home_promoted', 'away_promoted', 'home_demoted', 'away_demoted'\n",
    "]\n",
    "\n",
    "historical_position_cols = [col for col in df_basic.columns if 'position_pct_' in col]\n",
    "historical_goal_cols = [col for col in df_basic.columns if any(\n",
    "    f'{p}_{t}plus_pct_' in col for p in ['scored', 'conceded', 'total'] for t in [2, 3]\n",
    ")]\n",
    "\n",
    "features_to_merge = time_based_features + historical_position_cols + historical_goal_cols\n",
    "\n",
    "print(f\"Features to merge from df_basic: {len(features_to_merge)}\")\n",
    "print(f\"  Time-based features: {len(time_based_features)}\")\n",
    "print(f\"  Position percentiles: {len(historical_position_cols)}\")\n",
    "print(f\"  Goal patterns: {len(historical_goal_cols)}\")\n",
    "\n",
    "df_basic_sorted = df_basic.sort_values(['Div', 'Date']).reset_index(drop=True)\n",
    "df_extended_sorted = df_extended.sort_values(['Div', 'Date']).reset_index(drop=True)\n",
    "\n",
    "match_check = (\n",
    "    (df_basic_sorted['Div'] == df_extended_sorted['Div']) &\n",
    "    (df_basic_sorted['Date'] == df_extended_sorted['Date']) &\n",
    "    (df_basic_sorted['HomeTeam'] == df_extended_sorted['HomeTeam']) &\n",
    "    (df_basic_sorted['AwayTeam'] == df_extended_sorted['AwayTeam'])\n",
    ").all()\n",
    "\n",
    "if match_check:\n",
    "    print(f\"Row alignment verified - safe to merge features\")\n",
    "\n",
    "    for col in features_to_merge:\n",
    "        df_extended[col] = df_basic_sorted[col].values\n",
    "\n",
    "    print(f\" {len(features_to_merge)} features added to df_extended\")\n",
    "    print(f\"  - {len(time_based_features)} time-based features\")\n",
    "    print(f\"  - {len(historical_position_cols)} position percentiles\")\n",
    "    print(f\"  - {len(historical_goal_cols)} goal pattern features\")\n",
    "    print(f\"\\nFinal df_extended shape: {df_extended.shape}\")\n",
    "else:\n",
    "    print(\"Row mismatch detected - cannot safely merge features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2. More feautures for df_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3. Betting odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Final feature dataframe\n",
    "\n",
    "Combine all features into the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of engineered features\n",
    "print(\"Final engineered dataframe:\")\n",
    "print(f\"Shape: {df_basic.shape}\")\n",
    "print(f\"\\nColumns:\")\n",
    "print(df_basic.columns.tolist())\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df_basic.isnull().sum().sum())\n",
    "print(f\"\\nSample:\")\n",
    "df_basic[['Date', 'HomeTeam', 'AwayTeam', 'total_goals', 'over_2_5',\n",
    "    'home_days_since_last', 'home_goals_ma5', 'home_promoted', 'is_weekend']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Added Features**\n",
    "\n",
    "**Time-based metrics:**  \n",
    "- `home_days_since_last`, `away_days_since_last`: Days since each team’s previous match (average ≈ 9.5 days)\n",
    "\n",
    "**Recent performance (5-match moving averages):**  \n",
    "- `home_goals_ma5`, `away_goals_ma5`: Average goals scored in the last 5 matches  \n",
    "- `home_conceded_ma5`, `away_conceded_ma5`: Average goals conceded in the last 5 matches  \n",
    "\n",
    "**League transitions:**  \n",
    "- `home_promoted`, `away_promoted`, `home_demoted`, `away_demoted`: Indicators of team movement between divisions  \n",
    "\n",
    "#### **Main Observations** \n",
    "   - All features show very weak correlations (|r| < 0.02) with the Over/Under 2.5 goals target.  \n",
    "\n",
    "   - Chi-square tests are insignificant (p > 0.05), suggesting no strong individual relationships. \n",
    "\n",
    "   - League level shows the strongest (though still small) correlation at −0.012, indicating that lower leagues may have slightly fewer high-scoring games.\n",
    "\n",
    "   - The home team’s recent attacking form has a weak positive correlation (0.010), but it’s negligible in isolation.\n",
    "\n",
    "   - Newly promoted or relegated teams do not exhibit consistent differences in total goals per match.\n",
    "\n",
    "   - Rest days and weekend scheduling have no measurable effect on goal totals.\n",
    "\n",
    "#### **Implications for Modeling**\n",
    "\n",
    "While each feature provides limited predictive power on its own, they may still add value when used together in non-linear models such as tree-based or ensemble methods.  \n",
    "\n",
    "Weak linear relationships are expected, as the Over/Under 2.5 target is a roughly balanced binary outcome, making individual predictors inherently limited in isolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Categorical encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Dataset preparation and train-test split\n",
    "\n",
    "For time series data like sports matches, we need to be careful about temporal splitting to avoid data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Temporal train-test split\n",
    "\n",
    "Since this is time series data, we'll split chronologically to simulate real-world predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by date for temporal split\n",
    "all_matches_sorted = all_matches.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "# Use 80% for training (chronologically earlier), 20% for testing (more recent)\n",
    "split_date = all_matches_sorted['Date'].quantile(0.8)\n",
    "print(f\"Split date: {split_date}\")\n",
    "\n",
    "train_mask = all_matches_sorted['Date'] <= split_date\n",
    "test_mask = all_matches_sorted['Date'] > split_date\n",
    "\n",
    "train_data = all_matches_sorted[train_mask].copy()\n",
    "test_data = all_matches_sorted[test_mask].copy()\n",
    "\n",
    "print(f\"Training set: {len(train_data)} matches ({train_data['Date'].min()} to {train_data['Date'].max()})\")\n",
    "print(f\"Test set: {len(test_data)} matches ({test_data['Date'].min()} to {test_data['Date'].max()})\")\n",
    "print(f\"Train Over 2.5 rate: {train_data['over_2_5'].mean():.2%}\")\n",
    "print(f\"Test Over 2.5 rate: {test_data['over_2_5'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Basic vs Extended datasets\n",
    "\n",
    "Create two datasets as mentioned in the project goals:\n",
    "- **Basic dataset**: Core match statistics only\n",
    "- **Extended dataset**: Including betting odds and additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets for modeling\n",
    "basic_features = basic_core_features + basic_engineered_features\n",
    "extended_features = extended_core_features + extended_engineered_features + betting_features\n",
    "\n",
    "# Exclude columns that shouldn't be used directly in modeling\n",
    "excluded_from_modeling = ['Date', 'Time']  # These are used for feature engineering but not direct modeling\n",
    "\n",
    "# We'll handle categorical encoding in the modeling phase\n",
    "target = 'over_2_5'\n",
    "\n",
    "print(f\"Basic model features: {len(basic_features)}\")\n",
    "print(f\"Extended model features: {len(extended_features)}\")\n",
    "\n",
    "# Create datasets (without categorical encoding for now)\n",
    "def create_dataset(data, features, target_col):\n",
    "    \"\"\"Create feature matrix and target vector\"\"\"\n",
    "    # Only include features that exist in the data and exclude date/time columns for modeling\n",
    "    available_features = [f for f in features if f in data.columns and f not in excluded_from_modeling]\n",
    "\n",
    "    X = data[available_features].copy()\n",
    "    y = data[target_col].copy()\n",
    "\n",
    "    return X, y, available_features\n",
    "\n",
    "# Basic datasets (core columns + basic engineered features only)\n",
    "X_train_basic, y_train, basic_features_final = create_dataset(train_data, basic_features, target)\n",
    "X_test_basic, y_test, _ = create_dataset(test_data, basic_features, target)\n",
    "\n",
    "# Extended datasets (all match stats + betting odds + all engineered features)\n",
    "X_train_extended, _, extended_features_final = create_dataset(train_data, extended_features, target)\n",
    "X_test_extended, _, _ = create_dataset(test_data, extended_features, target)\n",
    "\n",
    "print(f\"\\nFinal feature counts:\")\n",
    "print(f\"Basic features available: {len(basic_features_final)}\")\n",
    "print(f\"Extended features available: {len(extended_features_final)}\")\n",
    "\n",
    "print(f\"\\nBasic features: {basic_features_final}\")\n",
    "print(f\"\\nExtended features sample (first 20): {extended_features_final[:20]}\")\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"X_train_basic: {X_train_basic.shape}\")\n",
    "print(f\"X_test_basic: {X_test_basic.shape}\")\n",
    "print(f\"X_train_extended: {X_train_extended.shape}\")\n",
    "print(f\"X_test_extended: {X_test_extended.shape}\")\n",
    "\n",
    "# Check for missing values in final datasets\n",
    "print(f\"\\nMissing values in basic features:\")\n",
    "print(X_train_basic.isnull().sum().sum())\n",
    "print(f\"Missing values in extended features:\")\n",
    "print(X_train_extended.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Save processed datasets\n",
    "\n",
    "Save the preprocessed data for use in modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed datasets\n",
    "datasets = {\n",
    "    'X_train_basic': X_train_basic_imputed,\n",
    "    'X_test_basic': X_test_basic_imputed,\n",
    "    'X_train_extended': X_train_extended_imputed,\n",
    "    'X_test_extended': X_test_extended_imputed,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test\n",
    "}\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    filepath = f\"{OUTPUT_DIR}/{name}.pkl\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "# Also save feature names\n",
    "feature_info = {\n",
    "    'basic_features': basic_features_final,\n",
    "    'extended_features': extended_features_final,\n",
    "    'target': target\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/feature_info.pkl\", 'wb') as f:\n",
    "    pickle.dump(feature_info, f)\n",
    "\n",
    "# Save also as CSV for easy inspection\n",
    "# X_train_basic_imputed.to_csv(f\"{OUTPUT_DIR}/X_train_basic.csv\", index=False)\n",
    "# X_train_extended_imputed.to_csv(f\"{OUTPUT_DIR}/X_train_extended.csv\", index=False)\n",
    "# y_train.to_csv(f\"{OUTPUT_DIR}/y_train.csv\", index=False)\n",
    "# y_test.to_csv(f\"{OUTPUT_DIR}/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Preprocessing Summary\n",
    "\n",
    "## What was accomplished:\n",
    "\n",
    "### Data Loading & Cleaning:\n",
    "- Loaded 42,593 matches from 11 countries and 21 leagues\n",
    "- Handled unnamed columns (100% missing data)\n",
    "- Normalized league codes (E0→E1, SC0→SC1, etc.)\n",
    "- Corrected English/Scottish yellow card counts\n",
    "- Converted data types properly (datetime, categorical, numerical)\n",
    "- Imputed missing values in key match statistics (<0.1% missing)\n",
    "\n",
    "### Exploratory Data Analysis:\n",
    "- Target variable (Over/Under 2.5 goals) perfectly balanced: 49.99% / 50.01%\n",
    "- Analyzed missing data patterns (betting odds 80%+ missing, match stats <0.1%)\n",
    "- Examined country/league distributions\n",
    "- Outlier analysis using z-scores (kept outliers as legitimate in football)\n",
    "\n",
    "### Feature Engineering:\n",
    "- Created target variable: `over_2_5` (Over/Under 2.5 goals)\n",
    "- Shot efficiency features: accuracy, total shots, dominance measures\n",
    "- Game intensity features: fouls, cards, card intensity\n",
    "- Temporal features: half-time patterns, second-half goals\n",
    "- League and seasonal features: tier, month, weekend indicator\n",
    "\n",
    "### Dataset Preparation:\n",
    "- **Temporal train-test split**: 80% train (2019-2024) / 20% test (2024-2025)\n",
    "- **Basic dataset**: ~7 features (ONLY original core columns: FTHG, FTAG, HTHG, HTAG + goal-based engineered features)\n",
    "- **Extended dataset**: ~40+ features (all match statistics + betting odds + all engineered features)\n",
    "- Missing value imputation for both datasets\n",
    "- Saved processed data for modeling\n",
    "\n",
    "## Data Quality:\n",
    "- **Training set**: 34,085 matches (49.56% Over 2.5)\n",
    "- **Test set**: 8,508 matches (51.72% Over 2.5)\n",
    "- **No missing values** in final processed datasets\n",
    "- **Temporally sorted** to prevent data leakage\n",
    "\n",
    "## Key Distinction:\n",
    "- **Basic Model**: Uses only original core data (goals) + simple derived features\n",
    "- **Extended Model**: Uses rich match statistics (shots, fouls, cards) + betting odds + complex features"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NBksARdgdMkP",
    "Qx3gpH8TdToi",
    "O5SNQWIMhxZA",
    "NNyVnO2Bk4zo",
    "XMiRfCrVzDVd",
    "fwwBVM5ex2SA"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
